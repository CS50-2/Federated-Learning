# -*- coding: utf-8 -*-
"""GRA with freeze and prune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ukIrbbcccAVDU1ESIFiL1I4WbEYBevgU
"""

# -*- coding: utf-8 -*-
"""Federated Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import copy
import os
import matplotlib.pyplot as plt
import csv
import pandas as pd
from datetime import datetime
import time
! pip install fvcore
from fvcore.nn import FlopCountAnalysis
import torch.nn.functional as F
import torch.nn.utils.prune as prune

# 定义 MLP 模型
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)  # 第一层，输入维度 784 -> 200
        self.fc2 = nn.Linear(200, 200)  # 第二层，200 -> 200
        self.fc3 = nn.Linear(200, 10)  # 输出层，200 -> 10
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)  # 展平输入 (batch_size, 1, 28, 28) -> (batch_size, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # 直接输出，不使用 Softmax（因为 PyTorch 的 CrossEntropyLoss 里已经包含了）
        return x


# 加载 MNIST 数据集
def load_mnist_data(data_path="./data"):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("✅ MNIST 数据集已存在，跳过下载。")
    else:
        print("⬇️ 正在下载 MNIST 数据集...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    # visualize_mnist_samples(train_data)
    return train_data, test_data


# 显示数据集示例图片
def visualize_mnist_samples(dataset, num_samples=10):
    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 1.2, 1.5))
    for i in range(num_samples):
        img, label = dataset[i]
        axes[i].imshow(img.squeeze(), cmap="gray")
        axes[i].set_title(label)
        axes[i].axis("off")
    plt.show()


def split_data_by_label(dataset, num_clients=10):
    """
    手动划分数据集，每个客户端包含 10 个类别，并自定义样本数量。
    :param dataset: 原始数据集（如 MNIST）
    :param num_clients: 客户端总数
    :return: (客户端数据集, 客户端数据大小)
    """
    #手动划分的样本数量（每个客户端 10 个类别的数据量）
    # client_data_sizes = {
    #     0: {0: 600, 1: 700, 2: 600, 3: 600, 4: 500, 5: 500, 6: 100, 7: 100, 8: 100, 9: 100},
    #     1: {0: 700, 1: 600, 2: 600, 3: 600, 4: 500, 5: 100, 6: 100, 7: 100, 8: 100, 9: 600},
    #     2: {0: 500, 1: 600, 2: 700, 3: 600, 4: 100, 5: 100, 6: 100, 7: 100, 8: 600, 9: 500},
    #     3: {0: 600, 1: 600, 2: 500, 3: 100, 4: 100, 5: 100, 6: 100, 7: 500, 8: 500, 9: 700},
    #     4: {0: 600, 1: 500, 2: 100, 3: 100, 4: 100, 5: 100, 6: 600, 7: 700, 8: 500, 9: 500},
    #     5: {0: 500, 1: 100, 2: 100, 3: 100, 4: 100, 5: 600, 6: 500, 7: 600, 8: 700, 9: 600},
    #     6: {0: 100, 1: 100, 2: 100, 3: 100, 4: 700, 5: 500, 6: 600, 7: 500, 8: 500, 9: 600},
    #     7: {0: 100, 1: 100, 2: 100, 3: 600, 4: 500, 5: 600, 6: 500, 7: 600, 8: 500, 9: 100},
    #     8: {0: 100, 1: 100, 2: 500, 3: 500, 4: 600, 5: 500, 6: 600, 7: 500, 8: 100, 9: 100},
    #     9: {0: 100, 1: 700, 2: 600, 3: 600, 4: 600, 5: 500, 6: 600, 7: 100, 8: 100, 9: 100}
    # }
    client_data_sizes = {
        0: {0: 600},
        1: {1: 700},
        2: {2: 500},
        3: {3: 600},
        4: {4: 600},
        5: {5: 500},
        6: {6: 100},
        7: {7: 100},
        8: {8: 100},
        9: {9: 100}
    }



    # 统计每个类别的数据索引
    label_to_indices = {i: [] for i in range(10)}  # 记录每个类别的索引
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    # 初始化客户端数据存储
    client_data_subsets = {}
    client_actual_sizes = {i: {label: 0 for label in range(10)} for i in range(num_clients)}  # 记录实际分配的数据量

    # 遍历每个客户端，为其分配指定类别的数据
    for client_id, label_info in client_data_sizes.items():
        selected_indices = []  # 临时存储该客户端所有选中的索引
        for label, size in label_info.items():
            # 确保不超出类别数据集实际大小
            available_size = len(label_to_indices[label])
            sample_size = min(available_size, size)

            if sample_size < size:
                print(f"⚠️ 警告：类别 {label} 的数据不足，客户端 {client_id} 只能获取 {sample_size} 条样本（需求 {size} 条）")

            # 从该类别中随机抽取样本
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)

            # 记录实际分配的数据量
            client_actual_sizes[client_id][label] = sample_size

        # 创建 PyTorch Subset
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)

    # 打印每个客户端的实际分配数据量
    print("\n📊 每个客户端实际数据分布:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"客户端 {client_id}: {label_sizes}")

    return client_data_subsets, client_actual_sizes


# 本地训练函数
def local_train(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=lr
        )
    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()

# 实时记录训练过程中的loss用做Fedgra
def local_train_fedgra_loss(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=lr
        )
    model.train()
    loss_sq_sum = 0.0

    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            loss_sq_sum += loss.item() ** 2

    h_i = loss_sq_sum #放大训练过程中 loss 的累积程度，从而增强客户端之间的区分度。
    return model, h_i

#  联邦平均聚合函数
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    subkey = [sublist[0] for sublist in client_state_dicts]
    new_client_sizes = dict(([(key, client_sizes[key]) for key in subkey]))
    total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())  # 计算所有客户端数据总量
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
            for (client_id, client_state) in client_state_dicts
        )
    global_model.load_state_dict(global_dict)
    return global_model


# 评估模型
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy

# 熵权法实现

def entropy_weight(l):
    l = np.array(l)

    # Step 1: Min-Max 归一化（避免负值和爆炸）
    X_norm = (l - l.min(axis=1, keepdims=True)) / (l.max(axis=1, keepdims=True) - l.min(axis=1, keepdims=True) + 1e-12)

    # Step 2: 转为概率矩阵 P_ki
    P = X_norm / (X_norm.sum(axis=1, keepdims=True) + 1e-12)

    # Step 3: 计算熵
    K = 1 / np.log(X_norm.shape[1])
    E = -K * np.sum(P * np.log(P + 1e-12), axis=1)  # shape: (2,)

    # Step 4: 计算信息效用值 & 权重
    d = 1 - E
    weights = d / np.sum(d)
    return weights.tolist()


# 灰色关联度实现
def calculate_GRC(global_model, client_models, client_losses):
    """
    计算 GRC 分数 + 熵权法权重
    修正：
      - 映射顺序错误
      - 熵权法使用错误指标
    """
    # 正确写法：使用整体参数向量计算 L2 范数（符合文献）
    param_diffs = []
    for model in client_models:
        global_vec = torch.nn.utils.parameters_to_vector(global_model.parameters()).detach()
        local_vec = torch.nn.utils.parameters_to_vector(model.parameters()).detach()
        diff = torch.norm(local_vec - global_vec).item()
        param_diffs.append(diff)

    # 2. 映射原始指标到 [0, 1] 区间（为熵权法准备）
    def map_sequence_loss(sequence):
        max_val, min_val = max(sequence), min(sequence)
        denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
        return [(max_val - x) / denom for x in sequence]  # 越小越好，负相关

    def map_sequence_diff(sequence):
        max_val, min_val = max(sequence), min(sequence)
        denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
        return [(x - min_val) / denom for x in sequence]  # 越大越好，正相关

    # 用于 GRC 的映射
    mapped_losses = map_sequence_loss(client_losses)
    mapped_diffs = map_sequence_diff(param_diffs)

    # 3. 熵权法计算权重（根据映射值，非 GRC）
    grc_metrics = np.vstack([mapped_losses, mapped_diffs])  # shape: (2, n_clients)
    weights = entropy_weight(grc_metrics)  # w_loss, w_diff

    # 4. 计算 GRC 分数（ξki），参考值为 1
    ref_loss, ref_diff = 1.0, 1.0
    delta_losses = [abs(x - ref_loss) for x in mapped_losses]
    delta_diffs = [abs(x - ref_diff) for x in mapped_diffs]
    all_deltas = delta_losses + delta_diffs
    max_delta, min_delta = max(all_deltas), min(all_deltas)

    grc_losses = []
    grc_diffs = []
    rho = 0.5  # 区分度因子
    for d_loss, d_diff in zip(delta_losses, delta_diffs):
        grc_loss = (min_delta + rho * max_delta) / (d_loss + rho * max_delta)
        grc_diff = (min_delta + rho * max_delta) / (d_diff + rho * max_delta)
        grc_losses.append(grc_loss)
        grc_diffs.append(grc_diff)

    # 5. 加权求和得到最终 GRC 分数
    grc_losses = np.array(grc_losses)
    grc_diffs = np.array(grc_diffs)
    weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]

    # 调试（每个客户端的 loss、diff、得分）
    print("\n GRC得分]")
    for i in range(len(client_models)):
        print(f"Client {i} | loss: {client_losses[i]:.4f}, diff: {param_diffs[i]:.4f}, "
              f"mapped_loss: {mapped_losses[i]:.4f}, mapped_diff: {mapped_diffs[i]:.4f}, "
              f"GRC: {weighted_score[i]:.4f}")
    print(f"熵权法权重: w_loss = {weights[0]:.4f}, w_diff = {weights[1]:.4f}")

    return weighted_score, weights

def compute_model_flops(
    model, input_shape=(1, 1, 28, 28), batch_size=1, num_samples=1, epochs=1, backward=False, verbose=False ,freeze = False, prune = False
):
    """
    计算训练阶段的总 FLOPs（估算）
    :param model: PyTorch 模型
    :param input_shape: 单个样本的输入 shape（如 MNIST 为 (1, 28, 28)）
    :param batch_size: 每次输入的样本数
    :param num_samples: 该客户端一共多少训练样本
    :param epochs: 每轮训练几个 epoch
    :param backward_factor: backward FLOPs 是 forward 的几倍（一般是 2~3）
    :param verbose: 是否打印每层 FLOPs
    :return: 估算总 FLOPs
    """
    model.eval()
    dummy_input = torch.randn((batch_size, *input_shape[1:]))  # 注意拆解 shape
    flops_analyzer = FlopCountAnalysis(model, dummy_input)

    if backward:
      if freeze:
        backward_factor = 1.42
      if prune:
        backward_factor = 1.7
      else:
        backward_factor = 3
    else:
      backward_factor = 0

    if verbose:
        print(flops_analyzer.by_module())  # 打印每层 FLOPs
        print(f"📊 单个 batch 的 forward FLOPs: {flops_analyzer.total() / 1e6:.2f} MFLOPs")

    # 单个 batch 的 forward FLOPs
    forward_flops_per_batch = flops_analyzer.total()

    # 训练 FLOPs ≈ (forward + backward) × 批次数 × epoch
    num_batches = int(np.ceil(num_samples / batch_size))
    total_training_flops = forward_flops_per_batch * (1 + backward_factor) * num_batches * epochs

    return total_training_flops

def apply_structured_pruning(model, amount=0.3):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            torch.nn.utils.prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)
            torch.nn.utils.prune.remove(module, 'weight')  # 🔥 移除掩码，实际修改模型结构
    return model

def structured_prune_layer(model, layer_name='fc2', prune_ratio=0.5, criterion='l1'):
    """
    通用结构化剪枝函数：对 fc1, fc2, fc3 中任意层按比例剪枝，并自动修改相应上下游层结构。
    """
    # 取出目标层
    layer = getattr(model, layer_name)
    weight = layer.weight.data  # shape: [out_features, in_features]
    bias = layer.bias.data

    # 计算神经元重要性（按输出神经元维度）
    if criterion == 'l1':
        scores = weight.abs().sum(dim=1)
    elif criterion == 'l2':
        scores = torch.norm(weight, p=2, dim=1)
    else:
        raise ValueError("Unsupported criterion. Use 'l1' or 'l2'.")

    # 决定要保留的神经元索引
    total_neurons = weight.shape[0]
    keep_num = max(1, int(total_neurons * prune_ratio))
    _, keep_indices = torch.topk(scores, keep_num)

    # 构造新的层
    new_layer = nn.Linear(weight.shape[1], keep_num)
    new_layer.weight.data = weight[keep_indices]
    new_layer.bias.data = bias[keep_indices]
    setattr(model, layer_name, new_layer)

    # 处理下游层或上游层的连通性
    if layer_name == 'fc1':
        # fc2 的输入需要裁剪列
        fc2 = model.fc2
        new_fc2 = nn.Linear(keep_num, fc2.out_features)
        new_fc2.weight.data = fc2.weight.data[:, keep_indices]
        new_fc2.bias.data = fc2.bias.data
        model.fc2 = new_fc2

    elif layer_name == 'fc2':
        # fc3 的输入需要裁剪列
        fc3 = model.fc3
        new_fc3 = nn.Linear(keep_num, fc3.out_features)
        new_fc3.weight.data = fc3.weight.data[:, keep_indices]
        new_fc3.bias.data = fc3.bias.data
        model.fc3 = new_fc3

    elif layer_name == 'fc3':
        # fc3 是输出层，不影响其他结构
        pass

    else:
        raise ValueError("Only supports 'fc1', 'fc2', or 'fc3'")

    return model

def apply_unstructured_pruning(model, amount=0.3):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=amount)
    return model

def apply_freezing(model):
    for name, param in model.named_parameters():
        if 'weight' in name or 'bias' in name:
            param.requires_grad = False
    return model

def apply_partial_freezing(model, freeze_layers=1):
    """
    冻结模型前 freeze_layers 个 Linear 层的权重和偏置。
    """
    count = 0
    for module in model.modules():
        if isinstance(module, nn.Linear):
            for name, param in module.named_parameters():
                param.requires_grad = False
            count += 1
            if count >= freeze_layers:
                break
    return model

# 客户端选择器
def select_clients(client_loaders, use_all_clients=False, num_select=None,
                   select_by_loss=False, global_model=None, grc=False,
                   fairness_tracker=None, prune=False, prune_amount=0.3, freeze=False, freeze_layers = 1,):
    total_flops = 0
    epochs = 1
    total_time = 0
    if grc:  # 使用 GRC 选择客户端
        client_models = []
        # 1. 训练本地模型并计算损失
        client_losses = []
        for client_id, client_loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # 同步全局模型
            if prune:
                local_model = structured_prune_layer(local_model, prune_ratio=prune_amount)
            if freeze:
                local_model = apply_partial_freezing(local_model, freeze_layers=freeze_layers)

            # 每轮都重新估计一次 FLOPs（避免模型变结构时不更新）
            sample_flops = compute_model_flops(local_model, input_shape=(1, 1, 28, 28), prune = prune ,freeze = freeze)
            n_samples = len(client_loader.dataset)
            total_flops += sample_flops * n_samples * epochs

            # 模型训练&计算时间
            client_pre_models = copy.deepcopy(local_model)
            start_time = time.time()
            trained_model, h_i = local_train_fedgra_loss(local_model, client_loader, epochs=5, lr=0.01)
            end_time = time.time()
            total_time += (end_time - start_time)
            client_models.append(trained_model)
            client_losses.append(h_i)
        print(f"📊 本轮客户端选择阶段估算总 FLOPs: {total_flops / 1e12:.4f} TFLOPs")

        # 2. 计算 GRC 分数
        grc_scores, grc_weights = calculate_GRC(client_pre_models, client_models, client_losses)
        select_clients.latest_weights = grc_weights  # 记录权重

        # 3. 按 GRC 分数排序（从高到低，GRC越高表示越好）
        client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
        client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # 降序排序

        # 4. 选择 GRC 最高的前 num_select 个客户端
        selected_clients = [client_id for client_id, _ in client_grc_pairs[:num_select]]
        return selected_clients, total_flops, total_time

    # 其余选择逻辑保持不变

    if use_all_clients is True:
        print("Selecting all clients")
        return list(client_loaders.keys())

    if num_select is None:
        raise ValueError("If use_all_clients=False, num_select cannot be None!")

    if select_by_loss and global_model:
        client_losses = {}

        for client_id, loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            if prune:
                local_model = structured_prune_layer(local_model, prune_ratio = prune_amount)
            if freeze:
                local_model = apply_partial_freezing(local_model, freeze_layers = freeze_layers)

            # 每轮都重新估计一次 FLOPs（避免模型变结构时不更新）
            sample_flops = compute_model_flops(local_model, input_shape=(1, 1, 28, 28), prune = prune ,freeze = freeze)
            n_samples = len(loader.dataset)
            total_flops += sample_flops * n_samples * epochs

            # 模型训练&计算推理时间
            start_infer = time.time()
            local_train(local_model, loader, epochs=5, lr=0.01)
            loss, _ = evaluate(local_model, loader)
            end_infer = time.time()
            total_time += (end_infer - start_infer)
            client_losses[client_id] = loss
        selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
        print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
    else:
        selected_clients = random.sample(list(client_loaders.keys()), num_select)
        print(f"Randomly selected {num_select} clients: {selected_clients}")

    return selected_clients, total_flops, total_time


def update_communication_counts(communication_counts, selected_clients, event):
    """
    客户端通信计数
    - event='receive' 表示客户端接收到全局模型
    - event='send' 表示客户端上传本地模型
    - event='full_round' 仅在客户端完成完整收发时增加
    """
    for client_id in selected_clients:
        communication_counts[client_id][event] += 1

        # 仅当客户端完成一次完整的 send 和 receive 时增加 full_round
        if event == "send" and communication_counts[client_id]['receive'] > 0:
            communication_counts[client_id]['full_round'] += 1


def main():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # 加载 MNIST 数据集
    train_data, test_data = load_mnist_data()

    # 生成客户端数据集，每个客户端包含多个类别
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # 创建数据加载器
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # 初始化全局模型
    global_model = MLPModel()
    global_accuracies = []  # 记录每轮全局模型的测试集准确率
    total_communication_counts = []  # 记录每轮客户端通信次数
    rounds = 100  # 联邦学习轮数
    use_all_clients = False  # 是否进行客户端选择
    num_selected_clients = 2  # 每轮选择客户端训练数量
    use_loss_based_selection = False  # 是否根据 loss 选择客户端
    grc = True

    # 初始化通信计数器
    communication_counts = {}
    for client_id in client_loaders.keys():
        communication_counts[client_id] = {
            'send': 0,
            'receive': 0,
            'full_round': 0
        }

    # 实验数据存储 CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}.csv"
    csv_data = []

    for r in range(rounds):
        print(f"\n🔄 第 {r + 1} 轮聚合")
        # 选择客户端

        selected_clients, round_flops, round_time = select_clients(
            client_loaders,
            use_all_clients=use_all_clients,
            num_select=num_selected_clients,
            select_by_loss=use_loss_based_selection,
            global_model=global_model,
            grc=grc
        )
        # # 设置随机阻断某个客户端的接收记录（验证用）
        # blocked_client = random.choice(selected_clients)
        # print(f" Blocking client {blocked_client} from receiving, skipping the receive event record.")

        # for client_id in selected_clients:
        #     if client_id == blocked_client:
        #         continue  # 直接跳过 receive 记录
        #     update_communication_counts(communication_counts, [client_id], "receive")

        # 记录客户端接收通信次数
        update_communication_counts(communication_counts, selected_clients, "receive")
        client_state_dicts = []

        # 客户端本地训练
        for client_id in selected_clients:
            client_loader = client_loaders[client_id]
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # 复制全局模型参数
            local_state = local_train(local_model, client_loader, epochs=5, lr=0.01)  # 训练 1 轮
            client_state_dicts.append((client_id, local_state))  # 存储 (客户端ID, 训练后的参数)

            update_communication_counts(communication_counts, [client_id], "send")  # 记录客户端上报通信次数

            param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
            print(f"  ✅ 客户端 {client_id} 训练完成 | 样本数量: {sum(client_data_sizes[client_id].values())}")
            print(f"  📌 客户端 {client_id} 模型参数均值: {param_mean}")

        # 计算本轮通信次数
        total_send = sum(communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_receive = sum(communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_comm = total_send + total_receive  # 每轮独立的总通信次数

        # 如果不是第一轮，累加前一轮的通信次数
        if len(total_communication_counts) > 0:
            total_comm += total_communication_counts[-1]
        total_communication_counts.append(total_comm)

        # 聚合模型参数
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

        # # 计算全局模型参数平均值
        # global_param_mean = {name: param.mean().item() for name, param in global_model.named_parameters()}
        # print(f"🔄 轮 {r + 1} 结束后，全局模型参数均值: {global_param_mean}")

        # 评估模型
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)
        print(f"📊 测试集损失: {loss:.4f} | 测试集准确率: {accuracy:.2f}%")

        # 记录数据到 CSV
        if grc and hasattr(select_clients, 'latest_weights'):
            w_loss = select_clients.latest_weights[0]
            w_diff = select_clients.latest_weights[1]
            print(f"📈 Round {r+1} | GRC 权重: w_loss = {w_loss:.4f}, w_diff = {w_diff:.4f}")

        else:
            w_loss = 'NA'
            w_diff = 'NA'

        csv_data.append([
            r + 1,
            accuracy,
            total_comm,
            ",".join(map(str, selected_clients)),
            w_loss,
            w_diff
        ])
        df = pd.DataFrame(csv_data, columns=[
            'Round', 'Accuracy', 'Total communication counts', 'Selected Clients',
            'GRC Weight - Loss', 'GRC Weight - Diff'])
        df.to_csv(csv_filename, index=False)

    # 输出最终模型的性能
    final_loss, final_accuracy = evaluate(global_model, test_loader)
    print(f"\n🎯 Loss of final model test dataset: {final_loss:.4f}")
    print(f"🎯 Final model test set accuracy: {final_accuracy:.2f}%")

    # 输出通信记录
    print("\n Client Communication Statistics:")
    for client_id, counts in communication_counts.items():
        print(
            f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")

    # 可视化全局模型准确率 vs 轮次
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, rounds + 1), global_accuracies, marker='o', linestyle='-', color='b', label="Test Accuracy")
    plt.xlabel("Federated Learning Rounds")
    plt.ylabel("Accuracy")
    plt.title("Test Accuracy Over Federated Learning Rounds")
    plt.legend()
    plt.grid(True)
    plt.show()

    # 可视化全局模型准确率 vs 客户端完整通信次数
    plt.figure(figsize=(8, 5))
    plt.plot(total_communication_counts, global_accuracies, marker='s', linestyle='-', color='r',
             label="Test Accuracy vs. Communication")
    plt.xlabel("Total Communication Count per Round")
    plt.ylabel("Accuracy")
    plt.title("Test Accuracy vs. Total Communication")
    plt.legend()
    plt.grid(True)
    plt.show()

# 高 Loss 跨轮次选择

def main2():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # 加载数据
    train_data, test_data = load_mnist_data()
    client_datasets, client_data_sizes = split_data_by_label(train_data)
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # 初始化全局模型
    global_model = MLPModel()
    global_accuracies = []
    total_communication_counts = []
    rounds = 100
    num_selected_clients = 2

    # 初始化通信与客户端记录结构
    communication_counts = {cid: {'send': 0, 'receive': 0, 'full_round': 0} for cid in client_loaders}
    client_loss_history = {cid: [] for cid in client_loaders}
    local_model_cache = {}   # 模型缓存
    selected_clients = []    # 当前选中的客户端（用于非通信轮复用）

    # 准备 CSV 文件保存路径
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}_main2.csv"
    csv_data = []

    total_comm = 0  # 跨轮次累计通信量

    for r in range(rounds):
        print(f"\n🔄 第 {r + 1} 轮聚合")
        is_comm_round = False

        # 判断是否为通信轮
        if r % 3 == 0:
            current_losses = {}
            for cid, loader in client_loaders.items():
                loss, _ = evaluate(global_model, loader)
                current_losses[cid] = loss
                client_loss_history[cid].append(loss)

            # 加权历史 loss（更偏向当前轮）
            weighted_losses = {}
            for cid in client_loaders:
                history = client_loss_history[cid][-3:]
                if len(history) == 3:
                    weights = [0.7, 0.2, 0.1]
                elif len(history) == 2:
                    weights = [0.8, 0.2]
                else:
                    weights = [1.0]
                weighted_losses[cid] = sum(h * w for h, w in zip(history, weights)) / sum(weights)

            # 按 weighted loss 降序选择客户端
            selected_clients = sorted(weighted_losses, key=weighted_losses.get, reverse=True)[:num_selected_clients]
            print(f"📌 通信轮 | Selected clients: {selected_clients}")
            update_communication_counts(communication_counts, selected_clients, "receive")
            is_comm_round = True

        else:
            print(f"⏳ 非通信轮 | 复用客户端模型: {selected_clients}")

        client_state_dicts = []

        for cid in selected_clients:
            if is_comm_round:
                loader = client_loaders[cid]
                model = MLPModel()
                model.load_state_dict(global_model.state_dict())
                local_train(model, loader, epochs=1, lr=0.01)
                local_model_cache[cid] = model
                client_state_dicts.append((cid, model.state_dict()))
                update_communication_counts(communication_counts, [cid], "send")
                total_comm += 2  # send + receive
            else:
                if cid in local_model_cache:
                    client_state_dicts.append((cid, local_model_cache[cid].state_dict()))
                else:
                    print(f"⚠️ 客户端 {cid} 无模型缓存，跳过")

        # 记录累计通信次数
        total_communication_counts.append(total_comm)

        # 模型聚合 + 评估
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
        loss, acc = evaluate(global_model, test_loader)
        global_accuracies.append(acc)
        print(f"📊 测试集损失: {loss:.4f} | 测试集准确率: {acc:.2f}%")

        # 写入 CSV
        csv_data.append([r + 1, acc, total_comm, ",".join(map(str, selected_clients)), 'NA', 'NA'])
        df = pd.DataFrame(csv_data, columns=['Round', 'Accuracy', 'Total communication counts',
                                             'Selected Clients', 'GRC Weight - Loss', 'GRC Weight - Diff'])
        df.to_csv(csv_filename, index=False)

    # 最终评估
    final_loss, final_acc = evaluate(global_model, test_loader)
    print(f"\n🎯 Final Loss: {final_loss:.4f}")
    print(f"🎯 Final Accuracy: {final_acc:.2f}%")



# Fedgra跨轮次选择


def main3():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    train_data, test_data = load_mnist_data()
    client_datasets, client_data_sizes = split_data_by_label(train_data)
    client_loaders = {cid: data.DataLoader(dataset, batch_size=32, shuffle=True) for cid, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    global_model = MLPModel()
    global_accuracies = []
    total_communication_counts = []
    rounds = 100
    num_selected_clients = 2

    # 初始化通信记录、缓存、GRC历史
    communication_counts = {cid: {'send': 0, 'receive': 0, 'full_round': 0} for cid in client_loaders}
    local_model_cache = {}  # 存储客户端模型，非通信轮使用
    client_grc_history = {cid: [] for cid in client_loaders}  # 存储 GRC 历史
    selected_clients = []  # 每轮选择结果

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}_main3.csv"
    csv_data = []

    total_comm = 0  # 总通信次数

    for r in range(rounds):
        print(f"\n🔄 第 {r + 1} 轮聚合")
        w_loss, w_diff = 'NA', 'NA'
        round_comm = 0
        is_comm_round = (r % 3 == 0)

        client_state_dicts = []

        if is_comm_round:
            print("📡 通信轮")

            # 所有客户端都进行一次模型训练和loss评估（GRC计算）
            client_models, client_losses = [], []
            for cid, loader in client_loaders.items():
                model = MLPModel()
                model.load_state_dict(global_model.state_dict())
                local_train(model, loader, epochs=1, lr=0.01)
                client_models.append(model)
                loss, _ = evaluate(model, loader)
                client_losses.append(loss)

            # 计算 GRC 得分并存储
            grc_scores, grc_weights = calculate_GRC(global_model, client_models, client_losses)
            w_loss, w_diff = grc_weights

            for i, cid in enumerate(client_loaders.keys()):
                client_grc_history[cid].append(grc_scores[i])

            # 根据历史 GRC 做加权平均
            weighted_grc = {}
            for cid in client_loaders:
                history = client_grc_history[cid][-3:]
                if len(history) == 3:
                    weights = [0.7, 0.2, 0.1]
                elif len(history) == 2:
                    weights = [0.8, 0.2]
                else:
                    weights = [1.0]
                weighted_grc[cid] = sum(h * w for h, w in zip(history, weights)) / sum(weights)

            # 选择得分最高的客户端
            selected_clients = sorted(weighted_grc, key=weighted_grc.get, reverse=True)[:num_selected_clients]
            print(f"✅ Selected Clients: {selected_clients}")

            update_communication_counts(communication_counts, selected_clients, "receive")

            # 对被选中的客户端进行训练并缓存模型
            for cid in selected_clients:
                model = MLPModel()
                model.load_state_dict(global_model.state_dict())
                local_train(model, client_loaders[cid], epochs=1, lr=0.01)
                local_model_cache[cid] = model  # 缓存本地模型
                client_state_dicts.append((cid, model.state_dict()))
                update_communication_counts(communication_counts, [cid], "send")
                round_comm += 2

        else:
            print(f"⏳ 非通信轮 | 复用模型: {selected_clients}")
            # 非通信轮直接从缓存加载模型
            for cid in selected_clients:
                if cid in local_model_cache:
                    client_state_dicts.append((cid, local_model_cache[cid].state_dict()))
                else:
                    print(f"⚠️ 缺失缓存，客户端 {cid} 被跳过")

        # 累加通信次数
        total_comm += round_comm
        total_communication_counts.append(total_comm)

        # 聚合全局模型
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

        # 测试全局模型
        loss, acc = evaluate(global_model, test_loader)
        global_accuracies.append(acc)
        print(f"📊 Loss: {loss:.4f} | Accuracy: {acc:.2f}%")

        # 保存轮次记录
        csv_data.append([r + 1, acc, total_comm, ",".join(map(str, selected_clients)), w_loss, w_diff])
        df = pd.DataFrame(csv_data, columns=[
            'Round', 'Accuracy', 'Total communication counts',
            'Selected Clients', 'GRC Weight - Loss', 'GRC Weight - Diff'
        ])
        df.to_csv(csv_filename, index=False)

    # 最终评估
    final_loss, final_acc = evaluate(global_model, test_loader)
    print(f"\n🎯 Final Loss: {final_loss:.4f}")
    print(f"🎯 Final Accuracy: {final_acc:.2f}%")

def run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                   freeze=False, freeze_layers = 1, prune=False, prune_amount = 0.3,
                   select_by_loss = False, grc = False, label="",):
    global_model = MLPModel()
    global_accuracies = []
    total_communication_counts = []
    total_tflops = 0
    total_time = 0

    # 初始化通信计数器
    communication_counts = {}
    for client_id in client_loaders.keys():
        communication_counts[client_id] = {
            'send': 0,
            'receive': 0,
            'full_round': 0
        }

    # 实验数据存储 CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}.csv"
    csv_data = []

    for r in range(rounds):
        print(f"\n🔁 [{label}] Round {r + 1}")
        selected_clients, round_flops, round_time = select_clients(
            client_loaders, use_all_clients=False, num_select=2,
            select_by_loss=select_by_loss, global_model=global_model,grc=grc,
            prune=prune, prune_amount = prune_amount, freeze=freeze, freeze_layers=freeze_layers
        )
        print(f"Selected Clients: {selected_clients}")

        update_communication_counts(communication_counts, selected_clients, "receive")
        total_tflops += round_flops / 1e12
        total_time += round_time

        client_state_dicts = []

        # 客户端本地训练
        for client_id in selected_clients:
            client_loader = client_loaders[client_id]
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # 复制全局模型参数
            local_state = local_train(local_model, client_loader, epochs=5, lr=0.01)  # 训练 1 轮
            client_state_dicts.append((client_id, local_state))  # 存储 (客户端ID, 训练后的参数)

            update_communication_counts(communication_counts, [client_id], "send")  # 记录客户端上报通信次数

            param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
            print(f"  ✅ 客户端 {client_id} 训练完成 | 样本数量: {sum(client_data_sizes[client_id].values())}")
            print(f"  📌 客户端 {client_id} 模型参数均值: {param_mean}")

        # 计算本轮通信次数
        total_send = sum(communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_receive = sum(communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_comm = total_send + total_receive  # 每轮独立的总通信次数

        # 如果不是第一轮，累加前一轮的通信次数
        if len(total_communication_counts) > 0:
            total_comm += total_communication_counts[-1]
        total_communication_counts.append(total_comm)

        # 聚合
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)

        # 输出最终模型的性能
        final_loss, final_accuracy = evaluate(global_model, test_loader)
        print(f"🎯 Final model test set accuracy: {final_accuracy:.2f}%")

        # 输出通信记录
        print("\n Client Communication Statistics:")
        for client_id, counts in communication_counts.items():
            print(f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")
        # 记录数据到 CSV
        if grc and hasattr(select_clients, 'latest_weights'):
            w_loss = select_clients.latest_weights[0]
            w_diff = select_clients.latest_weights[1]
            print(f"📈 Round {r + 1} | GRC 权重: w_loss = {w_loss:.4f}, w_diff = {w_diff:.4f}")

        else:
            w_loss = 'NA'
            w_diff = 'NA'

        csv_data.append([
            r + 1,
            accuracy,
            total_comm,
            ",".join(map(str, selected_clients)),
            w_loss,
            w_diff
        ])
        df = pd.DataFrame(csv_data, columns=[
            'Round', 'Accuracy', 'Total communication counts', 'Selected Clients',
            'GRC Weight - Loss', 'GRC Weight - Diff'])
        df.to_csv(csv_filename, index=False)

    return global_accuracies, total_communication_counts ,total_tflops * rounds, total_time

def main4():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # 加载 MNIST 数据集
    train_data, test_data = load_mnist_data()

    # 生成客户端数据集，每个客户端包含多个类别
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # 创建数据加载器
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # 初始化全局模型
    rounds = 100  # 联邦学习轮数
    use_all_clients = False  # 是否进行客户端选择
    num_selected_clients = 2  # 每轮选择客户端训练数量
    t_flops = 0
    grc = False
    prune = True
    freeze = False


    # acc_loss, comm_loss, flops_loss, model_size_loss, inference_time_loss = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                        freeze=False, prune=False,
    #                                                                                        select_by_loss=True, grc = False, label="Loss Only")
    # acc_freeze_1, comm_freeze_1, flops_freeze_1, model_size_freeze_1, inference_time_freeze_1 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                                            freeze=True, freeze_layers = 1, prune=False,
    #                                                                                                            select_by_loss=True, grc = False, label="Loss + Freeze 1 Layer")
    # acc_freeze_2, comm_freeze_2, flops_freeze_2, model_size_freeze_2, inference_time_freeze_2 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                                            freeze=True, freeze_layers = 2, prune=False,
    #                                                                                                            select_by_loss=True, grc = False, label="Loss + Freeze 2 Layers")
    # acc_prune_30, comm_prune_30, flops_prune_30, model_size_prune_30, inference_time_freeze_30 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                                             freeze=False, prune=True, prune_amount =0.3,
    #                                                                                                             select_by_loss=True, grc = False, label="Loss + Prune 0.3")
    # acc_prune_60, comm_prune_60, flops_prune_60, model_size_prune_60, inference_time_freeze_60 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                                             freeze=False, prune=True, prune_amount =0.6,
    #                                                                                                             select_by_loss=True, grc = False, label="Loss + Prune 0.6")

    acc_grc, comm_grc, flops_grc, time_grc = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                      freeze=False, prune=False,
                                                                                      select_by_loss=False, grc = True, label="GRA Only")
    acc_grc_freeze_1, comm_grc_freeze_1, flops_grc_freeze_1, time_grc_freeze_1 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                                                                   freeze=True, prune=False,
                                                                                                                                   select_by_loss=False, grc = True, label="GRA + Freeze 1 Layer")
    acc_grc_freeze_2, comm_grc_freeze_2, flops_grc_freeze_2, time_grc_freeze_2 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                                                                   freeze=True, freeze_layers = 2, prune=False,
                                                                                                                                   select_by_loss=False, grc = True, label="GRA + Freeze 2 Layers")
    acc_grc_prune_30, comm_grc_prune_30, flops_grc_prune_30, time_grc_prune_30 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                                                                    freeze=False, prune=True, prune_amount =0.3,
                                                                                                                                   select_by_loss=False, grc = True, label="GRA + Prune 0.3")
    acc_grc_prune_60, comm_grc_prune_60, flops_grc_prune_60, time_grc_prune_60 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                                                                    freeze=False, prune=True, prune_amount =0.6,
                                                                                                                                   select_by_loss=False, grc = True, label="GRA + Prune 0.6")
    # ✅ 定义标签与数据
    # labels = [
    #     "Loss Only",
    #     "Loss + Freeze 1 Layer",
    #     "Loss + Freeze 2 Layers",
    #     "Loss + Prune 30%",
    #     "Loss + Prune 60%",
    # ]

    # flops_values = [
    #     flops_loss,
    #     flops_freeze_1,
    #     flops_freeze_2,
    #     flops_prune_30,
    #     flops_prune_60,
    # ]

    # model_size_values = [
    #     model_size_loss,
    #     model_size_freeze_1,
    #     model_size_freeze_2,
    #     model_size_prune_30,
    #     model_size_prune_60,
    # ]

    # inference_time_values = [
    #     inference_time_loss,
    #     inference_time_freeze_1,
    #     inference_time_freeze_2,
    #     inference_time_freeze_30,
    #     inference_time_freeze_60,
    # ]

    labels_grc = [
        "GRA Only",
        "GRA + Freeze 1 Layer",
        "GRA + Freeze 2 Layers",
        "GRA + Prune 30%",
        "GRA + Prune 60%",
    ]

    flops_values_grc = [
        flops_grc,
        flops_grc_freeze_1,
        flops_grc_freeze_2,
        flops_grc_prune_30,
        flops_grc_prune_60,
    ]

    time_values_grc = [
        time_grc,
        time_grc_freeze_1,
        time_grc_freeze_2,
        time_grc_prune_30,
        time_grc_prune_60,
    ]

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    bar_colors = ['skyblue', 'lightgreen']
    metrics = ['FLOPs (TFLOPs)', 'Time (s)']
    titles = ['FLOPs Comparison', 'Time Comparison']

    # # Loss-based
    # loss_values = [flops_values, model_size_values, inference_time_values]
    # GRC-based
    grc_values = [flops_values_grc, time_values_grc]

    # 绘制 Loss-based (上排)
    # for i in range(3):
    #     bars = axes[0, i].bar(labels, loss_values[i], color=bar_colors[i])
    #     for bar in bars:
    #         height = bar.get_height()
    #         axes[0, i].text(bar.get_x() + bar.get_width()/2.0, height, f"{height:.4f}", ha='center', va='bottom')
    #     axes[0, i].set_title(titles[i] + " (Loss-based)")
    #     axes[0, i].set_ylabel(metrics[i])
    #     axes[0, i].tick_params(axis='x', rotation=20)
    #     axes[0, i].grid(axis='y', linestyle='--', alpha=0.7)

    # 绘制 GRC-based (下排)
    for i in range(2):
        bars = axes[i].bar(labels_grc, grc_values[i], color=bar_colors[i])
        for bar in bars:
            height = bar.get_height()
            axes[i].text(bar.get_x() + bar.get_width()/2.0, height, f"{height:.4f}", ha='center', va='bottom')
        axes[i].set_title(titles[i] + " (GRA-based)")
        axes[i].set_ylabel(metrics[i])
        axes[i].tick_params(axis='x', rotation=20)
        axes[i].grid(axis='y', linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.show()

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    # ① Loss-based Accuracy Over Rounds
    # axes[0, 0].plot(acc_loss, label="Loss Only", marker='o')
    # axes[0, 0].plot(acc_freeze_1, label="Freeze 1 Layer", marker='^')
    # axes[0, 0].plot(acc_freeze_2, label="Freeze 2 Layers", marker='v')
    # axes[0, 0].plot(acc_prune_30, label="Prune 30%", marker='s')
    # axes[0, 0].plot(acc_prune_60, label="Prune 60%", marker='D')
    # axes[0, 0].set_title("Accuracy Over Rounds (Loss)")
    # axes[0, 0].set_xlabel("Rounds")
    # axes[0, 0].set_ylabel("Accuracy (%)")
    # axes[0, 0].legend()
    # axes[0, 0].grid(True)

    # ② GRC-based Accuracy Over Rounds
    axes[0].plot(acc_grc, label="GRA Only", marker='o')
    axes[0].plot(acc_grc_freeze_1, label="GRA + Freeze 1 Layer", marker='^')
    axes[0].plot(acc_grc_freeze_2, label="GRA + Freeze 2 Layers", marker='v')
    axes[0].plot(acc_grc_prune_30, label="GRA + Prune 30%", marker='s')
    axes[0].plot(acc_grc_prune_60, label="GRA + Prune 60%", marker='D')
    axes[0].set_title("Accuracy Over Rounds (GRA)")
    axes[0].set_xlabel("Rounds")
    axes[0].set_ylabel("Accuracy (%)")
    axes[0].legend()
    axes[0].grid(True)

    # # ③ Loss-based Accuracy vs Communication
    # axes[1, 0].plot(comm_loss, acc_loss, label="Loss Only", marker='o')
    # axes[1, 0].plot(comm_freeze_1, acc_freeze_1, label="Freeze 1 Layer", marker='^')
    # axes[1, 0].plot(comm_freeze_2, acc_freeze_2, label="Freeze 2 Layers", marker='v')
    # axes[1, 0].plot(comm_prune_30, acc_prune_30, label="Prune 30%", marker='s')
    # axes[1, 0].plot(comm_prune_60, acc_prune_60, label="Prune 60%", marker='D')
    # axes[1, 0].set_title("Accuracy vs Communication (Loss)")
    # axes[1, 0].set_xlabel("Comm Count")
    # axes[1, 0].set_ylabel("Accuracy (%)")
    # axes[1, 0].legend()
    # axes[1, 0].grid(True)

    # ④ GRC-based Accuracy vs Communication
    axes[1].plot(comm_grc, acc_grc, label="GRA Only", marker='o')
    axes[1].plot(comm_grc_freeze_1, acc_grc_freeze_1, label="GRA + Freeze 1 Layer", marker='^')
    axes[1].plot(comm_grc_freeze_2, acc_grc_freeze_2, label="GRA + Freeze 2 Layers", marker='v')
    axes[1].plot(comm_grc_prune_30, acc_grc_prune_30, label="GRA + Prune 30%", marker='s')
    axes[1].plot(comm_grc_prune_60, acc_grc_prune_60, label="GRA + Prune 60%", marker='D')
    axes[1].set_title("Accuracy vs Communication (GRA)")
    axes[1].set_xlabel("Comm Count")
    axes[1].set_ylabel("Accuracy (%)")
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main4()