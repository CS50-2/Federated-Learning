# -*- coding: utf-8 -*-
"""GRA with freeze and prune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ukIrbbcccAVDU1ESIFiL1I4WbEYBevgU
"""

# -*- coding: utf-8 -*-
"""Federated Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import copy
import os
import matplotlib.pyplot as plt
import csv
import pandas as pd
from datetime import datetime
import time
! pip install fvcore
from fvcore.nn import FlopCountAnalysis
import torch.nn.functional as F
import torch.nn.utils.prune as prune

# å®šä¹‰ MLP æ¨¡å‹
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)  # ç¬¬ä¸€å±‚ï¼Œè¾“å…¥ç»´åº¦ 784 -> 200
        self.fc2 = nn.Linear(200, 200)  # ç¬¬äºŒå±‚ï¼Œ200 -> 200
        self.fc3 = nn.Linear(200, 10)  # è¾“å‡ºå±‚ï¼Œ200 -> 10
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)  # å±•å¹³è¾“å…¥ (batch_size, 1, 28, 28) -> (batch_size, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # ç›´æ¥è¾“å‡ºï¼Œä¸ä½¿ç”¨ Softmaxï¼ˆå› ä¸º PyTorch çš„ CrossEntropyLoss é‡Œå·²ç»åŒ…å«äº†ï¼‰
        return x


# åŠ è½½ MNIST æ•°æ®é›†
def load_mnist_data(data_path="./data"):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
    else:
        print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    # visualize_mnist_samples(train_data)
    return train_data, test_data


# æ˜¾ç¤ºæ•°æ®é›†ç¤ºä¾‹å›¾ç‰‡
def visualize_mnist_samples(dataset, num_samples=10):
    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 1.2, 1.5))
    for i in range(num_samples):
        img, label = dataset[i]
        axes[i].imshow(img.squeeze(), cmap="gray")
        axes[i].set_title(label)
        axes[i].axis("off")
    plt.show()


def split_data_by_label(dataset, num_clients=10):
    """
    æ‰‹åŠ¨åˆ’åˆ†æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å« 10 ä¸ªç±»åˆ«ï¼Œå¹¶è‡ªå®šä¹‰æ ·æœ¬æ•°é‡ã€‚
    :param dataset: åŸå§‹æ•°æ®é›†ï¼ˆå¦‚ MNISTï¼‰
    :param num_clients: å®¢æˆ·ç«¯æ€»æ•°
    :return: (å®¢æˆ·ç«¯æ•°æ®é›†, å®¢æˆ·ç«¯æ•°æ®å¤§å°)
    """
    #æ‰‹åŠ¨åˆ’åˆ†çš„æ ·æœ¬æ•°é‡ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯ 10 ä¸ªç±»åˆ«çš„æ•°æ®é‡ï¼‰
    # client_data_sizes = {
    #     0: {0: 600, 1: 700, 2: 600, 3: 600, 4: 500, 5: 500, 6: 100, 7: 100, 8: 100, 9: 100},
    #     1: {0: 700, 1: 600, 2: 600, 3: 600, 4: 500, 5: 100, 6: 100, 7: 100, 8: 100, 9: 600},
    #     2: {0: 500, 1: 600, 2: 700, 3: 600, 4: 100, 5: 100, 6: 100, 7: 100, 8: 600, 9: 500},
    #     3: {0: 600, 1: 600, 2: 500, 3: 100, 4: 100, 5: 100, 6: 100, 7: 500, 8: 500, 9: 700},
    #     4: {0: 600, 1: 500, 2: 100, 3: 100, 4: 100, 5: 100, 6: 600, 7: 700, 8: 500, 9: 500},
    #     5: {0: 500, 1: 100, 2: 100, 3: 100, 4: 100, 5: 600, 6: 500, 7: 600, 8: 700, 9: 600},
    #     6: {0: 100, 1: 100, 2: 100, 3: 100, 4: 700, 5: 500, 6: 600, 7: 500, 8: 500, 9: 600},
    #     7: {0: 100, 1: 100, 2: 100, 3: 600, 4: 500, 5: 600, 6: 500, 7: 600, 8: 500, 9: 100},
    #     8: {0: 100, 1: 100, 2: 500, 3: 500, 4: 600, 5: 500, 6: 600, 7: 500, 8: 100, 9: 100},
    #     9: {0: 100, 1: 700, 2: 600, 3: 600, 4: 600, 5: 500, 6: 600, 7: 100, 8: 100, 9: 100}
    # }
    client_data_sizes = {
        0: {0: 600},
        1: {1: 700},
        2: {2: 500},
        3: {3: 600},
        4: {4: 600},
        5: {5: 500},
        6: {6: 100},
        7: {7: 100},
        8: {8: 100},
        9: {9: 100}
    }



    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°æ®ç´¢å¼•
    label_to_indices = {i: [] for i in range(10)}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç´¢å¼•
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    # åˆå§‹åŒ–å®¢æˆ·ç«¯æ•°æ®å­˜å‚¨
    client_data_subsets = {}
    client_actual_sizes = {i: {label: 0 for label in range(10)} for i in range(num_clients)}  # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡

    # éå†æ¯ä¸ªå®¢æˆ·ç«¯ï¼Œä¸ºå…¶åˆ†é…æŒ‡å®šç±»åˆ«çš„æ•°æ®
    for client_id, label_info in client_data_sizes.items():
        selected_indices = []  # ä¸´æ—¶å­˜å‚¨è¯¥å®¢æˆ·ç«¯æ‰€æœ‰é€‰ä¸­çš„ç´¢å¼•
        for label, size in label_info.items():
            # ç¡®ä¿ä¸è¶…å‡ºç±»åˆ«æ•°æ®é›†å®é™…å¤§å°
            available_size = len(label_to_indices[label])
            sample_size = min(available_size, size)

            if sample_size < size:
                print(f"âš ï¸ è­¦å‘Šï¼šç±»åˆ« {label} çš„æ•°æ®ä¸è¶³ï¼Œå®¢æˆ·ç«¯ {client_id} åªèƒ½è·å– {sample_size} æ¡æ ·æœ¬ï¼ˆéœ€æ±‚ {size} æ¡ï¼‰")

            # ä»è¯¥ç±»åˆ«ä¸­éšæœºæŠ½å–æ ·æœ¬
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)

            # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡
            client_actual_sizes[client_id][label] = sample_size

        # åˆ›å»º PyTorch Subset
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)

    # æ‰“å°æ¯ä¸ªå®¢æˆ·ç«¯çš„å®é™…åˆ†é…æ•°æ®é‡
    print("\nğŸ“Š æ¯ä¸ªå®¢æˆ·ç«¯å®é™…æ•°æ®åˆ†å¸ƒ:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"å®¢æˆ·ç«¯ {client_id}: {label_sizes}")

    return client_data_subsets, client_actual_sizes


# æœ¬åœ°è®­ç»ƒå‡½æ•°
def local_train(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=lr
        )
    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()

# å®æ—¶è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„lossç”¨åšFedgra
def local_train_fedgra_loss(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=lr
        )
    model.train()
    loss_sq_sum = 0.0

    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            loss_sq_sum += loss.item() ** 2

    h_i = loss_sq_sum #æ”¾å¤§è®­ç»ƒè¿‡ç¨‹ä¸­ loss çš„ç´¯ç§¯ç¨‹åº¦ï¼Œä»è€Œå¢å¼ºå®¢æˆ·ç«¯ä¹‹é—´çš„åŒºåˆ†åº¦ã€‚
    return model, h_i

#  è”é‚¦å¹³å‡èšåˆå‡½æ•°
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    subkey = [sublist[0] for sublist in client_state_dicts]
    new_client_sizes = dict(([(key, client_sizes[key]) for key in subkey]))
    total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())  # è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®æ€»é‡
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
            for (client_id, client_state) in client_state_dicts
        )
    global_model.load_state_dict(global_dict)
    return global_model


# è¯„ä¼°æ¨¡å‹
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy

# ç†µæƒæ³•å®ç°

def entropy_weight(l):
    l = np.array(l)

    # Step 1: Min-Max å½’ä¸€åŒ–ï¼ˆé¿å…è´Ÿå€¼å’Œçˆ†ç‚¸ï¼‰
    X_norm = (l - l.min(axis=1, keepdims=True)) / (l.max(axis=1, keepdims=True) - l.min(axis=1, keepdims=True) + 1e-12)

    # Step 2: è½¬ä¸ºæ¦‚ç‡çŸ©é˜µ P_ki
    P = X_norm / (X_norm.sum(axis=1, keepdims=True) + 1e-12)

    # Step 3: è®¡ç®—ç†µ
    K = 1 / np.log(X_norm.shape[1])
    E = -K * np.sum(P * np.log(P + 1e-12), axis=1)  # shape: (2,)

    # Step 4: è®¡ç®—ä¿¡æ¯æ•ˆç”¨å€¼ & æƒé‡
    d = 1 - E
    weights = d / np.sum(d)
    return weights.tolist()


# ç°è‰²å…³è”åº¦å®ç°
def calculate_GRC(global_model, client_models, client_losses):
    """
    è®¡ç®— GRC åˆ†æ•° + ç†µæƒæ³•æƒé‡
    ä¿®æ­£ï¼š
      - æ˜ å°„é¡ºåºé”™è¯¯
      - ç†µæƒæ³•ä½¿ç”¨é”™è¯¯æŒ‡æ ‡
    """
    # æ­£ç¡®å†™æ³•ï¼šä½¿ç”¨æ•´ä½“å‚æ•°å‘é‡è®¡ç®— L2 èŒƒæ•°ï¼ˆç¬¦åˆæ–‡çŒ®ï¼‰
    param_diffs = []
    for model in client_models:
        global_vec = torch.nn.utils.parameters_to_vector(global_model.parameters()).detach()
        local_vec = torch.nn.utils.parameters_to_vector(model.parameters()).detach()
        diff = torch.norm(local_vec - global_vec).item()
        param_diffs.append(diff)

    # 2. æ˜ å°„åŸå§‹æŒ‡æ ‡åˆ° [0, 1] åŒºé—´ï¼ˆä¸ºç†µæƒæ³•å‡†å¤‡ï¼‰
    def map_sequence_loss(sequence):
        max_val, min_val = max(sequence), min(sequence)
        denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
        return [(max_val - x) / denom for x in sequence]  # è¶Šå°è¶Šå¥½ï¼Œè´Ÿç›¸å…³

    def map_sequence_diff(sequence):
        max_val, min_val = max(sequence), min(sequence)
        denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
        return [(x - min_val) / denom for x in sequence]  # è¶Šå¤§è¶Šå¥½ï¼Œæ­£ç›¸å…³

    # ç”¨äº GRC çš„æ˜ å°„
    mapped_losses = map_sequence_loss(client_losses)
    mapped_diffs = map_sequence_diff(param_diffs)

    # 3. ç†µæƒæ³•è®¡ç®—æƒé‡ï¼ˆæ ¹æ®æ˜ å°„å€¼ï¼Œé GRCï¼‰
    grc_metrics = np.vstack([mapped_losses, mapped_diffs])  # shape: (2, n_clients)
    weights = entropy_weight(grc_metrics)  # w_loss, w_diff

    # 4. è®¡ç®— GRC åˆ†æ•°ï¼ˆÎ¾kiï¼‰ï¼Œå‚è€ƒå€¼ä¸º 1
    ref_loss, ref_diff = 1.0, 1.0
    delta_losses = [abs(x - ref_loss) for x in mapped_losses]
    delta_diffs = [abs(x - ref_diff) for x in mapped_diffs]
    all_deltas = delta_losses + delta_diffs
    max_delta, min_delta = max(all_deltas), min(all_deltas)

    grc_losses = []
    grc_diffs = []
    rho = 0.5  # åŒºåˆ†åº¦å› å­
    for d_loss, d_diff in zip(delta_losses, delta_diffs):
        grc_loss = (min_delta + rho * max_delta) / (d_loss + rho * max_delta)
        grc_diff = (min_delta + rho * max_delta) / (d_diff + rho * max_delta)
        grc_losses.append(grc_loss)
        grc_diffs.append(grc_diff)

    # 5. åŠ æƒæ±‚å’Œå¾—åˆ°æœ€ç»ˆ GRC åˆ†æ•°
    grc_losses = np.array(grc_losses)
    grc_diffs = np.array(grc_diffs)
    weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]

    # è°ƒè¯•ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯çš„ lossã€diffã€å¾—åˆ†ï¼‰
    print("\n GRCå¾—åˆ†]")
    for i in range(len(client_models)):
        print(f"Client {i} | loss: {client_losses[i]:.4f}, diff: {param_diffs[i]:.4f}, "
              f"mapped_loss: {mapped_losses[i]:.4f}, mapped_diff: {mapped_diffs[i]:.4f}, "
              f"GRC: {weighted_score[i]:.4f}")
    print(f"ç†µæƒæ³•æƒé‡: w_loss = {weights[0]:.4f}, w_diff = {weights[1]:.4f}")

    return weighted_score, weights

def compute_model_flops(
    model, input_shape=(1, 1, 28, 28), batch_size=1, num_samples=1, epochs=1, backward=False, verbose=False ,freeze = False, prune = False
):
    """
    è®¡ç®—è®­ç»ƒé˜¶æ®µçš„æ€» FLOPsï¼ˆä¼°ç®—ï¼‰
    :param model: PyTorch æ¨¡å‹
    :param input_shape: å•ä¸ªæ ·æœ¬çš„è¾“å…¥ shapeï¼ˆå¦‚ MNIST ä¸º (1, 28, 28)ï¼‰
    :param batch_size: æ¯æ¬¡è¾“å…¥çš„æ ·æœ¬æ•°
    :param num_samples: è¯¥å®¢æˆ·ç«¯ä¸€å…±å¤šå°‘è®­ç»ƒæ ·æœ¬
    :param epochs: æ¯è½®è®­ç»ƒå‡ ä¸ª epoch
    :param backward_factor: backward FLOPs æ˜¯ forward çš„å‡ å€ï¼ˆä¸€èˆ¬æ˜¯ 2~3ï¼‰
    :param verbose: æ˜¯å¦æ‰“å°æ¯å±‚ FLOPs
    :return: ä¼°ç®—æ€» FLOPs
    """
    model.eval()
    dummy_input = torch.randn((batch_size, *input_shape[1:]))  # æ³¨æ„æ‹†è§£ shape
    flops_analyzer = FlopCountAnalysis(model, dummy_input)

    if backward:
      if freeze:
        backward_factor = 1.42
      if prune:
        backward_factor = 1.7
      else:
        backward_factor = 3
    else:
      backward_factor = 0

    if verbose:
        print(flops_analyzer.by_module())  # æ‰“å°æ¯å±‚ FLOPs
        print(f"ğŸ“Š å•ä¸ª batch çš„ forward FLOPs: {flops_analyzer.total() / 1e6:.2f} MFLOPs")

    # å•ä¸ª batch çš„ forward FLOPs
    forward_flops_per_batch = flops_analyzer.total()

    # è®­ç»ƒ FLOPs â‰ˆ (forward + backward) Ã— æ‰¹æ¬¡æ•° Ã— epoch
    num_batches = int(np.ceil(num_samples / batch_size))
    total_training_flops = forward_flops_per_batch * (1 + backward_factor) * num_batches * epochs

    return total_training_flops

def apply_structured_pruning(model, amount=0.3):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            torch.nn.utils.prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)
            torch.nn.utils.prune.remove(module, 'weight')  # ğŸ”¥ ç§»é™¤æ©ç ï¼Œå®é™…ä¿®æ”¹æ¨¡å‹ç»“æ„
    return model

def structured_prune_layer(model, layer_name='fc2', prune_ratio=0.5, criterion='l1'):
    """
    é€šç”¨ç»“æ„åŒ–å‰ªæå‡½æ•°ï¼šå¯¹ fc1, fc2, fc3 ä¸­ä»»æ„å±‚æŒ‰æ¯”ä¾‹å‰ªæï¼Œå¹¶è‡ªåŠ¨ä¿®æ”¹ç›¸åº”ä¸Šä¸‹æ¸¸å±‚ç»“æ„ã€‚
    """
    # å–å‡ºç›®æ ‡å±‚
    layer = getattr(model, layer_name)
    weight = layer.weight.data  # shape: [out_features, in_features]
    bias = layer.bias.data

    # è®¡ç®—ç¥ç»å…ƒé‡è¦æ€§ï¼ˆæŒ‰è¾“å‡ºç¥ç»å…ƒç»´åº¦ï¼‰
    if criterion == 'l1':
        scores = weight.abs().sum(dim=1)
    elif criterion == 'l2':
        scores = torch.norm(weight, p=2, dim=1)
    else:
        raise ValueError("Unsupported criterion. Use 'l1' or 'l2'.")

    # å†³å®šè¦ä¿ç•™çš„ç¥ç»å…ƒç´¢å¼•
    total_neurons = weight.shape[0]
    keep_num = max(1, int(total_neurons * prune_ratio))
    _, keep_indices = torch.topk(scores, keep_num)

    # æ„é€ æ–°çš„å±‚
    new_layer = nn.Linear(weight.shape[1], keep_num)
    new_layer.weight.data = weight[keep_indices]
    new_layer.bias.data = bias[keep_indices]
    setattr(model, layer_name, new_layer)

    # å¤„ç†ä¸‹æ¸¸å±‚æˆ–ä¸Šæ¸¸å±‚çš„è¿é€šæ€§
    if layer_name == 'fc1':
        # fc2 çš„è¾“å…¥éœ€è¦è£å‰ªåˆ—
        fc2 = model.fc2
        new_fc2 = nn.Linear(keep_num, fc2.out_features)
        new_fc2.weight.data = fc2.weight.data[:, keep_indices]
        new_fc2.bias.data = fc2.bias.data
        model.fc2 = new_fc2

    elif layer_name == 'fc2':
        # fc3 çš„è¾“å…¥éœ€è¦è£å‰ªåˆ—
        fc3 = model.fc3
        new_fc3 = nn.Linear(keep_num, fc3.out_features)
        new_fc3.weight.data = fc3.weight.data[:, keep_indices]
        new_fc3.bias.data = fc3.bias.data
        model.fc3 = new_fc3

    elif layer_name == 'fc3':
        # fc3 æ˜¯è¾“å‡ºå±‚ï¼Œä¸å½±å“å…¶ä»–ç»“æ„
        pass

    else:
        raise ValueError("Only supports 'fc1', 'fc2', or 'fc3'")

    return model

def apply_unstructured_pruning(model, amount=0.3):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=amount)
    return model

def apply_freezing(model):
    for name, param in model.named_parameters():
        if 'weight' in name or 'bias' in name:
            param.requires_grad = False
    return model

def apply_partial_freezing(model, freeze_layers=1):
    """
    å†»ç»“æ¨¡å‹å‰ freeze_layers ä¸ª Linear å±‚çš„æƒé‡å’Œåç½®ã€‚
    """
    count = 0
    for module in model.modules():
        if isinstance(module, nn.Linear):
            for name, param in module.named_parameters():
                param.requires_grad = False
            count += 1
            if count >= freeze_layers:
                break
    return model

# å®¢æˆ·ç«¯é€‰æ‹©å™¨
def select_clients(client_loaders, use_all_clients=False, num_select=None,
                   select_by_loss=False, global_model=None, grc=False,
                   fairness_tracker=None, prune=False, prune_amount=0.3, freeze=False, freeze_layers = 1,):
    total_flops = 0
    epochs = 1
    total_time = 0
    if grc:  # ä½¿ç”¨ GRC é€‰æ‹©å®¢æˆ·ç«¯
        client_models = []
        # 1. è®­ç»ƒæœ¬åœ°æ¨¡å‹å¹¶è®¡ç®—æŸå¤±
        client_losses = []
        for client_id, client_loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # åŒæ­¥å…¨å±€æ¨¡å‹
            if prune:
                local_model = structured_prune_layer(local_model, prune_ratio=prune_amount)
            if freeze:
                local_model = apply_partial_freezing(local_model, freeze_layers=freeze_layers)

            # æ¯è½®éƒ½é‡æ–°ä¼°è®¡ä¸€æ¬¡ FLOPsï¼ˆé¿å…æ¨¡å‹å˜ç»“æ„æ—¶ä¸æ›´æ–°ï¼‰
            sample_flops = compute_model_flops(local_model, input_shape=(1, 1, 28, 28), prune = prune ,freeze = freeze)
            n_samples = len(client_loader.dataset)
            total_flops += sample_flops * n_samples * epochs

            # æ¨¡å‹è®­ç»ƒ&è®¡ç®—æ—¶é—´
            client_pre_models = copy.deepcopy(local_model)
            start_time = time.time()
            trained_model, h_i = local_train_fedgra_loss(local_model, client_loader, epochs=5, lr=0.01)
            end_time = time.time()
            total_time += (end_time - start_time)
            client_models.append(trained_model)
            client_losses.append(h_i)
        print(f"ğŸ“Š æœ¬è½®å®¢æˆ·ç«¯é€‰æ‹©é˜¶æ®µä¼°ç®—æ€» FLOPs: {total_flops / 1e12:.4f} TFLOPs")

        # 2. è®¡ç®— GRC åˆ†æ•°
        grc_scores, grc_weights = calculate_GRC(client_pre_models, client_models, client_losses)
        select_clients.latest_weights = grc_weights  # è®°å½•æƒé‡

        # 3. æŒ‰ GRC åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼ŒGRCè¶Šé«˜è¡¨ç¤ºè¶Šå¥½ï¼‰
        client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
        client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # é™åºæ’åº

        # 4. é€‰æ‹© GRC æœ€é«˜çš„å‰ num_select ä¸ªå®¢æˆ·ç«¯
        selected_clients = [client_id for client_id, _ in client_grc_pairs[:num_select]]
        return selected_clients, total_flops, total_time

    # å…¶ä½™é€‰æ‹©é€»è¾‘ä¿æŒä¸å˜

    if use_all_clients is True:
        print("Selecting all clients")
        return list(client_loaders.keys())

    if num_select is None:
        raise ValueError("If use_all_clients=False, num_select cannot be None!")

    if select_by_loss and global_model:
        client_losses = {}

        for client_id, loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            if prune:
                local_model = structured_prune_layer(local_model, prune_ratio = prune_amount)
            if freeze:
                local_model = apply_partial_freezing(local_model, freeze_layers = freeze_layers)

            # æ¯è½®éƒ½é‡æ–°ä¼°è®¡ä¸€æ¬¡ FLOPsï¼ˆé¿å…æ¨¡å‹å˜ç»“æ„æ—¶ä¸æ›´æ–°ï¼‰
            sample_flops = compute_model_flops(local_model, input_shape=(1, 1, 28, 28), prune = prune ,freeze = freeze)
            n_samples = len(loader.dataset)
            total_flops += sample_flops * n_samples * epochs

            # æ¨¡å‹è®­ç»ƒ&è®¡ç®—æ¨ç†æ—¶é—´
            start_infer = time.time()
            local_train(local_model, loader, epochs=5, lr=0.01)
            loss, _ = evaluate(local_model, loader)
            end_infer = time.time()
            total_time += (end_infer - start_infer)
            client_losses[client_id] = loss
        selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
        print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
    else:
        selected_clients = random.sample(list(client_loaders.keys()), num_select)
        print(f"Randomly selected {num_select} clients: {selected_clients}")

    return selected_clients, total_flops, total_time


def update_communication_counts(communication_counts, selected_clients, event):
    """
    å®¢æˆ·ç«¯é€šä¿¡è®¡æ•°
    - event='receive' è¡¨ç¤ºå®¢æˆ·ç«¯æ¥æ”¶åˆ°å…¨å±€æ¨¡å‹
    - event='send' è¡¨ç¤ºå®¢æˆ·ç«¯ä¸Šä¼ æœ¬åœ°æ¨¡å‹
    - event='full_round' ä»…åœ¨å®¢æˆ·ç«¯å®Œæˆå®Œæ•´æ”¶å‘æ—¶å¢åŠ 
    """
    for client_id in selected_clients:
        communication_counts[client_id][event] += 1

        # ä»…å½“å®¢æˆ·ç«¯å®Œæˆä¸€æ¬¡å®Œæ•´çš„ send å’Œ receive æ—¶å¢åŠ  full_round
        if event == "send" and communication_counts[client_id]['receive'] > 0:
            communication_counts[client_id]['full_round'] += 1


def main():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # åŠ è½½ MNIST æ•°æ®é›†
    train_data, test_data = load_mnist_data()

    # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å«å¤šä¸ªç±»åˆ«
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    global_model = MLPModel()
    global_accuracies = []  # è®°å½•æ¯è½®å…¨å±€æ¨¡å‹çš„æµ‹è¯•é›†å‡†ç¡®ç‡
    total_communication_counts = []  # è®°å½•æ¯è½®å®¢æˆ·ç«¯é€šä¿¡æ¬¡æ•°
    rounds = 100  # è”é‚¦å­¦ä¹ è½®æ•°
    use_all_clients = False  # æ˜¯å¦è¿›è¡Œå®¢æˆ·ç«¯é€‰æ‹©
    num_selected_clients = 2  # æ¯è½®é€‰æ‹©å®¢æˆ·ç«¯è®­ç»ƒæ•°é‡
    use_loss_based_selection = False  # æ˜¯å¦æ ¹æ® loss é€‰æ‹©å®¢æˆ·ç«¯
    grc = True

    # åˆå§‹åŒ–é€šä¿¡è®¡æ•°å™¨
    communication_counts = {}
    for client_id in client_loaders.keys():
        communication_counts[client_id] = {
            'send': 0,
            'receive': 0,
            'full_round': 0
        }

    # å®éªŒæ•°æ®å­˜å‚¨ CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}.csv"
    csv_data = []

    for r in range(rounds):
        print(f"\nğŸ”„ ç¬¬ {r + 1} è½®èšåˆ")
        # é€‰æ‹©å®¢æˆ·ç«¯

        selected_clients, round_flops, round_time = select_clients(
            client_loaders,
            use_all_clients=use_all_clients,
            num_select=num_selected_clients,
            select_by_loss=use_loss_based_selection,
            global_model=global_model,
            grc=grc
        )
        # # è®¾ç½®éšæœºé˜»æ–­æŸä¸ªå®¢æˆ·ç«¯çš„æ¥æ”¶è®°å½•ï¼ˆéªŒè¯ç”¨ï¼‰
        # blocked_client = random.choice(selected_clients)
        # print(f" Blocking client {blocked_client} from receiving, skipping the receive event record.")

        # for client_id in selected_clients:
        #     if client_id == blocked_client:
        #         continue  # ç›´æ¥è·³è¿‡ receive è®°å½•
        #     update_communication_counts(communication_counts, [client_id], "receive")

        # è®°å½•å®¢æˆ·ç«¯æ¥æ”¶é€šä¿¡æ¬¡æ•°
        update_communication_counts(communication_counts, selected_clients, "receive")
        client_state_dicts = []

        # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
        for client_id in selected_clients:
            client_loader = client_loaders[client_id]
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # å¤åˆ¶å…¨å±€æ¨¡å‹å‚æ•°
            local_state = local_train(local_model, client_loader, epochs=5, lr=0.01)  # è®­ç»ƒ 1 è½®
            client_state_dicts.append((client_id, local_state))  # å­˜å‚¨ (å®¢æˆ·ç«¯ID, è®­ç»ƒåçš„å‚æ•°)

            update_communication_counts(communication_counts, [client_id], "send")  # è®°å½•å®¢æˆ·ç«¯ä¸ŠæŠ¥é€šä¿¡æ¬¡æ•°

            param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
            print(f"  âœ… å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå®Œæˆ | æ ·æœ¬æ•°é‡: {sum(client_data_sizes[client_id].values())}")
            print(f"  ğŸ“Œ å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å‚æ•°å‡å€¼: {param_mean}")

        # è®¡ç®—æœ¬è½®é€šä¿¡æ¬¡æ•°
        total_send = sum(communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_receive = sum(communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_comm = total_send + total_receive  # æ¯è½®ç‹¬ç«‹çš„æ€»é€šä¿¡æ¬¡æ•°

        # å¦‚æœä¸æ˜¯ç¬¬ä¸€è½®ï¼Œç´¯åŠ å‰ä¸€è½®çš„é€šä¿¡æ¬¡æ•°
        if len(total_communication_counts) > 0:
            total_comm += total_communication_counts[-1]
        total_communication_counts.append(total_comm)

        # èšåˆæ¨¡å‹å‚æ•°
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

        # # è®¡ç®—å…¨å±€æ¨¡å‹å‚æ•°å¹³å‡å€¼
        # global_param_mean = {name: param.mean().item() for name, param in global_model.named_parameters()}
        # print(f"ğŸ”„ è½® {r + 1} ç»“æŸåï¼Œå…¨å±€æ¨¡å‹å‚æ•°å‡å€¼: {global_param_mean}")

        # è¯„ä¼°æ¨¡å‹
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)
        print(f"ğŸ“Š æµ‹è¯•é›†æŸå¤±: {loss:.4f} | æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")

        # è®°å½•æ•°æ®åˆ° CSV
        if grc and hasattr(select_clients, 'latest_weights'):
            w_loss = select_clients.latest_weights[0]
            w_diff = select_clients.latest_weights[1]
            print(f"ğŸ“ˆ Round {r+1} | GRC æƒé‡: w_loss = {w_loss:.4f}, w_diff = {w_diff:.4f}")

        else:
            w_loss = 'NA'
            w_diff = 'NA'

        csv_data.append([
            r + 1,
            accuracy,
            total_comm,
            ",".join(map(str, selected_clients)),
            w_loss,
            w_diff
        ])
        df = pd.DataFrame(csv_data, columns=[
            'Round', 'Accuracy', 'Total communication counts', 'Selected Clients',
            'GRC Weight - Loss', 'GRC Weight - Diff'])
        df.to_csv(csv_filename, index=False)

    # è¾“å‡ºæœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½
    final_loss, final_accuracy = evaluate(global_model, test_loader)
    print(f"\nğŸ¯ Loss of final model test dataset: {final_loss:.4f}")
    print(f"ğŸ¯ Final model test set accuracy: {final_accuracy:.2f}%")

    # è¾“å‡ºé€šä¿¡è®°å½•
    print("\n Client Communication Statistics:")
    for client_id, counts in communication_counts.items():
        print(
            f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")

    # å¯è§†åŒ–å…¨å±€æ¨¡å‹å‡†ç¡®ç‡ vs è½®æ¬¡
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, rounds + 1), global_accuracies, marker='o', linestyle='-', color='b', label="Test Accuracy")
    plt.xlabel("Federated Learning Rounds")
    plt.ylabel("Accuracy")
    plt.title("Test Accuracy Over Federated Learning Rounds")
    plt.legend()
    plt.grid(True)
    plt.show()

    # å¯è§†åŒ–å…¨å±€æ¨¡å‹å‡†ç¡®ç‡ vs å®¢æˆ·ç«¯å®Œæ•´é€šä¿¡æ¬¡æ•°
    plt.figure(figsize=(8, 5))
    plt.plot(total_communication_counts, global_accuracies, marker='s', linestyle='-', color='r',
             label="Test Accuracy vs. Communication")
    plt.xlabel("Total Communication Count per Round")
    plt.ylabel("Accuracy")
    plt.title("Test Accuracy vs. Total Communication")
    plt.legend()
    plt.grid(True)
    plt.show()

# é«˜ Loss è·¨è½®æ¬¡é€‰æ‹©

def main2():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # åŠ è½½æ•°æ®
    train_data, test_data = load_mnist_data()
    client_datasets, client_data_sizes = split_data_by_label(train_data)
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    global_model = MLPModel()
    global_accuracies = []
    total_communication_counts = []
    rounds = 100
    num_selected_clients = 2

    # åˆå§‹åŒ–é€šä¿¡ä¸å®¢æˆ·ç«¯è®°å½•ç»“æ„
    communication_counts = {cid: {'send': 0, 'receive': 0, 'full_round': 0} for cid in client_loaders}
    client_loss_history = {cid: [] for cid in client_loaders}
    local_model_cache = {}   # æ¨¡å‹ç¼“å­˜
    selected_clients = []    # å½“å‰é€‰ä¸­çš„å®¢æˆ·ç«¯ï¼ˆç”¨äºéé€šä¿¡è½®å¤ç”¨ï¼‰

    # å‡†å¤‡ CSV æ–‡ä»¶ä¿å­˜è·¯å¾„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}_main2.csv"
    csv_data = []

    total_comm = 0  # è·¨è½®æ¬¡ç´¯è®¡é€šä¿¡é‡

    for r in range(rounds):
        print(f"\nğŸ”„ ç¬¬ {r + 1} è½®èšåˆ")
        is_comm_round = False

        # åˆ¤æ–­æ˜¯å¦ä¸ºé€šä¿¡è½®
        if r % 3 == 0:
            current_losses = {}
            for cid, loader in client_loaders.items():
                loss, _ = evaluate(global_model, loader)
                current_losses[cid] = loss
                client_loss_history[cid].append(loss)

            # åŠ æƒå†å² lossï¼ˆæ›´åå‘å½“å‰è½®ï¼‰
            weighted_losses = {}
            for cid in client_loaders:
                history = client_loss_history[cid][-3:]
                if len(history) == 3:
                    weights = [0.7, 0.2, 0.1]
                elif len(history) == 2:
                    weights = [0.8, 0.2]
                else:
                    weights = [1.0]
                weighted_losses[cid] = sum(h * w for h, w in zip(history, weights)) / sum(weights)

            # æŒ‰ weighted loss é™åºé€‰æ‹©å®¢æˆ·ç«¯
            selected_clients = sorted(weighted_losses, key=weighted_losses.get, reverse=True)[:num_selected_clients]
            print(f"ğŸ“Œ é€šä¿¡è½® | Selected clients: {selected_clients}")
            update_communication_counts(communication_counts, selected_clients, "receive")
            is_comm_round = True

        else:
            print(f"â³ éé€šä¿¡è½® | å¤ç”¨å®¢æˆ·ç«¯æ¨¡å‹: {selected_clients}")

        client_state_dicts = []

        for cid in selected_clients:
            if is_comm_round:
                loader = client_loaders[cid]
                model = MLPModel()
                model.load_state_dict(global_model.state_dict())
                local_train(model, loader, epochs=1, lr=0.01)
                local_model_cache[cid] = model
                client_state_dicts.append((cid, model.state_dict()))
                update_communication_counts(communication_counts, [cid], "send")
                total_comm += 2  # send + receive
            else:
                if cid in local_model_cache:
                    client_state_dicts.append((cid, local_model_cache[cid].state_dict()))
                else:
                    print(f"âš ï¸ å®¢æˆ·ç«¯ {cid} æ— æ¨¡å‹ç¼“å­˜ï¼Œè·³è¿‡")

        # è®°å½•ç´¯è®¡é€šä¿¡æ¬¡æ•°
        total_communication_counts.append(total_comm)

        # æ¨¡å‹èšåˆ + è¯„ä¼°
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
        loss, acc = evaluate(global_model, test_loader)
        global_accuracies.append(acc)
        print(f"ğŸ“Š æµ‹è¯•é›†æŸå¤±: {loss:.4f} | æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.2f}%")

        # å†™å…¥ CSV
        csv_data.append([r + 1, acc, total_comm, ",".join(map(str, selected_clients)), 'NA', 'NA'])
        df = pd.DataFrame(csv_data, columns=['Round', 'Accuracy', 'Total communication counts',
                                             'Selected Clients', 'GRC Weight - Loss', 'GRC Weight - Diff'])
        df.to_csv(csv_filename, index=False)

    # æœ€ç»ˆè¯„ä¼°
    final_loss, final_acc = evaluate(global_model, test_loader)
    print(f"\nğŸ¯ Final Loss: {final_loss:.4f}")
    print(f"ğŸ¯ Final Accuracy: {final_acc:.2f}%")



# Fedgraè·¨è½®æ¬¡é€‰æ‹©


def main3():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    train_data, test_data = load_mnist_data()
    client_datasets, client_data_sizes = split_data_by_label(train_data)
    client_loaders = {cid: data.DataLoader(dataset, batch_size=32, shuffle=True) for cid, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    global_model = MLPModel()
    global_accuracies = []
    total_communication_counts = []
    rounds = 100
    num_selected_clients = 2

    # åˆå§‹åŒ–é€šä¿¡è®°å½•ã€ç¼“å­˜ã€GRCå†å²
    communication_counts = {cid: {'send': 0, 'receive': 0, 'full_round': 0} for cid in client_loaders}
    local_model_cache = {}  # å­˜å‚¨å®¢æˆ·ç«¯æ¨¡å‹ï¼Œéé€šä¿¡è½®ä½¿ç”¨
    client_grc_history = {cid: [] for cid in client_loaders}  # å­˜å‚¨ GRC å†å²
    selected_clients = []  # æ¯è½®é€‰æ‹©ç»“æœ

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}_main3.csv"
    csv_data = []

    total_comm = 0  # æ€»é€šä¿¡æ¬¡æ•°

    for r in range(rounds):
        print(f"\nğŸ”„ ç¬¬ {r + 1} è½®èšåˆ")
        w_loss, w_diff = 'NA', 'NA'
        round_comm = 0
        is_comm_round = (r % 3 == 0)

        client_state_dicts = []

        if is_comm_round:
            print("ğŸ“¡ é€šä¿¡è½®")

            # æ‰€æœ‰å®¢æˆ·ç«¯éƒ½è¿›è¡Œä¸€æ¬¡æ¨¡å‹è®­ç»ƒå’Œlossè¯„ä¼°ï¼ˆGRCè®¡ç®—ï¼‰
            client_models, client_losses = [], []
            for cid, loader in client_loaders.items():
                model = MLPModel()
                model.load_state_dict(global_model.state_dict())
                local_train(model, loader, epochs=1, lr=0.01)
                client_models.append(model)
                loss, _ = evaluate(model, loader)
                client_losses.append(loss)

            # è®¡ç®— GRC å¾—åˆ†å¹¶å­˜å‚¨
            grc_scores, grc_weights = calculate_GRC(global_model, client_models, client_losses)
            w_loss, w_diff = grc_weights

            for i, cid in enumerate(client_loaders.keys()):
                client_grc_history[cid].append(grc_scores[i])

            # æ ¹æ®å†å² GRC åšåŠ æƒå¹³å‡
            weighted_grc = {}
            for cid in client_loaders:
                history = client_grc_history[cid][-3:]
                if len(history) == 3:
                    weights = [0.7, 0.2, 0.1]
                elif len(history) == 2:
                    weights = [0.8, 0.2]
                else:
                    weights = [1.0]
                weighted_grc[cid] = sum(h * w for h, w in zip(history, weights)) / sum(weights)

            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å®¢æˆ·ç«¯
            selected_clients = sorted(weighted_grc, key=weighted_grc.get, reverse=True)[:num_selected_clients]
            print(f"âœ… Selected Clients: {selected_clients}")

            update_communication_counts(communication_counts, selected_clients, "receive")

            # å¯¹è¢«é€‰ä¸­çš„å®¢æˆ·ç«¯è¿›è¡Œè®­ç»ƒå¹¶ç¼“å­˜æ¨¡å‹
            for cid in selected_clients:
                model = MLPModel()
                model.load_state_dict(global_model.state_dict())
                local_train(model, client_loaders[cid], epochs=1, lr=0.01)
                local_model_cache[cid] = model  # ç¼“å­˜æœ¬åœ°æ¨¡å‹
                client_state_dicts.append((cid, model.state_dict()))
                update_communication_counts(communication_counts, [cid], "send")
                round_comm += 2

        else:
            print(f"â³ éé€šä¿¡è½® | å¤ç”¨æ¨¡å‹: {selected_clients}")
            # éé€šä¿¡è½®ç›´æ¥ä»ç¼“å­˜åŠ è½½æ¨¡å‹
            for cid in selected_clients:
                if cid in local_model_cache:
                    client_state_dicts.append((cid, local_model_cache[cid].state_dict()))
                else:
                    print(f"âš ï¸ ç¼ºå¤±ç¼“å­˜ï¼Œå®¢æˆ·ç«¯ {cid} è¢«è·³è¿‡")

        # ç´¯åŠ é€šä¿¡æ¬¡æ•°
        total_comm += round_comm
        total_communication_counts.append(total_comm)

        # èšåˆå…¨å±€æ¨¡å‹
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

        # æµ‹è¯•å…¨å±€æ¨¡å‹
        loss, acc = evaluate(global_model, test_loader)
        global_accuracies.append(acc)
        print(f"ğŸ“Š Loss: {loss:.4f} | Accuracy: {acc:.2f}%")

        # ä¿å­˜è½®æ¬¡è®°å½•
        csv_data.append([r + 1, acc, total_comm, ",".join(map(str, selected_clients)), w_loss, w_diff])
        df = pd.DataFrame(csv_data, columns=[
            'Round', 'Accuracy', 'Total communication counts',
            'Selected Clients', 'GRC Weight - Loss', 'GRC Weight - Diff'
        ])
        df.to_csv(csv_filename, index=False)

    # æœ€ç»ˆè¯„ä¼°
    final_loss, final_acc = evaluate(global_model, test_loader)
    print(f"\nğŸ¯ Final Loss: {final_loss:.4f}")
    print(f"ğŸ¯ Final Accuracy: {final_acc:.2f}%")

def run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                   freeze=False, freeze_layers = 1, prune=False, prune_amount = 0.3,
                   select_by_loss = False, grc = False, label="",):
    global_model = MLPModel()
    global_accuracies = []
    total_communication_counts = []
    total_tflops = 0
    total_time = 0

    # åˆå§‹åŒ–é€šä¿¡è®¡æ•°å™¨
    communication_counts = {}
    for client_id in client_loaders.keys():
        communication_counts[client_id] = {
            'send': 0,
            'receive': 0,
            'full_round': 0
        }

    # å®éªŒæ•°æ®å­˜å‚¨ CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}.csv"
    csv_data = []

    for r in range(rounds):
        print(f"\nğŸ” [{label}] Round {r + 1}")
        selected_clients, round_flops, round_time = select_clients(
            client_loaders, use_all_clients=False, num_select=2,
            select_by_loss=select_by_loss, global_model=global_model,grc=grc,
            prune=prune, prune_amount = prune_amount, freeze=freeze, freeze_layers=freeze_layers
        )
        print(f"Selected Clients: {selected_clients}")

        update_communication_counts(communication_counts, selected_clients, "receive")
        total_tflops += round_flops / 1e12
        total_time += round_time

        client_state_dicts = []

        # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
        for client_id in selected_clients:
            client_loader = client_loaders[client_id]
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # å¤åˆ¶å…¨å±€æ¨¡å‹å‚æ•°
            local_state = local_train(local_model, client_loader, epochs=5, lr=0.01)  # è®­ç»ƒ 1 è½®
            client_state_dicts.append((client_id, local_state))  # å­˜å‚¨ (å®¢æˆ·ç«¯ID, è®­ç»ƒåçš„å‚æ•°)

            update_communication_counts(communication_counts, [client_id], "send")  # è®°å½•å®¢æˆ·ç«¯ä¸ŠæŠ¥é€šä¿¡æ¬¡æ•°

            param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
            print(f"  âœ… å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå®Œæˆ | æ ·æœ¬æ•°é‡: {sum(client_data_sizes[client_id].values())}")
            print(f"  ğŸ“Œ å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å‚æ•°å‡å€¼: {param_mean}")

        # è®¡ç®—æœ¬è½®é€šä¿¡æ¬¡æ•°
        total_send = sum(communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_receive = sum(communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_comm = total_send + total_receive  # æ¯è½®ç‹¬ç«‹çš„æ€»é€šä¿¡æ¬¡æ•°

        # å¦‚æœä¸æ˜¯ç¬¬ä¸€è½®ï¼Œç´¯åŠ å‰ä¸€è½®çš„é€šä¿¡æ¬¡æ•°
        if len(total_communication_counts) > 0:
            total_comm += total_communication_counts[-1]
        total_communication_counts.append(total_comm)

        # èšåˆ
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)

        # è¾“å‡ºæœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½
        final_loss, final_accuracy = evaluate(global_model, test_loader)
        print(f"ğŸ¯ Final model test set accuracy: {final_accuracy:.2f}%")

        # è¾“å‡ºé€šä¿¡è®°å½•
        print("\n Client Communication Statistics:")
        for client_id, counts in communication_counts.items():
            print(f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")
        # è®°å½•æ•°æ®åˆ° CSV
        if grc and hasattr(select_clients, 'latest_weights'):
            w_loss = select_clients.latest_weights[0]
            w_diff = select_clients.latest_weights[1]
            print(f"ğŸ“ˆ Round {r + 1} | GRC æƒé‡: w_loss = {w_loss:.4f}, w_diff = {w_diff:.4f}")

        else:
            w_loss = 'NA'
            w_diff = 'NA'

        csv_data.append([
            r + 1,
            accuracy,
            total_comm,
            ",".join(map(str, selected_clients)),
            w_loss,
            w_diff
        ])
        df = pd.DataFrame(csv_data, columns=[
            'Round', 'Accuracy', 'Total communication counts', 'Selected Clients',
            'GRC Weight - Loss', 'GRC Weight - Diff'])
        df.to_csv(csv_filename, index=False)

    return global_accuracies, total_communication_counts ,total_tflops * rounds, total_time

def main4():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # åŠ è½½ MNIST æ•°æ®é›†
    train_data, test_data = load_mnist_data()

    # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å«å¤šä¸ªç±»åˆ«
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    rounds = 100  # è”é‚¦å­¦ä¹ è½®æ•°
    use_all_clients = False  # æ˜¯å¦è¿›è¡Œå®¢æˆ·ç«¯é€‰æ‹©
    num_selected_clients = 2  # æ¯è½®é€‰æ‹©å®¢æˆ·ç«¯è®­ç»ƒæ•°é‡
    t_flops = 0
    grc = False
    prune = True
    freeze = False


    # acc_loss, comm_loss, flops_loss, model_size_loss, inference_time_loss = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                        freeze=False, prune=False,
    #                                                                                        select_by_loss=True, grc = False, label="Loss Only")
    # acc_freeze_1, comm_freeze_1, flops_freeze_1, model_size_freeze_1, inference_time_freeze_1 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                                            freeze=True, freeze_layers = 1, prune=False,
    #                                                                                                            select_by_loss=True, grc = False, label="Loss + Freeze 1 Layer")
    # acc_freeze_2, comm_freeze_2, flops_freeze_2, model_size_freeze_2, inference_time_freeze_2 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                                            freeze=True, freeze_layers = 2, prune=False,
    #                                                                                                            select_by_loss=True, grc = False, label="Loss + Freeze 2 Layers")
    # acc_prune_30, comm_prune_30, flops_prune_30, model_size_prune_30, inference_time_freeze_30 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                                             freeze=False, prune=True, prune_amount =0.3,
    #                                                                                                             select_by_loss=True, grc = False, label="Loss + Prune 0.3")
    # acc_prune_60, comm_prune_60, flops_prune_60, model_size_prune_60, inference_time_freeze_60 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
    #                                                                                                             freeze=False, prune=True, prune_amount =0.6,
    #                                                                                                             select_by_loss=True, grc = False, label="Loss + Prune 0.6")

    acc_grc, comm_grc, flops_grc, time_grc = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                      freeze=False, prune=False,
                                                                                      select_by_loss=False, grc = True, label="GRA Only")
    acc_grc_freeze_1, comm_grc_freeze_1, flops_grc_freeze_1, time_grc_freeze_1 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                                                                   freeze=True, prune=False,
                                                                                                                                   select_by_loss=False, grc = True, label="GRA + Freeze 1 Layer")
    acc_grc_freeze_2, comm_grc_freeze_2, flops_grc_freeze_2, time_grc_freeze_2 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                                                                   freeze=True, freeze_layers = 2, prune=False,
                                                                                                                                   select_by_loss=False, grc = True, label="GRA + Freeze 2 Layers")
    acc_grc_prune_30, comm_grc_prune_30, flops_grc_prune_30, time_grc_prune_30 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                                                                    freeze=False, prune=True, prune_amount =0.3,
                                                                                                                                   select_by_loss=False, grc = True, label="GRA + Prune 0.3")
    acc_grc_prune_60, comm_grc_prune_60, flops_grc_prune_60, time_grc_prune_60 = run_experiment(rounds, client_loaders, test_loader, client_data_sizes,
                                                                                                                                    freeze=False, prune=True, prune_amount =0.6,
                                                                                                                                   select_by_loss=False, grc = True, label="GRA + Prune 0.6")
    # âœ… å®šä¹‰æ ‡ç­¾ä¸æ•°æ®
    # labels = [
    #     "Loss Only",
    #     "Loss + Freeze 1 Layer",
    #     "Loss + Freeze 2 Layers",
    #     "Loss + Prune 30%",
    #     "Loss + Prune 60%",
    # ]

    # flops_values = [
    #     flops_loss,
    #     flops_freeze_1,
    #     flops_freeze_2,
    #     flops_prune_30,
    #     flops_prune_60,
    # ]

    # model_size_values = [
    #     model_size_loss,
    #     model_size_freeze_1,
    #     model_size_freeze_2,
    #     model_size_prune_30,
    #     model_size_prune_60,
    # ]

    # inference_time_values = [
    #     inference_time_loss,
    #     inference_time_freeze_1,
    #     inference_time_freeze_2,
    #     inference_time_freeze_30,
    #     inference_time_freeze_60,
    # ]

    labels_grc = [
        "GRA Only",
        "GRA + Freeze 1 Layer",
        "GRA + Freeze 2 Layers",
        "GRA + Prune 30%",
        "GRA + Prune 60%",
    ]

    flops_values_grc = [
        flops_grc,
        flops_grc_freeze_1,
        flops_grc_freeze_2,
        flops_grc_prune_30,
        flops_grc_prune_60,
    ]

    time_values_grc = [
        time_grc,
        time_grc_freeze_1,
        time_grc_freeze_2,
        time_grc_prune_30,
        time_grc_prune_60,
    ]

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    bar_colors = ['skyblue', 'lightgreen']
    metrics = ['FLOPs (TFLOPs)', 'Time (s)']
    titles = ['FLOPs Comparison', 'Time Comparison']

    # # Loss-based
    # loss_values = [flops_values, model_size_values, inference_time_values]
    # GRC-based
    grc_values = [flops_values_grc, time_values_grc]

    # ç»˜åˆ¶ Loss-based (ä¸Šæ’)
    # for i in range(3):
    #     bars = axes[0, i].bar(labels, loss_values[i], color=bar_colors[i])
    #     for bar in bars:
    #         height = bar.get_height()
    #         axes[0, i].text(bar.get_x() + bar.get_width()/2.0, height, f"{height:.4f}", ha='center', va='bottom')
    #     axes[0, i].set_title(titles[i] + " (Loss-based)")
    #     axes[0, i].set_ylabel(metrics[i])
    #     axes[0, i].tick_params(axis='x', rotation=20)
    #     axes[0, i].grid(axis='y', linestyle='--', alpha=0.7)

    # ç»˜åˆ¶ GRC-based (ä¸‹æ’)
    for i in range(2):
        bars = axes[i].bar(labels_grc, grc_values[i], color=bar_colors[i])
        for bar in bars:
            height = bar.get_height()
            axes[i].text(bar.get_x() + bar.get_width()/2.0, height, f"{height:.4f}", ha='center', va='bottom')
        axes[i].set_title(titles[i] + " (GRA-based)")
        axes[i].set_ylabel(metrics[i])
        axes[i].tick_params(axis='x', rotation=20)
        axes[i].grid(axis='y', linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.show()

    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    # â‘  Loss-based Accuracy Over Rounds
    # axes[0, 0].plot(acc_loss, label="Loss Only", marker='o')
    # axes[0, 0].plot(acc_freeze_1, label="Freeze 1 Layer", marker='^')
    # axes[0, 0].plot(acc_freeze_2, label="Freeze 2 Layers", marker='v')
    # axes[0, 0].plot(acc_prune_30, label="Prune 30%", marker='s')
    # axes[0, 0].plot(acc_prune_60, label="Prune 60%", marker='D')
    # axes[0, 0].set_title("Accuracy Over Rounds (Loss)")
    # axes[0, 0].set_xlabel("Rounds")
    # axes[0, 0].set_ylabel("Accuracy (%)")
    # axes[0, 0].legend()
    # axes[0, 0].grid(True)

    # â‘¡ GRC-based Accuracy Over Rounds
    axes[0].plot(acc_grc, label="GRA Only", marker='o')
    axes[0].plot(acc_grc_freeze_1, label="GRA + Freeze 1 Layer", marker='^')
    axes[0].plot(acc_grc_freeze_2, label="GRA + Freeze 2 Layers", marker='v')
    axes[0].plot(acc_grc_prune_30, label="GRA + Prune 30%", marker='s')
    axes[0].plot(acc_grc_prune_60, label="GRA + Prune 60%", marker='D')
    axes[0].set_title("Accuracy Over Rounds (GRA)")
    axes[0].set_xlabel("Rounds")
    axes[0].set_ylabel("Accuracy (%)")
    axes[0].legend()
    axes[0].grid(True)

    # # â‘¢ Loss-based Accuracy vs Communication
    # axes[1, 0].plot(comm_loss, acc_loss, label="Loss Only", marker='o')
    # axes[1, 0].plot(comm_freeze_1, acc_freeze_1, label="Freeze 1 Layer", marker='^')
    # axes[1, 0].plot(comm_freeze_2, acc_freeze_2, label="Freeze 2 Layers", marker='v')
    # axes[1, 0].plot(comm_prune_30, acc_prune_30, label="Prune 30%", marker='s')
    # axes[1, 0].plot(comm_prune_60, acc_prune_60, label="Prune 60%", marker='D')
    # axes[1, 0].set_title("Accuracy vs Communication (Loss)")
    # axes[1, 0].set_xlabel("Comm Count")
    # axes[1, 0].set_ylabel("Accuracy (%)")
    # axes[1, 0].legend()
    # axes[1, 0].grid(True)

    # â‘£ GRC-based Accuracy vs Communication
    axes[1].plot(comm_grc, acc_grc, label="GRA Only", marker='o')
    axes[1].plot(comm_grc_freeze_1, acc_grc_freeze_1, label="GRA + Freeze 1 Layer", marker='^')
    axes[1].plot(comm_grc_freeze_2, acc_grc_freeze_2, label="GRA + Freeze 2 Layers", marker='v')
    axes[1].plot(comm_grc_prune_30, acc_grc_prune_30, label="GRA + Prune 30%", marker='s')
    axes[1].plot(comm_grc_prune_60, acc_grc_prune_60, label="GRA + Prune 60%", marker='D')
    axes[1].set_title("Accuracy vs Communication (GRA)")
    axes[1].set_xlabel("Comm Count")
    axes[1].set_ylabel("Accuracy (%)")
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main4()