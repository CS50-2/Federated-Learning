# -*- coding: utf-8 -*-
"""Federated Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import csv
import pandas as pd
from datetime import datetime


# 定义 MLP 模型
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)  # 第一层，输入维度 784 -> 200
        self.fc2 = nn.Linear(200, 200)  # 第二层，200 -> 200
        self.fc3 = nn.Linear(200, 10)  # 输出层，200 -> 10
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)  # 展平输入 (batch_size, 1, 28, 28) -> (batch_size, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # 直接输出，不使用 Softmax（因为 PyTorch 的 CrossEntropyLoss 里已经包含了）
        return x


# 加载 MNIST 数据集
def load_mnist_data(data_path="./data"):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("✅ MNIST 数据集已存在，跳过下载。")
    else:
        print("⬇️ 正在下载 MNIST 数据集...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    # visualize_mnist_samples(train_data)
    return train_data, test_data


# 显示数据集示例图片
def visualize_mnist_samples(dataset, num_samples=10):
    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 1.2, 1.5))
    for i in range(num_samples):
        img, label = dataset[i]
        axes[i].imshow(img.squeeze(), cmap="gray")
        axes[i].set_title(label)
        axes[i].axis("off")
    plt.show()


# 分割 MNIST 数据，使每个客户端只包含某个数字类别
# def split_data_by_label(dataset):
#     # 自定义每个类别的数据量
#     client_data_sizes = {
#         0: 5000,
#         1: 7000,
#         2: 6000,
#         3: 8000,
#         4: 4000,
#         5: 9000,
#         6: 3000,
#         7: 10000,
#         8: 7500,
#         9: 6500
#     }

#     label_to_indices = {i: [] for i in range(10)}  # 记录每个类别的索引

#     # 收集每个类别的数据索引
#     for idx, (_, label) in enumerate(dataset):
#         label_to_indices[label].append(idx)

#     # 为每个 client 选择对应类别的数据，并裁剪成需要的数量
#     client_datasets = []
#     for label, size in client_data_sizes.items():
#         indices = label_to_indices[label][:size]  # 取前 size 个样本
#         client_datasets.append((label, torch.utils.data.Subset(dataset, indices)))  # 存储 (类别, 数据集)

#     print("📊 客户端数据分布:", client_data_sizes)
#     return client_datasets, client_data_sizes

def split_data_by_label(dataset, num_clients=10):
    """
    手动划分数据集，每个客户端包含 10 个类别，并自定义样本数量。
    :param dataset: 原始数据集（如 MNIST）
    :param num_clients: 客户端总数
    :return: (客户端数据集, 客户端数据大小)
    """
    # 手动划分的样本数量（每个客户端 10 个类别的数据量）
    # client_data_sizes = {
    #     0: {0: 600, 1: 700, 2: 600, 3: 600, 4: 500, 5: 500, 6: 100, 7: 100, 8: 100, 9: 100},
    #     1: {0: 700, 1: 600, 2: 600, 3: 600, 4: 500, 5: 100, 6: 100, 7: 100, 8: 100, 9: 600},
    #     2: {0: 500, 1: 600, 2: 700, 3: 600, 4: 100, 5: 100, 6: 100, 7: 100, 8: 600, 9: 500},
    #     3: {0: 600, 1: 600, 2: 500, 3: 100, 4: 100, 5: 100, 6: 100, 7: 500, 8: 500, 9: 700},
    #     4: {0: 600, 1: 500, 2: 100, 3: 100, 4: 100, 5: 100, 6: 600, 7: 700, 8: 500, 9: 500},
    #     5: {0: 500, 1: 100, 2: 100, 3: 100, 4: 100, 5: 600, 6: 500, 7: 600, 8: 700, 9: 600},
    #     6: {0: 100, 1: 100, 2: 100, 3: 100, 4: 700, 5: 500, 6: 600, 7: 500, 8: 500, 9: 600},
    #     7: {0: 100, 1: 100, 2: 100, 3: 600, 4: 500, 5: 600, 6: 500, 7: 600, 8: 500, 9: 100},
    #     8: {0: 100, 1: 100, 2: 500, 3: 500, 4: 600, 5: 500, 6: 600, 7: 500, 8: 100, 9: 100},
    #     9: {0: 100, 1: 700, 2: 600, 3: 600, 4: 600, 5: 500, 6: 600, 7: 100, 8: 100, 9: 100}
    # }

    client_data_sizes = {
        0: {0: 600},
        1: {1: 700},
        2: {2: 500},
        3: {3: 600},
        4: {4: 600},
        5: {5: 500},
        6: {6: 100},
        7: {7: 100},
        8: {8: 100},
        9: {9: 100}
    }



    # 统计每个类别的数据索引
    label_to_indices = {i: [] for i in range(10)}  # 记录每个类别的索引
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    # 初始化客户端数据存储
    client_data_subsets = {}
    client_actual_sizes = {i: {label: 0 for label in range(10)} for i in range(num_clients)}  # 记录实际分配的数据量

    # 遍历每个客户端，为其分配指定类别的数据
    for client_id, label_info in client_data_sizes.items():
        selected_indices = []  # 临时存储该客户端所有选中的索引
        for label, size in label_info.items():
            # 确保不超出类别数据集实际大小
            available_size = len(label_to_indices[label])
            sample_size = min(available_size, size)

            if sample_size < size:
                print(f"⚠️ 警告：类别 {label} 的数据不足，客户端 {client_id} 只能获取 {sample_size} 条样本（需求 {size} 条）")

            # 从该类别中随机抽取样本
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)

            # 记录实际分配的数据量
            client_actual_sizes[client_id][label] = sample_size

        # 创建 PyTorch Subset
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)

    # 打印每个客户端的实际分配数据量
    print("\n📊 每个客户端实际数据分布:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"客户端 {client_id}: {label_sizes}")

    return client_data_subsets, client_actual_sizes


# 本地训练函数
def local_train(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()


#  联邦平均聚合函数
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    subkey = [sublist[0] for sublist in client_state_dicts]
    new_client_sizes = dict(([(key, client_sizes[key]) for key in subkey]))
    total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())  # 计算所有客户端数据总量
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
            for (client_id, client_state) in client_state_dicts
        )
    global_model.load_state_dict(global_dict)
    return global_model


# 评估模型
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy


def entropy_weight(l):
    weight = []
    for X in l:

        P = X / X.sum(axis=0)

        K = 1 / np.log(len(X))
        E = -K * (P * np.log(P + 1e-12)).sum(axis=0)

        W = (1 - E) / (1 - E).sum()
        weight.append(W)
    return weight


def calculate_GRC(global_model, client_models, client_losses):
    """
    计算客户端的 GRC 分数。

    参数:
        global_model (nn.Module): 全局模型。
        client_models (list): 客户端本地模型列表。
        client_losses (list): 客户端训练损失列表。

    返回:
        list: 每个客户端的 GRC 分数。
    """
    # 1. 构建参考序列（理想值：损失=0，模型参数差异=0）
    ref_loss = 0.0
    ref_param_diff = 0.0

    # 2. 计算客户端指标
    param_diffs = []
    for model in client_models:
        diff = 0.0
        for g_param, l_param in zip(global_model.parameters(), model.parameters()):
            diff += torch.norm(g_param - l_param).item()  # 参数差异（L2范数）
        param_diffs.append(diff)

    # 3. 对 losses 和 diffs 进行 mapping
    def map_sequence(sequence):
        max_val = max(sequence)
        min_val = min(sequence)
        return [(max_val + x) / (max_val + min_val) for x in sequence]

    client_losses = map_sequence(client_losses)  # 映射后的 losses
    param_diffs = map_sequence(param_diffs)  # 映射后的 diffs


    max_loss = max(client_losses)
    max_diff = max(param_diffs)


    # 4. 计算全局极值
    all_deltas = []
    for nl, nd in zip(client_losses, param_diffs):
        all_deltas.append(abs(nl - max_loss))  # 损失差值
        all_deltas.append(abs(nd - max_diff))  # 参数差异差值
    max_delta = max(all_deltas)  # 全局最大值
    min_delta = min(all_deltas)  # 全局最小值

    # 5. 计算 GRC（分辨系数 ρ=0.5）
    grc_scores = []
    grc_losses = []
    grc_diffs = []
    for nl, nd in zip(client_losses, param_diffs):
        delta_loss = abs(nl - max_loss)
        delta_diff = abs(nd - max_diff)

        # 灰色关联系数公式
        grc_loss = (min_delta + 0.5 * max_delta) / (delta_loss + 0.5 * max_delta)
        grc_diff = (min_delta + 0.5 * max_delta) / (delta_diff + 0.5 * max_delta)

        grc_losses.append(grc_loss)
        grc_diffs.append(grc_diff)

    # 将 grc_loss 和 grc_diff 组合成指标矩阵 (n_clients × 2)
    grc_metrics = np.array([client_losses, param_diffs])

    # 计算熵权法权重
    weights = entropy_weight(grc_metrics)  # 形如 [w_loss, w_diff]



    weighted_score = grc_losses / weights[0] + grc_diffs / weights[1]


    return weighted_score


def select_clients(client_loaders, use_all_clients=False, num_select=None,
                   select_by_loss=False, global_model=None, grc=False):
    if grc:  # 使用 GRC 选择客户端
        client_models = []

        # 1. 训练本地模型并计算损失
        client_losses = []
        for client_id, client_loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # 同步全局模型
            local_state = local_train(local_model, client_loader, epochs=1, lr=0.01)
            client_models.append(local_model)
            loss, _ = evaluate(global_model, client_loader)
            client_losses.append(loss)

        # 2. 计算 GRC 分数
        grc_scores = calculate_GRC(global_model, client_models, client_losses)

        # 3. 按 GRC 分数排序（从高到低，GRC越高表示越好）
        client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
        client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # 降序排序

        # 4. 选择 GRC 最高的前 num_select 个客户端
        selected = [client_id for client_id, _ in client_grc_pairs[:num_select]]
        return selected

    # 其余选择逻辑保持不变
    if use_all_clients is True:
        print("Selecting all clients")
        return list(client_loaders.keys())

    if num_select is None:
        raise ValueError("If use_all_clients=False, num_select cannot be None!")

    if select_by_loss and global_model:
        client_losses = {}
        for client_id, loader in client_loaders.items():
            loss, _ = evaluate(global_model, loader)
            client_losses[client_id] = loss

        selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
        print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
    else:
        selected_clients = random.sample(list(client_loaders.keys()), num_select)
        print(f"Randomly selected {num_select} clients: {selected_clients}")

    return selected_clients



def update_communication_counts(communication_counts, selected_clients, event):
    """
    客户端通信计数
    - event='receive' 表示客户端接收到全局模型
    - event='send' 表示客户端上传本地模型
    - event='full_round' 仅在客户端完成完整收发时增加
    """
    for client_id in selected_clients:
        communication_counts[client_id][event] += 1

        # 仅当客户端完成一次完整的 send 和 receive 时增加 full_round
        if event == "send" and communication_counts[client_id]['receive'] > 0:
            communication_counts[client_id]['full_round'] += 1


def main():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # 加载 MNIST 数据集
    train_data, test_data = load_mnist_data()

    # 生成客户端数据集，每个客户端包含多个类别
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # 创建数据加载器
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # 初始化全局模型
    global_model = MLPModel()
    global_accuracies = []  # 记录每轮全局模型的测试集准确率
    total_communication_counts = []  # 记录每轮客户端通信次数
    rounds = 300  # 联邦学习轮数
    use_all_clients = False  # 是否进行客户端选择
    num_selected_clients = 2  # 每轮选择客户端训练数量
    use_loss_based_selection = True  # 是否根据 loss 选择客户端
    grc = True

    # 初始化通信计数器
    communication_counts = {}
    for client_id in client_loaders.keys():
        communication_counts[client_id] = {
            'send': 0,  # 记录发送次数
            'receive': 0,  # 记录接收次数
            'full_round': 0  # 记录完整收发次数
        }
    # 实验数据存储 CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}.csv"
    csv_data = []

    for r in range(rounds):
        print(f"\n🔄 第 {r + 1} 轮聚合")
        # 选择客户端
        if r % 3 == 0:
            selected_clients = select_clients(client_loaders, use_all_clients=use_all_clients,
                                          num_select=num_selected_clients,
                                          select_by_loss=use_loss_based_selection, global_model=global_model, grc=grc)

        # # 设置随机阻断某个客户端的接收记录（验证用）
        # blocked_client = random.choice(selected_clients)
        # print(f" Blocking client {blocked_client} from receiving, skipping the receive event record.")

        # for client_id in selected_clients:
        #     if client_id == blocked_client:
        #         continue  # 直接跳过 receive 记录
        #     update_communication_counts(communication_counts, [client_id], "receive")

        # 记录客户端接收通信次数
        update_communication_counts(communication_counts, selected_clients, "receive")
        client_state_dicts = []

        # 客户端本地训练
        for client_id in selected_clients:
            client_loader = client_loaders[client_id]
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # 复制全局模型参数
            local_state = local_train(local_model, client_loader, epochs=1, lr=0.01)  # 训练 1 轮
            client_state_dicts.append((client_id, local_state))  # 存储 (客户端ID, 训练后的参数)

            update_communication_counts(communication_counts, [client_id], "send")  # 记录客户端上报通信次数

            param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
            print(f"  ✅ 客户端 {client_id} 训练完成 | 样本数量: {sum(client_data_sizes[client_id].values())}")
            print(f"  📌 客户端 {client_id} 模型参数均值: {param_mean}")

        # 计算本轮通信次数
        total_send = sum(
            communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_receive = sum(
            communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_comm = total_send + total_receive  # 每轮独立的总通信次数
        total_communication_counts.append(total_comm)  # 记录当前轮的通信次数

        # 聚合模型参数
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

        # # 计算全局模型参数平均值
        # global_param_mean = {name: param.mean().item() for name, param in global_model.named_parameters()}
        # print(f"🔄 轮 {r + 1} 结束后，全局模型参数均值: {global_param_mean}")

        # 评估模型
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)
        print(f"📊 测试集损失: {loss:.4f} | 测试集准确率: {accuracy:.2f}%")

        # 记录数据到 CSV
        csv_data.append([
            r + 1,
            accuracy,
            total_comm,
            ",".join(map(str, selected_clients))
        ])

    # 保存数据到 CSV 文件
    df = pd.DataFrame(csv_data, columns=[
        'Round', 'Accuracy', 'Total communication counts', 'Selected Clients'
    ])
    df.to_csv(csv_filename, index=False)
    print(f"训练数据已保存至 {csv_filename}")

    # 输出最终模型的性能
    final_loss, final_accuracy = evaluate(global_model, test_loader)
    print(f"\n🎯 Loss of final model test dataset: {final_loss:.4f}")
    print(f"🎯 Final model test set accuracy: {final_accuracy:.2f}%")

    # 输出通信记录
    print("\n Client Communication Statistics:")
    for client_id, counts in communication_counts.items():
        print(
            f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")

    # 可视化全局模型准确率 vs 轮次
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, rounds + 1), global_accuracies, marker='o', linestyle='-', color='b', label="Test Accuracy")
    plt.xlabel("Federated Learning Rounds")
    plt.ylabel("Accuracy")
    plt.title("Test Accuracy Over Federated Learning Rounds")
    plt.legend()
    plt.grid(True)
    plt.show()

    # 可视化全局模型准确率 vs 客户端完整通信次数
    plt.figure(figsize=(8, 5))
    plt.plot(total_communication_counts, global_accuracies, marker='s', linestyle='-', color='r',
             label="Test Accuracy vs. Communication")
    plt.xlabel("Total Communication Count per Round")
    plt.ylabel("Accuracy")
    plt.title("Test Accuracy vs. Total Communication")
    plt.legend()
    plt.grid(True)
    plt.show()


# def main2():
#     torch.manual_seed(0)
#     random.seed(0)
#     np.random.seed(0)
#
#     # 加载 MNIST 数据集
#     train_data, test_data = load_mnist_data()
#
#     # 生成客户端数据集，每个客户端包含多个类别
#     client_datasets, client_data_sizes = split_data_by_label(train_data)
#
#     # 创建数据加载器
#     client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
#                       for client_id, dataset in client_datasets.items()}
#     test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)
#
#     # 初始化全局模型
#     global_model = MLPModel()
#     global_accuracies = []
#     total_communication_counts = []
#     rounds = 300
#     use_all_clients = False
#     num_selected_clients = 2
#     use_loss_based_selection = True
#     grc = False
#
#     # 初始化通信计数器
#     communication_counts = {}
#     for client_id in client_loaders.keys():
#         communication_counts[client_id] = {
#             'send': 0,
#             'receive': 0,
#             'full_round': 0
#         }
#
#     # 新增：用于记录客户端历史loss
#     client_loss_history = {client_id: [] for client_id in client_loaders.keys()}
#     client_selection_history = []  # 记录每轮选择的客户端
#
#     # 实验数据存储 CSV
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     csv_filename = f"training_data_{timestamp}.csv"
#     csv_data = []
#
#     for r in range(rounds):
#         print(f"\n🔄 第 {r + 1} 轮聚合")
#
#         # 选择客户端
#         if r % 3 == 0:
#             # 每3轮计算一次所有客户端的loss
#             current_losses = {}
#             for client_id, loader in client_loaders.items():
#                 loss, _ = evaluate(global_model, loader)
#                 current_losses[client_id] = loss
#                 client_loss_history[client_id].append(loss)
#
#             # 对于最近被选中的客户端，使用加权平均loss
#             weighted_losses = {}
#             for client_id in client_loaders.keys():
#                 if len(client_selection_history) > 0 and client_id in client_selection_history[-1]:
#                     # 最近被选中的2个client，计算加权平均loss
#                     history = client_loss_history[client_id][-3:]
#                     weights = [0.5, 0.25, 0.25]
#                     weighted_loss = sum(h * w for h, w in zip(history, weights)) / sum(weights)
#                     weighted_losses[client_id] = weighted_loss
#                 else:
#                     # 其他客户端使用当前loss
#                     weighted_losses[client_id] = current_losses[client_id]
#
#             # 选择loss最大的2个客户端
#             selected_clients = sorted(weighted_losses, key=weighted_losses.get, reverse=True)[:num_selected_clients]
#             print(f"Selected {num_selected_clients} clients with the highest weighted loss: {selected_clients}")
#
#         client_selection_history.append(selected_clients)  # 记录选择历史
#
#         # 记录客户端接收通信次数
#         update_communication_counts(communication_counts, selected_clients, "receive")
#         client_state_dicts = []
#
#         # 客户端本地训练
#         for client_id in selected_clients:
#             client_loader = client_loaders[client_id]
#             local_model = MLPModel()
#             local_model.load_state_dict(global_model.state_dict())
#             local_state = local_train(local_model, client_loader, epochs=1, lr=0.01)
#             client_state_dicts.append((client_id, local_state))
#
#             update_communication_counts(communication_counts, [client_id], "send")
#
#             param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
#             print(f"  ✅ 客户端 {client_id} 训练完成 | 样本数量: {sum(client_data_sizes[client_id].values())}")
#
#         # 计算本轮通信次数
#         total_send = sum(
#             communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
#         total_receive = sum(
#             communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in
#             selected_clients)
#         total_comm = total_send + total_receive
#         total_communication_counts.append(total_comm)
#
#         # 聚合模型参数
#         global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
#
#         # 评估模型
#         loss, accuracy = evaluate(global_model, test_loader)
#         global_accuracies.append(accuracy)
#         print(f"📊 测试集损失: {loss:.4f} | 测试集准确率: {accuracy:.2f}%")
#
#         # 记录数据到 CSV
#         csv_data.append([
#             r + 1,
#             accuracy,
#             total_comm,
#             ",".join(map(str, selected_clients))
#         ])
#
#     # 保存数据到 CSV 文件
#     df = pd.DataFrame(csv_data, columns=[
#         'Round', 'Accuracy', 'Total communication counts', 'Selected Clients'
#     ])
#     df.to_csv(csv_filename, index=False)
#     print(f"训练数据已保存至 {csv_filename}")
#
#     # 输出最终模型的性能
#     final_loss, final_accuracy = evaluate(global_model, test_loader)
#     print(f"\n🎯 Loss of final model test dataset: {final_loss:.4f}")
#     print(f"🎯 Final model test set accuracy: {final_accuracy:.2f}%")
#
#     # 可视化结果
#     plt.figure(figsize=(8, 5))
#     plt.plot(range(1, rounds + 1), global_accuracies, marker='o', linestyle='-', color='b', label="Test Accuracy")
#     plt.xlabel("Federated Learning Rounds")
#     plt.ylabel("Accuracy")
#     plt.title("Test Accuracy Over Federated Learning Rounds")
#     plt.legend()
#     plt.grid(True)
#     plt.show()


if __name__ == "__main__":
    main()

# 4,7 GRC
# 5,8 loss
# 6,9 total
# 10 loss choose per 3 times
# 11 prc choose per 3 times
# 12 loss weight mean choose per 3 times
# 13 new GRG