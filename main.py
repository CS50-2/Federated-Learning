
# import time
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import torch.utils.data as data
# import torchvision.transforms as transforms
# import torchvision.datasets as datasets
# import numpy as np
# import random
# import os
# import matplotlib.pyplot as plt
# import csv
# import pandas as pd
# from datetime import datetime
#
#
# # å®šä¹‰ MLP æ¨¡å‹
# class MLPModel(nn.Module):
#     def __init__(self):
#         super(MLPModel, self).__init__()
#         self.fc1 = nn.Linear(28 * 28, 200)  # ç¬¬ä¸€å±‚ï¼Œè¾“å…¥ç»´åº¦ 784 -> 200
#         self.fc2 = nn.Linear(200, 200)  # ç¬¬äºŒå±‚ï¼Œ200 -> 200
#         self.fc3 = nn.Linear(200, 10)  # è¾“å‡ºå±‚ï¼Œ200 -> 10
#         self.relu = nn.ReLU()
#
#     def forward(self, x):
#         x = x.view(x.size(0), -1)  # å±•å¹³è¾“å…¥ (batch_size, 1, 28, 28) -> (batch_size, 784)
#         x = self.relu(self.fc1(x))
#         x = self.relu(self.fc2(x))
#         x = self.fc3(x)  # ç›´æ¥è¾“å‡ºï¼Œä¸ä½¿ç”¨ Softmaxï¼ˆå› ä¸º PyTorch çš„ CrossEntropyLoss é‡Œå·²ç»åŒ…å«äº†ï¼‰
#         return x
#
#
# # åŠ è½½ MNIST æ•°æ®é›†
# def load_mnist_data(data_path="./data"):
#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
#
#     if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
#         print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
#     else:
#         print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")
#
#     train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
#     test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)
#
#     # visualize_mnist_samples(train_data)
#     return train_data, test_data
#
#
# # æ˜¾ç¤ºæ•°æ®é›†ç¤ºä¾‹å›¾ç‰‡
# def visualize_mnist_samples(dataset, num_samples=10):
#     fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 1.2, 1.5))
#     for i in range(num_samples):
#         img, label = dataset[i]
#         axes[i].imshow(img.squeeze(), cmap="gray")
#         axes[i].set_title(label)
#         axes[i].axis("off")
#     plt.show()
#
#
# def split_data_by_label(dataset, num_clients=10):
#
#     client_data_sizes = {
#         0: {0: 600},
#         1: {1: 700},
#         2: {2: 500},
#         3: {3: 600},
#         4: {4: 600},
#         5: {5: 500},
#         6: {6: 100},
#         7: {7: 100},
#         8: {8: 100},
#         9: {9: 100}
#     }
#
#     # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°æ®ç´¢å¼•
#     label_to_indices = {i: [] for i in range(10)}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç´¢å¼•
#     for idx, (_, label) in enumerate(dataset):
#         label_to_indices[label].append(idx)
#
#     # åˆå§‹åŒ–å®¢æˆ·ç«¯æ•°æ®å­˜å‚¨
#     client_data_subsets = {}
#     client_actual_sizes = {i: {label: 0 for label in range(10)} for i in range(num_clients)}  # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡
#
#     # éå†æ¯ä¸ªå®¢æˆ·ç«¯ï¼Œä¸ºå…¶åˆ†é…æŒ‡å®šç±»åˆ«çš„æ•°æ®
#     for client_id, label_info in client_data_sizes.items():
#         selected_indices = []  # ä¸´æ—¶å­˜å‚¨è¯¥å®¢æˆ·ç«¯æ‰€æœ‰é€‰ä¸­çš„ç´¢å¼•
#         for label, size in label_info.items():
#             # ç¡®ä¿ä¸è¶…å‡ºç±»åˆ«æ•°æ®é›†å®é™…å¤§å°
#             available_size = len(label_to_indices[label])
#             sample_size = min(available_size, size)
#
#             if sample_size < size:
#                 print(f"âš ï¸ è­¦å‘Šï¼šç±»åˆ« {label} çš„æ•°æ®ä¸è¶³ï¼Œå®¢æˆ·ç«¯ {client_id} åªèƒ½è·å– {sample_size} æ¡æ ·æœ¬ï¼ˆéœ€æ±‚ {size} æ¡ï¼‰")
#
#             # ä»è¯¥ç±»åˆ«ä¸­éšæœºæŠ½å–æ ·æœ¬
#             sampled_indices = random.sample(label_to_indices[label], sample_size)
#             selected_indices.extend(sampled_indices)
#
#             # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡
#             client_actual_sizes[client_id][label] = sample_size
#
#         # åˆ›å»º PyTorch Subset
#         client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)
#
#     # æ‰“å°æ¯ä¸ªå®¢æˆ·ç«¯çš„å®é™…åˆ†é…æ•°æ®é‡
#     print("\nğŸ“Š æ¯ä¸ªå®¢æˆ·ç«¯å®é™…æ•°æ®åˆ†å¸ƒ:")
#     for client_id, label_sizes in client_actual_sizes.items():
#         print(f"å®¢æˆ·ç«¯ {client_id}: {label_sizes}")
#
#     return client_data_subsets, client_actual_sizes
#
#
# # æœ¬åœ°è®­ç»ƒå‡½æ•°
# def local_train(model, train_loader, epochs=5, lr=0.01):
#     criterion = nn.CrossEntropyLoss()
#     optimizer = optim.SGD(model.parameters(), lr=lr)
#     model.train()
#     for epoch in range(epochs):
#         for batch_x, batch_y in train_loader:
#             optimizer.zero_grad()
#             outputs = model(batch_x)
#             loss = criterion(outputs, batch_y)
#             loss.backward()
#             optimizer.step()
#     return model.state_dict()
#
#
# # å®æ—¶è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„lossç”¨åšFedgra
# def local_train_fedgra_loss(model, train_loader, epochs=5, lr=0.01):
#     criterion = nn.CrossEntropyLoss()
#     optimizer = optim.SGD(model.parameters(), lr=lr)
#     model.train()
#     loss_sq_sum = 0.0
#
#     for epoch in range(epochs):
#         for batch_x, batch_y in train_loader:
#             optimizer.zero_grad()
#             outputs = model(batch_x)
#             loss = criterion(outputs, batch_y)
#             loss.backward()
#             optimizer.step()
#             loss_sq_sum += loss.item() ** 2
#
#     h_i = loss_sq_sum  # æ”¾å¤§è®­ç»ƒè¿‡ç¨‹ä¸­ loss çš„ç´¯ç§¯ç¨‹åº¦ï¼Œä»è€Œå¢å¼ºå®¢æˆ·ç«¯ä¹‹é—´çš„åŒºåˆ†åº¦ã€‚
#     return model, h_i
#
#
# #  è”é‚¦å¹³å‡èšåˆå‡½æ•°
# def fed_avg(global_model, client_state_dicts, client_sizes):
#     global_dict = global_model.state_dict()
#     subkey = [sublist[0] for sublist in client_state_dicts]
#     new_client_sizes = dict(([(key, client_sizes[key]) for key in subkey]))
#     total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())  # è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®æ€»é‡
#     for key in global_dict.keys():
#         global_dict[key] = sum(
#             client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
#             for (client_id, client_state) in client_state_dicts
#         )
#     global_model.load_state_dict(global_dict)
#     return global_model
#
#
# # è¯„ä¼°æ¨¡å‹
# def evaluate(model, test_loader):
#     model.eval()
#     criterion = nn.CrossEntropyLoss()
#     correct, total, total_loss = 0, 0, 0.0
#     with torch.no_grad():
#         for batch_x, batch_y in test_loader:
#             outputs = model(batch_x)
#             loss = criterion(outputs, batch_y)
#             total_loss += loss.item()
#             _, predicted = torch.max(outputs, 1)
#             total += batch_y.size(0)
#             correct += (predicted == batch_y).sum().item()
#     accuracy = correct / total * 100
#     return total_loss / len(test_loader), accuracy
#
#
# # ç†µæƒæ³•å®ç°
#
# def entropy_weight(l):
#     l = np.array(l)
#
#     # Step 1: Min-Max å½’ä¸€åŒ–ï¼ˆé¿å…è´Ÿå€¼å’Œçˆ†ç‚¸ï¼‰
#     X_norm = (l - l.min(axis=1, keepdims=True)) / (l.max(axis=1, keepdims=True) - l.min(axis=1, keepdims=True) + 1e-12)
#
#     # Step 2: è½¬ä¸ºæ¦‚ç‡çŸ©é˜µ P_ki
#     P = X_norm / (X_norm.sum(axis=1, keepdims=True) + 1e-12)
#
#     # Step 3: è®¡ç®—ç†µ
#     K = 1 / np.log(X_norm.shape[1])
#     E = -K * np.sum(P * np.log(P + 1e-12), axis=1)  # shape: (2,)
#
#     # Step 4: è®¡ç®—ä¿¡æ¯æ•ˆç”¨å€¼ & æƒé‡
#     d = 1 - E
#     weights = d / np.sum(d)
#     return weights.tolist()
#
#
# # ç°è‰²å…³è”åº¦å®ç°
# def calculate_GRC(global_model, client_models, client_losses):
#     """
#     è®¡ç®— GRC åˆ†æ•° + ç†µæƒæ³•æƒé‡
#     ä¿®æ­£ï¼š
#       - æ˜ å°„é¡ºåºé”™è¯¯
#       - ç†µæƒæ³•ä½¿ç”¨é”™è¯¯æŒ‡æ ‡
#     """
#     # æ­£ç¡®å†™æ³•ï¼šä½¿ç”¨æ•´ä½“å‚æ•°å‘é‡è®¡ç®— L2 èŒƒæ•°ï¼ˆç¬¦åˆæ–‡çŒ®ï¼‰
#     param_diffs = []
#     for model in client_models:
#         global_vec = torch.nn.utils.parameters_to_vector(global_model.parameters()).detach()
#         local_vec = torch.nn.utils.parameters_to_vector(model.parameters()).detach()
#         diff = torch.norm(local_vec - global_vec).item()
#         param_diffs.append(diff)
#
#     # 2. æ˜ å°„åŸå§‹æŒ‡æ ‡åˆ° [0, 1] åŒºé—´ï¼ˆä¸ºç†µæƒæ³•å‡†å¤‡ï¼‰
#     def map_sequence_loss(sequence):
#         max_val, min_val = max(sequence), min(sequence)
#         denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
#         return [(max_val - x) / denom for x in sequence]  # è¶Šå°è¶Šå¥½ï¼Œè´Ÿç›¸å…³
#
#     def map_sequence_diff(sequence):
#         max_val, min_val = max(sequence), min(sequence)
#         denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
#         return [(x - min_val) / denom for x in sequence]  # è¶Šå¤§è¶Šå¥½ï¼Œæ­£ç›¸å…³
#
#     # ç”¨äº GRC çš„æ˜ å°„
#     mapped_losses = map_sequence_loss(client_losses)
#     mapped_diffs = map_sequence_diff(param_diffs)
#
#     # 3. ç†µæƒæ³•è®¡ç®—æƒé‡ï¼ˆæ ¹æ®æ˜ å°„å€¼ï¼Œé GRCï¼‰
#     grc_metrics = np.vstack([mapped_losses, mapped_diffs])  # shape: (2, n_clients)
#     weights = entropy_weight(grc_metrics)  # w_loss, w_diff
#
#     # 4. è®¡ç®— GRC åˆ†æ•°ï¼ˆÎ¾kiï¼‰ï¼Œå‚è€ƒå€¼ä¸º 1
#     ref_loss, ref_diff = 1.0, 1.0
#     delta_losses = [abs(x - ref_loss) for x in mapped_losses]
#     delta_diffs = [abs(x - ref_diff) for x in mapped_diffs]
#     all_deltas = delta_losses + delta_diffs
#     max_delta, min_delta = max(all_deltas), min(all_deltas)
#
#     grc_losses = []
#     grc_diffs = []
#     rho = 0.5  # åŒºåˆ†åº¦å› å­
#     for d_loss, d_diff in zip(delta_losses, delta_diffs):
#         grc_loss = (min_delta + rho * max_delta) / (d_loss + rho * max_delta)
#         grc_diff = (min_delta + rho * max_delta) / (d_diff + rho * max_delta)
#         grc_losses.append(grc_loss)
#         grc_diffs.append(grc_diff)
#
#     # 5. åŠ æƒæ±‚å’Œå¾—åˆ°æœ€ç»ˆ GRC åˆ†æ•°
#     grc_losses = np.array(grc_losses)
#     grc_diffs = np.array(grc_diffs)
#     weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]
#
#     # è°ƒè¯•ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯çš„ lossã€diffã€å¾—åˆ†ï¼‰
#     print("\n GRCå¾—åˆ†]")
#     for i in range(len(client_models)):
#         print(f"Client {i} | loss: {client_losses[i]:.4f}, diff: {param_diffs[i]:.4f}, "
#               f"mapped_loss: {mapped_losses[i]:.4f}, mapped_diff: {mapped_diffs[i]:.4f}, "
#               f"GRC: {weighted_score[i]:.4f}")
#     print(f"ç†µæƒæ³•æƒé‡: w_loss = {weights[0]:.4f}, w_diff = {weights[1]:.4f}")
#
#     return weighted_score, weights
#
#
# # å®¢æˆ·ç«¯é€‰æ‹©å™¨
# def select_clients(client_loaders, use_all_clients=False, num_select=None,
#                    select_by_loss=False, global_model=None, grc=False,
#                    fairness_tracker=None):
#     if grc:  # ä½¿ç”¨ GRC é€‰æ‹©å®¢æˆ·ç«¯
#         client_models = []
#         # 1. è®­ç»ƒæœ¬åœ°æ¨¡å‹å¹¶è®¡ç®—æŸå¤±
#         client_losses = []
#         for client_id, client_loader in client_loaders.items():
#             local_model = MLPModel()
#             local_model.load_state_dict(global_model.state_dict())  # åŒæ­¥å…¨å±€æ¨¡å‹
#             trained_model, h_i = local_train_fedgra_loss(local_model, client_loader, epochs=5, lr=0.01)
#             client_models.append(trained_model)
#             client_losses.append(h_i)
#
#         # 2. è®¡ç®— GRC åˆ†æ•°
#         grc_scores, grc_weights = calculate_GRC(global_model, client_models, client_losses)
#         select_clients.latest_weights = grc_weights  # è®°å½•æƒé‡
#
#         # 3. æŒ‰ GRC åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼ŒGRCè¶Šé«˜è¡¨ç¤ºè¶Šå¥½ï¼‰
#         client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
#         client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # é™åºæ’åº
#
#         # 4. é€‰æ‹© GRC æœ€é«˜çš„å‰ num_select ä¸ªå®¢æˆ·ç«¯
#         selected_clients = [client_id for client_id, _ in client_grc_pairs[:num_select]]
#         return selected_clients
#
#     # å…¶ä½™é€‰æ‹©é€»è¾‘ä¿æŒä¸å˜
#
#     if use_all_clients is True:
#         print("Selecting all clients")
#         return list(client_loaders.keys())
#
#     if num_select is None:
#         raise ValueError("If use_all_clients=False, num_select cannot be None!")
#
#     if select_by_loss and global_model:
#         client_losses = {}
#
#         for client_id, loader in client_loaders.items():
#             local_model = MLPModel()
#             local_model.load_state_dict(global_model.state_dict())
#             local_train(local_model, loader, epochs=5, lr=0.01)
#             loss, _ = evaluate(local_model, loader)
#             client_losses[client_id] = loss
#         selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
#         print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
#     else:
#         selected_clients = random.sample(list(client_loaders.keys()), num_select)
#         print(f"Randomly selected {num_select} clients: {selected_clients}")
#
#     return selected_clients
#
#
# def update_communication_counts(communication_counts, selected_clients, event):
#
#     for client_id in selected_clients:
#         communication_counts[client_id][event] += 1
#
#
#         if event == "send" and communication_counts[client_id]['receive'] > 0:
#             communication_counts[client_id]['full_round'] += 1
#
#
# def main():
#     T1 = time.time()
#     torch.manual_seed(0)
#     random.seed(0)
#     np.random.seed(0)
#
#     # åŠ è½½ MNIST æ•°æ®é›†
#     train_data, test_data = load_mnist_data()
#
#     # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å«å¤šä¸ªç±»åˆ«
#     client_datasets, client_data_sizes = split_data_by_label(train_data)
#
#     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
#     client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
#                       for client_id, dataset in client_datasets.items()}
#     test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)
#
#     # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
#     global_model = MLPModel()
#     global_accuracies = []  # è®°å½•æ¯è½®å…¨å±€æ¨¡å‹çš„æµ‹è¯•é›†å‡†ç¡®ç‡
#     total_communication_counts = []  # è®°å½•æ¯è½®å®¢æˆ·ç«¯é€šä¿¡æ¬¡æ•°
#     rounds = 300  # è”é‚¦å­¦ä¹ è½®æ•°
#     use_all_clients = False  # æ˜¯å¦è¿›è¡Œå®¢æˆ·ç«¯é€‰æ‹©
#     num_selected_clients = 2  # æ¯è½®é€‰æ‹©å®¢æˆ·ç«¯è®­ç»ƒæ•°é‡
#     use_loss_based_selection = False  # æ˜¯å¦æ ¹æ® loss é€‰æ‹©å®¢æˆ·ç«¯
#     grc = True
#
#     # åˆå§‹åŒ–é€šä¿¡è®¡æ•°å™¨
#     communication_counts = {}
#     for client_id in client_loaders.keys():
#         communication_counts[client_id] = {
#             'send': 0,
#             'receive': 0,
#             'full_round': 0
#         }
#
#     # å®éªŒæ•°æ®å­˜å‚¨ CSV
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     csv_filename = f"training_data_{timestamp}.csv"
#     csv_data = []
#
#     for r in range(rounds):
#         print(f"\nğŸ”„ ç¬¬ {r + 1} è½®èšåˆ")
#         # é€‰æ‹©å®¢æˆ·ç«¯
#
#         selected_clients = select_clients(
#             client_loaders,
#             use_all_clients=use_all_clients,
#             num_select=num_selected_clients,
#             select_by_loss=use_loss_based_selection,
#             global_model=global_model,
#             grc=grc
#         )
#         # # è®¾ç½®éšæœºé˜»æ–­æŸä¸ªå®¢æˆ·ç«¯çš„æ¥æ”¶è®°å½•ï¼ˆéªŒè¯ç”¨ï¼‰
#         # blocked_client = random.choice(selected_clients)
#         # print(f" Blocking client {blocked_client} from receiving, skipping the receive event record.")
#
#         # for client_id in selected_clients:
#         #     if client_id == blocked_client:
#         #         continue  # ç›´æ¥è·³è¿‡ receive è®°å½•
#         #     update_communication_counts(communication_counts, [client_id], "receive")
#
#         # è®°å½•å®¢æˆ·ç«¯æ¥æ”¶é€šä¿¡æ¬¡æ•°
#         update_communication_counts(communication_counts, selected_clients, "receive")
#         client_state_dicts = []
#
#         # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
#         for client_id in selected_clients:
#             client_loader = client_loaders[client_id]
#             local_model = MLPModel()
#             local_model.load_state_dict(global_model.state_dict())  # å¤åˆ¶å…¨å±€æ¨¡å‹å‚æ•°
#             local_state = local_train(local_model, client_loader, epochs=5, lr=0.01)  # è®­ç»ƒ 1 è½®
#             client_state_dicts.append((client_id, local_state))  # å­˜å‚¨ (å®¢æˆ·ç«¯ID, è®­ç»ƒåçš„å‚æ•°)
#
#             update_communication_counts(communication_counts, [client_id], "send")  # è®°å½•å®¢æˆ·ç«¯ä¸ŠæŠ¥é€šä¿¡æ¬¡æ•°
#
#             param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
#             print(f"  âœ… å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå®Œæˆ | æ ·æœ¬æ•°é‡: {sum(client_data_sizes[client_id].values())}")
#             print(f"  ğŸ“Œ å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å‚æ•°å‡å€¼: {param_mean}")
#
#         # è®¡ç®—æœ¬è½®é€šä¿¡æ¬¡æ•°
#         total_send = sum(
#             communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
#         total_receive = sum(
#             communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
#         total_comm = total_send + total_receive  # æ¯è½®ç‹¬ç«‹çš„æ€»é€šä¿¡æ¬¡æ•°
#
#         # å¦‚æœä¸æ˜¯ç¬¬ä¸€è½®ï¼Œç´¯åŠ å‰ä¸€è½®çš„é€šä¿¡æ¬¡æ•°
#         if len(total_communication_counts) > 0:
#             total_comm += total_communication_counts[-1]
#         total_communication_counts.append(total_comm)
#
#         # èšåˆæ¨¡å‹å‚æ•°
#         global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
#
#         # # è®¡ç®—å…¨å±€æ¨¡å‹å‚æ•°å¹³å‡å€¼
#         # global_param_mean = {name: param.mean().item() for name, param in global_model.named_parameters()}
#         # print(f"ğŸ”„ è½® {r + 1} ç»“æŸåï¼Œå…¨å±€æ¨¡å‹å‚æ•°å‡å€¼: {global_param_mean}")
#
#         # è¯„ä¼°æ¨¡å‹
#         loss, accuracy = evaluate(global_model, test_loader)
#         global_accuracies.append(accuracy)
#         print(f"ğŸ“Š æµ‹è¯•é›†æŸå¤±: {loss:.4f} | æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")
#
#         # è®°å½•æ•°æ®åˆ° CSV
#         if grc and hasattr(select_clients, 'latest_weights'):
#             w_loss = select_clients.latest_weights[0]
#             w_diff = select_clients.latest_weights[1]
#             print(f"ğŸ“ˆ Round {r + 1} | GRC æƒé‡: w_loss = {w_loss:.4f}, w_diff = {w_diff:.4f}")
#
#         else:
#             w_loss = 'NA'
#             w_diff = 'NA'
#
#         csv_data.append([
#             r + 1,
#             accuracy,
#             total_comm,
#             ",".join(map(str, selected_clients)),
#             w_loss,
#             w_diff
#         ])
#         df = pd.DataFrame(csv_data, columns=[
#             'Round', 'Accuracy', 'Total communication counts', 'Selected Clients',
#             'GRC Weight - Loss', 'GRC Weight - Diff'])
#         df.to_csv(csv_filename, index=False)
#
#     # è¾“å‡ºæœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½
#     final_loss, final_accuracy = evaluate(global_model, test_loader)
#     print(f"\nğŸ¯ Loss of final model test dataset: {final_loss:.4f}")
#     print(f"ğŸ¯ Final model test set accuracy: {final_accuracy:.2f}%")
#
#     # è¾“å‡ºé€šä¿¡è®°å½•
#     print("\n Client Communication Statistics:")
#     for client_id, counts in communication_counts.items():
#         print(
#             f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")
#
#     T2 = time.time()
#     print('ç¨‹åºè¿è¡Œæ—¶é—´:%sç§’' % ((T2 - T1)))
#
#     # å¯è§†åŒ–å…¨å±€æ¨¡å‹å‡†ç¡®ç‡ vs è½®æ¬¡
#     plt.figure(figsize=(8, 5))
#     plt.plot(range(1, rounds + 1), global_accuracies, marker='o', linestyle='-', color='b', label="Test Accuracy")
#     plt.xlabel("Federated Learning Rounds")
#     plt.ylabel("Accuracy")
#     plt.title("Test Accuracy Over Federated Learning Rounds")
#     plt.legend()
#     plt.grid(True)
#     plt.show()
#
#     # å¯è§†åŒ–å…¨å±€æ¨¡å‹å‡†ç¡®ç‡ vs å®¢æˆ·ç«¯å®Œæ•´é€šä¿¡æ¬¡æ•°
#     plt.figure(figsize=(8, 5))
#     plt.plot(total_communication_counts, global_accuracies, marker='s', linestyle='-', color='r',
#              label="Test Accuracy vs. Communication")
#     plt.xlabel("Total Communication Count per Round")
#     plt.ylabel("Accuracy")
#     plt.title("Test Accuracy vs. Total Communication")
#     plt.legend()
#     plt.grid(True)
#     plt.show()
#
#
#
#
#
# if __name__ == "__main__":
#
#     main()
#     #1927.4226


# import torch
# import torch.nn as nn
# import torch.optim as optim
# import torch.utils.data as data
# import torchvision.transforms as transforms
# import torchvision.datasets as datasets
# import numpy as np
# import random
# import os
# import matplotlib.pyplot as plt
# import csv
# import pandas as pd
# from datetime import datetime
# import time
#
#
# # å®šä¹‰ MLP æ¨¡å‹
# class MLPModel(nn.Module):
#     def __init__(self):
#         super(MLPModel, self).__init__()
#         self.fc1 = nn.Linear(28 * 28, 1024)  # ç¬¬ä¸€å±‚ï¼Œè¾“å…¥ç»´åº¦ 784 -> 200
#         self.fc2 = nn.Linear(1024, 1024)  # ç¬¬äºŒå±‚ï¼Œ200 -> 200
#         self.fc3 = nn.Linear(1024, 10)  # è¾“å‡ºå±‚ï¼Œ200 -> 10
#         self.relu = nn.ReLU()
#
#     def forward(self, x):
#         x = x.view(x.size(0), -1)  # å±•å¹³è¾“å…¥ (batch_size, 1, 28, 28) -> (batch_size, 784)
#         x = self.relu(self.fc1(x))
#         x = self.relu(self.fc2(x))
#         x = self.fc3(x)  # ç›´æ¥è¾“å‡ºï¼Œä¸ä½¿ç”¨ Softmaxï¼ˆå› ä¸º PyTorch çš„ CrossEntropyLoss é‡Œå·²ç»åŒ…å«äº†ï¼‰
#         return x
#
#
#
# class LoRALayer(nn.Module):
#     def __init__(self, in_features, out_features, rank=4, alpha=1):
#         super(LoRALayer, self).__init__()
#         self.rank = rank
#         self.alpha = alpha
#         self.scaling = alpha / rank
#         self.A = nn.Parameter(torch.zeros(in_features, rank))
#         self.B = nn.Parameter(torch.zeros(rank, out_features))
#
#         nn.init.normal_(self.A, mean=0.0, std=0.02)
#         nn.init.zeros_(self.B)
#
#     def forward(self, x):
#         return x @ (self.A @ self.B) * self.scaling
#
#
#
# class LoRAMLPModel(nn.Module):
#     def __init__(self, base_model, rank=4, alpha=1):
#         super(LoRAMLPModel, self).__init__()
#         self.base_model = base_model
#         for param in base_model.parameters():
#             param.requires_grad = False
#
#         self.lora_fc1 = LoRALayer(28 * 28, 1024, rank=rank, alpha=alpha)
#         self.lora_fc2 = LoRALayer(1024, 1024, rank=rank, alpha=alpha)
#         self.lora_fc3 = LoRALayer(1024, 10, rank=rank, alpha=alpha)
#
#     def forward(self, x):
#         x = x.view(x.size(0), -1)  # å±•å¹³è¾“å…¥
#
#         # å‰å‘ä¼ æ’­ç»“åˆåŸºç¡€æ¨¡å‹å’Œ LoRA éƒ¨åˆ†
#         fc1_out = self.base_model.relu(self.base_model.fc1(x) + self.lora_fc1(x))
#         fc2_out = self.base_model.relu(self.base_model.fc2(fc1_out) + self.lora_fc2(fc1_out))
#         out = self.base_model.fc3(fc2_out) + self.lora_fc3(fc2_out)
#
#         return out
#
#
# # åŠ è½½ MNIST æ•°æ®é›†
# def load_mnist_data(data_path="./data"):
#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
#
#     if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
#         print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
#     else:
#         print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")
#
#     train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
#     test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)
#
#     # visualize_mnist_samples(train_data)
#     return train_data, test_data
#
#
# # æ˜¾ç¤ºæ•°æ®é›†ç¤ºä¾‹å›¾ç‰‡
# def visualize_mnist_samples(dataset, num_samples=10):
#     fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 1.2, 1.5))
#     for i in range(num_samples):
#         img, label = dataset[i]
#         axes[i].imshow(img.squeeze(), cmap="gray")
#         axes[i].set_title(label)
#         axes[i].axis("off")
#     plt.show()
#
#
# def split_data_by_label(dataset, num_clients=10):
#     client_data_sizes = {
#         0: {0: 600},
#         1: {1: 700},
#         2: {2: 500},
#         3: {3: 600},
#         4: {4: 600},
#         5: {5: 500},
#         6: {6: 100},
#         7: {7: 100},
#         8: {8: 100},
#         9: {9: 100}
#     }
#
#     # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°æ®ç´¢å¼•
#     label_to_indices = {i: [] for i in range(10)}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç´¢å¼•
#     for idx, (_, label) in enumerate(dataset):
#         label_to_indices[label].append(idx)
#
#     # åˆå§‹åŒ–å®¢æˆ·ç«¯æ•°æ®å­˜å‚¨
#     client_data_subsets = {}
#     client_actual_sizes = {i: {label: 0 for label in range(10)} for i in range(num_clients)}  # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡
#
#     # éå†æ¯ä¸ªå®¢æˆ·ç«¯ï¼Œä¸ºå…¶åˆ†é…æŒ‡å®šç±»åˆ«çš„æ•°æ®
#     for client_id, label_info in client_data_sizes.items():
#         selected_indices = []  # ä¸´æ—¶å­˜å‚¨è¯¥å®¢æˆ·ç«¯æ‰€æœ‰é€‰ä¸­çš„ç´¢å¼•
#         for label, size in label_info.items():
#             # ç¡®ä¿ä¸è¶…å‡ºç±»åˆ«æ•°æ®é›†å®é™…å¤§å°
#             available_size = len(label_to_indices[label])
#             sample_size = min(available_size, size)
#
#             if sample_size < size:
#                 print(f"âš ï¸ è­¦å‘Šï¼šç±»åˆ« {label} çš„æ•°æ®ä¸è¶³ï¼Œå®¢æˆ·ç«¯ {client_id} åªèƒ½è·å– {sample_size} æ¡æ ·æœ¬ï¼ˆéœ€æ±‚ {size} æ¡ï¼‰")
#
#             # ä»è¯¥ç±»åˆ«ä¸­éšæœºæŠ½å–æ ·æœ¬
#             sampled_indices = random.sample(label_to_indices[label], sample_size)
#             selected_indices.extend(sampled_indices)
#
#             # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡
#             client_actual_sizes[client_id][label] = sample_size
#
#         # åˆ›å»º PyTorch Subset
#         client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)
#
#     # æ‰“å°æ¯ä¸ªå®¢æˆ·ç«¯çš„å®é™…åˆ†é…æ•°æ®é‡
#     print("\nğŸ“Š æ¯ä¸ªå®¢æˆ·ç«¯å®é™…æ•°æ®åˆ†å¸ƒ:")
#     for client_id, label_sizes in client_actual_sizes.items():
#         print(f"å®¢æˆ·ç«¯ {client_id}: {label_sizes}")
#
#     return client_data_subsets, client_actual_sizes
#
#
# # æœ¬åœ°è®­ç»ƒå‡½æ•° (ç”¨äºæ­£å¸¸çš„è”é‚¦è®­ç»ƒï¼Œè®­ç»ƒæ‰€æœ‰å‚æ•°)
# def local_train(model, train_loader, epochs=5, lr=0.01):
#     criterion = nn.CrossEntropyLoss()
#     optimizer = optim.SGD(model.parameters(), lr=lr)
#     model.train()
#     for epoch in range(epochs):
#         for batch_x, batch_y in train_loader:
#             optimizer.zero_grad()
#             outputs = model(batch_x)
#             loss = criterion(outputs, batch_y)
#             loss.backward()
#             optimizer.step()
#     return model.state_dict()
#
#
# # LoRAè®­ç»ƒå‡½æ•° (åªè®­ç»ƒLoRAå‚æ•°ï¼Œç”¨äºå®¢æˆ·ç«¯é€‰æ‹©é˜¶æ®µ)
# def local_train_lora(base_model, train_loader, epochs=2, lr=0.01, rank=4, alpha=1):
#     # åˆ›å»ºLoRAæ¨¡å‹
#     lora_model = LoRAMLPModel(base_model, rank=rank, alpha=alpha)
#
#     criterion = nn.CrossEntropyLoss()
#     # åªä¼˜åŒ–LoRAå‚æ•°
#     optimizer = optim.SGD([p for n, p in lora_model.named_parameters() if p.requires_grad], lr=lr)
#
#     lora_model.train()
#     loss_sq_sum = 0.0
#
#     for epoch in range(epochs):
#         for batch_x, batch_y in train_loader:
#             optimizer.zero_grad()
#             outputs = lora_model(batch_x)
#             loss = criterion(outputs, batch_y)
#             loss.backward()
#             optimizer.step()
#             loss_sq_sum += loss.item() ** 2
#
#     h_i = loss_sq_sum  # ç´¯ç§¯losså¹³æ–¹å’Œ
#     return lora_model, h_i
#
#
# # å®æ—¶è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„lossç”¨åšFedGRA (ä½¿ç”¨LoRA)
# def local_train_fedgra_loss_lora(model, train_loader, epochs=2, lr=0.01, rank=4, alpha=1):
#     # ä½¿ç”¨LoRAæ¨¡å‹è¿›è¡Œè½»é‡çº§è®­ç»ƒ
#     lora_model, h_i = local_train_lora(model, train_loader, epochs=epochs, lr=lr, rank=rank, alpha=alpha)
#     return lora_model, h_i
#
#
# #  è”é‚¦å¹³å‡èšåˆå‡½æ•°
# def fed_avg(global_model, client_state_dicts, client_sizes):
#     global_dict = global_model.state_dict()
#     subkey = [sublist[0] for sublist in client_state_dicts]
#     new_client_sizes = dict(([(key, client_sizes[key]) for key in subkey]))
#     total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())  # è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®æ€»é‡
#     for key in global_dict.keys():
#         global_dict[key] = sum(
#             client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
#             for (client_id, client_state) in client_state_dicts
#         )
#     global_model.load_state_dict(global_dict)
#     return global_model
#
#
# # è¯„ä¼°æ¨¡å‹
# def evaluate(model, test_loader):
#     model.eval()
#     criterion = nn.CrossEntropyLoss()
#     correct, total, total_loss = 0, 0, 0.0
#     with torch.no_grad():
#         for batch_x, batch_y in test_loader:
#             outputs = model(batch_x)
#             loss = criterion(outputs, batch_y)
#             total_loss += loss.item()
#             _, predicted = torch.max(outputs, 1)
#             total += batch_y.size(0)
#             correct += (predicted == batch_y).sum().item()
#     accuracy = correct / total * 100
#     return total_loss / len(test_loader), accuracy
#
#
# # ç†µæƒæ³•å®ç°
# def entropy_weight(l):
#     l = np.array(l)
#
#     # Step 1: Min-Max å½’ä¸€åŒ–ï¼ˆé¿å…è´Ÿå€¼å’Œçˆ†ç‚¸ï¼‰
#     X_norm = (l - l.min(axis=1, keepdims=True)) / (l.max(axis=1, keepdims=True) - l.min(axis=1, keepdims=True) + 1e-12)
#
#     # Step 2: è½¬ä¸ºæ¦‚ç‡çŸ©é˜µ P_ki
#     P = X_norm / (X_norm.sum(axis=1, keepdims=True) + 1e-12)
#
#     # Step 3: è®¡ç®—ç†µ
#     K = 1 / np.log(X_norm.shape[1])
#     E = -K * np.sum(P * np.log(P + 1e-12), axis=1)  # shape: (2,)
#
#     # Step 4: è®¡ç®—ä¿¡æ¯æ•ˆç”¨å€¼ & æƒé‡
#     d = 1 - E
#     weights = d / np.sum(d)
#     return weights.tolist()
#
#
# # ç°è‰²å…³è”åº¦å®ç° (ä½¿ç”¨LoRAæ¨¡å‹)
# def calculate_GRC(global_model, client_lora_models, client_losses):
#     """
#     è®¡ç®— GRC åˆ†æ•° + ç†µæƒæ³•æƒé‡
#     ä½¿ç”¨LoRAæ¨¡å‹ä¸­çš„å·®å¼‚è®¡ç®—GRC
#     """
#     # è®¡ç®—å‚æ•°å·®å¼‚ï¼ˆåªè€ƒè™‘LoRAéƒ¨åˆ†çš„å‚æ•°ï¼‰
#     param_diffs = []
#     for lora_model in client_lora_models:
#         # æå–æ‰€æœ‰LoRAå‚æ•°å½¢æˆä¸€ä¸ªå‘é‡
#         lora_params = []
#         for name, param in lora_model.named_parameters():
#             if param.requires_grad:  # åªè€ƒè™‘LoRAéƒ¨åˆ†å‚æ•°
#                 lora_params.append(param.detach().view(-1))
#
#         if lora_params:
#             lora_vec = torch.cat(lora_params)
#             # ç”¨LoRAå‚æ•°çš„L2èŒƒæ•°ä½œä¸ºå·®å¼‚åº¦é‡
#             diff = torch.norm(lora_vec).item()
#             param_diffs.append(diff)
#         else:
#             param_diffs.append(0.0)
#
#     # 2. æ˜ å°„åŸå§‹æŒ‡æ ‡åˆ° [0, 1] åŒºé—´ï¼ˆä¸ºç†µæƒæ³•å‡†å¤‡ï¼‰
#     def map_sequence_loss(sequence):
#         max_val, min_val = max(sequence), min(sequence)
#         denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
#         return [(max_val - x) / denom for x in sequence]  # è¶Šå°è¶Šå¥½ï¼Œè´Ÿç›¸å…³
#
#     def map_sequence_diff(sequence):
#         max_val, min_val = max(sequence), min(sequence)
#         denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
#         return [(x - min_val) / denom for x in sequence]  # è¶Šå¤§è¶Šå¥½ï¼Œæ­£ç›¸å…³
#
#     # ç”¨äº GRC çš„æ˜ å°„
#     mapped_losses = map_sequence_loss(client_losses)
#     mapped_diffs = map_sequence_diff(param_diffs)
#
#     # 3. ç†µæƒæ³•è®¡ç®—æƒé‡ï¼ˆæ ¹æ®æ˜ å°„å€¼ï¼Œé GRCï¼‰
#     grc_metrics = np.vstack([mapped_losses, mapped_diffs])  # shape: (2, n_clients)
#     weights = entropy_weight(grc_metrics)  # w_loss, w_diff
#
#     # 4. è®¡ç®— GRC åˆ†æ•°ï¼ˆÎ¾kiï¼‰ï¼Œå‚è€ƒå€¼ä¸º 1
#     ref_loss, ref_diff = 1.0, 1.0
#     delta_losses = [abs(x - ref_loss) for x in mapped_losses]
#     delta_diffs = [abs(x - ref_diff) for x in mapped_diffs]
#     all_deltas = delta_losses + delta_diffs
#     max_delta, min_delta = max(all_deltas), min(all_deltas)
#
#     grc_losses = []
#     grc_diffs = []
#     rho = 0.5  # åŒºåˆ†åº¦å› å­
#     for d_loss, d_diff in zip(delta_losses, delta_diffs):
#         grc_loss = (min_delta + rho * max_delta) / (d_loss + rho * max_delta)
#         grc_diff = (min_delta + rho * max_delta) / (d_diff + rho * max_delta)
#         grc_losses.append(grc_loss)
#         grc_diffs.append(grc_diff)
#
#     # 5. åŠ æƒæ±‚å’Œå¾—åˆ°æœ€ç»ˆ GRC åˆ†æ•°
#     grc_losses = np.array(grc_losses)
#     grc_diffs = np.array(grc_diffs)
#     weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]
#
#     # è°ƒè¯•ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯çš„ lossã€diffã€å¾—åˆ†ï¼‰
#     print("\n GRCå¾—åˆ†]")
#     for i in range(len(client_lora_models)):
#         print(f"Client {i} | loss: {client_losses[i]:.4f}, diff: {param_diffs[i]:.4f}, "
#               f"mapped_loss: {mapped_losses[i]:.4f}, mapped_diff: {mapped_diffs[i]:.4f}, "
#               f"GRC: {weighted_score[i]:.4f}")
#     print(f"ç†µæƒæ³•æƒé‡: w_loss = {weights[0]:.4f}, w_diff = {weights[1]:.4f}")
#
#     return weighted_score, weights
#
#
# # å®¢æˆ·ç«¯é€‰æ‹©å™¨ (ä½¿ç”¨LoRA)
# def select_clients(client_loaders, use_all_clients=False, num_select=None,
#                    select_by_loss=False, global_model=None, grc=False,
#                    fairness_tracker=None, lora_rank=4, lora_alpha=1):
#     if grc:  # ä½¿ç”¨ GRC é€‰æ‹©å®¢æˆ·ç«¯
#         client_lora_models = []
#         # 1. ä½¿ç”¨LoRAè®­ç»ƒæœ¬åœ°æ¨¡å‹å¹¶è®¡ç®—æŸå¤± (è½»é‡çº§è®­ç»ƒ)
#         client_losses = []
#         for client_id, client_loader in client_loaders.items():
#             # ä½¿ç”¨LoRAè®­ç»ƒ - å‡å°‘è®­ç»ƒæˆæœ¬
#             trained_lora_model, h_i = local_train_fedgra_loss_lora(
#                 global_model, client_loader, epochs=2, lr=0.001, rank=lora_rank, alpha=lora_alpha
#             )
#             client_lora_models.append(trained_lora_model)
#             client_losses.append(h_i)
#
#         # 2. è®¡ç®— GRC åˆ†æ•°
#         grc_scores, grc_weights = calculate_GRC(global_model, client_lora_models, client_losses)
#         select_clients.latest_weights = grc_weights  # è®°å½•æƒé‡
#
#         # 3. æŒ‰ GRC åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼ŒGRCè¶Šé«˜è¡¨ç¤ºè¶Šå¥½ï¼‰
#         client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
#         client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # é™åºæ’åº
#
#         # 4. é€‰æ‹© GRC æœ€é«˜çš„å‰ num_select ä¸ªå®¢æˆ·ç«¯
#         selected_clients = [client_id for client_id, _ in client_grc_pairs[:num_select]]
#         return selected_clients
#
#     # å…¶ä½™é€‰æ‹©é€»è¾‘ä¿æŒä¸å˜
#
#     if use_all_clients is True:
#         print("Selecting all clients")
#         return list(client_loaders.keys())
#
#     if num_select is None:
#         raise ValueError("If use_all_clients=False, num_select cannot be None!")
#
#     if select_by_loss and global_model:
#         client_losses = {}
#
#         for client_id, loader in client_loaders.items():
#             local_model = MLPModel()
#             local_model.load_state_dict(global_model.state_dict())
#             local_train(local_model, loader, epochs=5, lr=0.01)
#             loss, _ = evaluate(local_model, loader)
#             client_losses[client_id] = loss
#         selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
#         print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
#     else:
#         selected_clients = random.sample(list(client_loaders.keys()), num_select)
#         print(f"Randomly selected {num_select} clients: {selected_clients}")
#
#     return selected_clients
#
#
# def update_communication_counts(communication_counts, selected_clients, event):
#     for client_id in selected_clients:
#         communication_counts[client_id][event] += 1
#
#         if event == "send" and communication_counts[client_id]['receive'] > 0:
#             communication_counts[client_id]['full_round'] += 1
#
#
# def main():
#     torch.manual_seed(0)
#     random.seed(0)
#     np.random.seed(0)
#
#     # åŠ è½½ MNIST æ•°æ®é›†
#     train_data, test_data = load_mnist_data()
#
#     # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å«å¤šä¸ªç±»åˆ«
#     client_datasets, client_data_sizes = split_data_by_label(train_data)
#
#     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
#     client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
#                       for client_id, dataset in client_datasets.items()}
#     test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)
#
#     # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
#     global_model = MLPModel()
#     global_accuracies = []  # è®°å½•æ¯è½®å…¨å±€æ¨¡å‹çš„æµ‹è¯•é›†å‡†ç¡®ç‡
#     total_communication_counts = []  # è®°å½•æ¯è½®å®¢æˆ·ç«¯é€šä¿¡æ¬¡æ•°
#     rounds = 300  # è”é‚¦å­¦ä¹ è½®æ•°
#     use_all_clients = False  # æ˜¯å¦è¿›è¡Œå®¢æˆ·ç«¯é€‰æ‹©
#     num_selected_clients = 2  # æ¯è½®é€‰æ‹©å®¢æˆ·ç«¯è®­ç»ƒæ•°é‡
#     use_loss_based_selection = False  # æ˜¯å¦æ ¹æ® loss é€‰æ‹©å®¢æˆ·ç«¯
#     grc = True
#
#     # LoRAè¶…å‚æ•°
#     lora_rank = 4  # LoRAç§©
#     lora_alpha = 16  # LoRAç¼©æ”¾å› å­
#
#     # åˆå§‹åŒ–é€šä¿¡è®¡æ•°å™¨
#     communication_counts = {}
#     for client_id in client_loaders.keys():
#         communication_counts[client_id] = {
#             'send': 0,
#             'receive': 0,
#             'full_round': 0
#         }
#
#     # å®éªŒæ•°æ®å­˜å‚¨ CSV
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     csv_filename = f"training_data_lora_{timestamp}.csv"
#     csv_data = []
#
#     for r in range(rounds):
#         print(f"\nğŸ”„ ç¬¬ {r + 1} è½®èšåˆ")
#         # é€‰æ‹©å®¢æˆ·ç«¯ (ä½¿ç”¨LoRAå‡å°‘è®¡ç®—æˆæœ¬)
#         selected_clients = select_clients(
#             client_loaders,
#             use_all_clients=use_all_clients,
#             num_select=num_selected_clients,
#             select_by_loss=use_loss_based_selection,
#             global_model=global_model,
#             grc=grc,
#             lora_rank=lora_rank,
#             lora_alpha=lora_alpha
#         )
#
#         # è®°å½•å®¢æˆ·ç«¯æ¥æ”¶é€šä¿¡æ¬¡æ•°
#         update_communication_counts(communication_counts, selected_clients, "receive")
#         client_state_dicts = []
#
#         # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ (æ­£å¸¸è®­ç»ƒæ‰€æœ‰å‚æ•°)
#         for client_id in selected_clients:
#             client_loader = client_loaders[client_id]
#             local_model = MLPModel()
#             local_model.load_state_dict(global_model.state_dict())  # å¤åˆ¶å…¨å±€æ¨¡å‹å‚æ•°
#             local_state = local_train(local_model, client_loader, epochs=1, lr=0.01)  # è®­ç»ƒ5è½®
#             client_state_dicts.append((client_id, local_state))  # å­˜å‚¨ (å®¢æˆ·ç«¯ID, è®­ç»ƒåçš„å‚æ•°)
#
#             update_communication_counts(communication_counts, [client_id], "send")  # è®°å½•å®¢æˆ·ç«¯ä¸ŠæŠ¥é€šä¿¡æ¬¡æ•°
#
#             param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
#             print(f"  âœ… å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå®Œæˆ | æ ·æœ¬æ•°é‡: {sum(client_data_sizes[client_id].values())}")
#             print(f"  ğŸ“Œ å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å‚æ•°å‡å€¼: {param_mean}")
#
#         # è®¡ç®—æœ¬è½®é€šä¿¡æ¬¡æ•°
#         total_send = sum(
#             communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
#         total_receive = sum(
#             communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
#         total_comm = total_send + total_receive  # æ¯è½®ç‹¬ç«‹çš„æ€»é€šä¿¡æ¬¡æ•°
#
#         # å¦‚æœä¸æ˜¯ç¬¬ä¸€è½®ï¼Œç´¯åŠ å‰ä¸€è½®çš„é€šä¿¡æ¬¡æ•°
#         if len(total_communication_counts) > 0:
#             total_comm += total_communication_counts[-1]
#         total_communication_counts.append(total_comm)
#
#         # èšåˆæ¨¡å‹å‚æ•°
#         global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
#
#         # è¯„ä¼°æ¨¡å‹
#         loss, accuracy = evaluate(global_model, test_loader)
#         global_accuracies.append(accuracy)
#         print(f"ğŸ“Š æµ‹è¯•é›†æŸå¤±: {loss:.4f} | æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")
#
#         # è®°å½•æ•°æ®åˆ° CSV
#         if grc and hasattr(select_clients, 'latest_weights'):
#             w_loss = select_clients.latest_weights[0]
#             w_diff = select_clients.latest_weights[1]
#             print(f"ğŸ“ˆ Round {r + 1} | GRC æƒé‡: w_loss = {w_loss:.4f}, w_diff = {w_diff:.4f}")
#
#         else:
#             w_loss = 'NA'
#             w_diff = 'NA'
#
#         csv_data.append([
#             r + 1,
#             accuracy,
#             total_comm,
#             ",".join(map(str, selected_clients)),
#             w_loss,
#             w_diff
#         ])
#         df = pd.DataFrame(csv_data, columns=[
#             'Round', 'Accuracy', 'Total communication counts', 'Selected Clients',
#             'GRC Weight - Loss', 'GRC Weight - Diff'])
#         df.to_csv(csv_filename, index=False)
#
#     # è¾“å‡ºæœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½
#     final_loss, final_accuracy = evaluate(global_model, test_loader)
#     print(f"\nğŸ¯ Loss of final model test dataset: {final_loss:.4f}")
#     print(f"ğŸ¯ Final model test set accuracy: {final_accuracy:.2f}%")
#
#     # è¾“å‡ºé€šä¿¡è®°å½•
#     print("\n Client Communication Statistics:")
#     for client_id, counts in communication_counts.items():
#         print(
#             f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")
#
#
#
# if __name__ == "__main__":
#     T1 = time.time()
#     main()
#     T2 = time.time()
#     print('ç¨‹åºè¿è¡Œæ—¶é—´:%sç§’' % ((T2 - T1)))
#     #1320.884345293045
#
#     # 1553.40735
#
#
#
# # -*- coding: utf-8 -*-
#
# import pandas as pd
# import matplotlib.pyplot as plt
#
#
# loss_csv_300 = "training_data_lora_20250507_104136.csv"
# grc_csv_300 = "training_data_20250507_115745.csv"
#
#
# df_loss_300 = pd.read_csv(loss_csv_300)
# df_grc_300 = pd.read_csv(grc_csv_300)
#
#
# plt.figure(figsize=(8, 5))
# plt.plot(df_loss_300['Round'], df_loss_300['Accuracy'], label='GRC-based-with-lora', color='blue')
# plt.plot(df_grc_300['Round'], df_grc_300['Accuracy'], label='GRC-based', color='red')
# plt.xlabel("Round")
# plt.ylabel("Test Accuracy (%)")
# plt.title("Accuracy over Rounds (300 rounds)")
# plt.legend()
# plt.grid(True)
# plt.tight_layout()
# plt.show()


# -*- coding: utf-8 -*-
"""Federated Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

# import torch
# import torch.nn as nn
# import torch.optim as optim
# import torch.utils.data as data
# import torchvision.transforms as transforms
# import torchvision.datasets as datasets
# import numpy as np
# import random
# import os
# import matplotlib.pyplot as plt
# import ssl
# from datetime import datetime
# import pandas as pd
# import torch.nn.functional as F
# import time
#
#
# class LoRALayer(nn.Module):
#     def __init__(self, in_dim, out_dim, rank, alpha, use_svd=False, linear=None):
#         super().__init__()
#         self.use_svd = use_svd  # Flag to decide if SVD should be used
#         self.alpha = alpha
#
#         if use_svd:
#             # Initialize LoRA parameters using SVD
#             source_linear = linear.weight.data
#             source_linear = source_linear.float()
#             U, S, V = torch.linalg.svd(source_linear)  # Perform SVD
#             U_r = U[:, :rank]  # Take the first 'rank' singular vectors
#             S_r = torch.diag(S[:rank])  # Create the diagonal matrix for the top 'rank' singular values
#             V_r = V[:, :rank].t()  # Take the first 'rank' singular vectors
#
#             # Use SVD components to initialize A and B
#             self.A = nn.Parameter(U_r.contiguous())  # The A matrix
#             self.B = nn.Parameter((S_r @ V_r).contiguous())  # The B matrix (rank x out_dim) using S_r * V_r^T
#         else:
#             # Random initialization
#             std_dev = 1 / torch.sqrt(torch.tensor(rank).float())
#             self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)
#             self.B = nn.Parameter(torch.zeros(rank, out_dim))
#
#     # def forward(self, x):
#     #     x=self.alpha*(x@self.A@self.B)
#     #     return x
#
#
# class LinearWithLoRA(nn.Module):
#     def __init__(self, linear, rank, alpha, use_svd=False):
#         super().__init__()
#         self.linear = linear
#         self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha, use_svd=use_svd, linear=linear)
#
#     def forward(self, x):
#         # Apply LoRA to original weights
#         lora = self.lora.A @ self.lora.B  # combine LoRA metrices
#         combined_weight = self.linear.weight + self.lora.alpha * lora
#         return F.linear(x, combined_weight, self.linear.bias)
#
#
# # å®šä¹‰ MLP æ¨¡å‹
# class MLPModel(nn.Module):
#     def __init__(self, is_LoRA=False, rank=4, alpha=8, use_svd=False):
#         super(MLPModel, self).__init__()
#
#         self.is_LoRA = is_LoRA
#         self.use_svd = use_svd
#
#         if not self.is_LoRA:  # Use the original linear layers
#             self.fc1 = nn.Linear(28 * 28, 1024)  # ç¬¬ä¸€å±‚ï¼Œè¾“å…¥ç»´åº¦ 784 -> 200
#             self.fc2 = nn.Linear(1024, 1024)  # ç¬¬äºŒå±‚ï¼Œ200 -> 200
#             self.fc3 = nn.Linear(1024, 10)  # è¾“å‡ºå±‚ï¼Œ200 -> 10
#             self.relu = nn.ReLU()
#         else:  # Use LoRA-enhanced layers
#             self.fc1 = nn.Linear(28 * 28, 1024)
#             self.fc2 = LinearWithLoRA(nn.Linear(1024, 1024), rank, alpha, use_svd=self.use_svd)
#             self.fc3 = nn.Linear(1024, 10)
#             self.relu = nn.ReLU()
#
#     def forward(self, x):
#         x = x.view(x.size(0), -1)  # Flatten the input (batch_size, 1, 28, 28) -> (batch_size, 784)
#         x = self.relu(self.fc1(x))
#         x = self.relu(self.fc2(x))
#         x = self.fc3(x)  # Direct output, no need for Softmax (CrossEntropyLoss already includes it)
#         return x
#
#
# def freeze_linear_layers(model):
#     """Freeze the original layers in the model, leaving only LoRA layers trainable."""
#     for child in model.children():
#         if isinstance(child, LinearWithLoRA):
#             # Freeze the original weights (linear layers)
#             for param in child.linear.parameters():
#                 param.requires_grad = False  # not accepting additional gradients
#         else:
#             # Recursively freeze linear layers in children modules
#             freeze_linear_layers(child)
#
#
# # åŠ è½½ MNIST æ•°æ®é›†
# def load_mnist_data(data_path="./data"):
#     # Temporarily Skip SSL velidation step
#     ssl._create_default_https_context = ssl._create_unverified_context
#
#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
#
#     if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
#         print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
#     else:
#         print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")
#
#     train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
#     test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)
#
#     return train_data, test_data
#
#
# # åˆ†å‰² MNIST æ•°æ®ï¼Œä½¿æ¯ä¸ªå®¢æˆ·ç«¯åªåŒ…å«æŸä¸ªæ•°å­—ç±»åˆ«
# def split_data_by_label(dataset, num_clients=10):
#     # Mannually set each client'id and corresponding dataset distribution
#     # client_data_sizes = {
#     #     0: {0: 600, 1: 700, 2: 600, 3: 600, 4: 500, 5: 500, 6: 100, 7: 100, 8: 100, 9: 100},
#     #     1: {0: 700, 1: 600, 2: 600, 3: 600, 4: 500, 5: 100, 6: 100, 7: 100, 8: 100, 9: 600},
#     #     2: {0: 500, 1: 600, 2: 700, 3: 600, 4: 100, 5: 100, 6: 100, 7: 100, 8: 600, 9: 500},
#     #     3: {0: 600, 1: 600, 2: 500, 3: 100, 4: 100, 5: 100, 6: 100, 7: 500, 8: 500, 9: 700},
#     #     4: {0: 600, 1: 500, 2: 100, 3: 100, 4: 100, 5: 100, 6: 600, 7: 700, 8: 500, 9: 500},
#     #     5: {0: 500, 1: 100, 2: 100, 3: 100, 4: 100, 5: 600, 6: 500, 7: 600, 8: 700, 9: 600},
#     #     6: {0: 100, 1: 100, 2: 100, 3: 100, 4: 700, 5: 500, 6: 600, 7: 500, 8: 500, 9: 600},
#     #     7: {0: 100, 1: 100, 2: 100, 3: 600, 4: 500, 5: 600, 6: 500, 7: 600, 8: 500, 9: 100},
#     #     8: {0: 100, 1: 100, 2: 500, 3: 500, 4: 600, 5: 500, 6: 600, 7: 500, 8: 100, 9: 100},
#     #     9: {0: 100, 1: 700, 2: 600, 3: 600, 4: 600, 5: 500, 6: 600, 7: 100, 8: 100, 9: 100}
#     # }
#
#     client_data_sizes = {
#         0: {0: 600},
#         1: {1: 600},
#         2: {2: 500},
#         3: {3: 600},
#         4: {4: 600},
#         5: {5: 500},
#         6: {6: 100},
#         7: {7: 100},
#         8: {8: 100},
#         9: {9: 100}
#     }
#
#     # Initialize an empty dictionary to store indices for each label (from 0 to 9)
#     label_to_indices = {}
#
#     for label in range(10):
#         label_to_indices[label] = []
#
#         # Loop through the dataset using enumerate to get both the index and the data item.
#     # Each data item is a tuple (image, label).
#     for index, (_, label) in enumerate(dataset):
#         # Append the current index to the list corresponding to the data's label.
#         label_to_indices[label].append(index)
#
#     # Create an empty dictionary to store the data subset for each client.
#     client_data_subsets = {}
#
#     # Initialize a dictionary to record the actual number of samples allocated for each label in each client.
#     client_actual_sizes = {}
#     for client_id in range(num_clients):
#         # For each client, initialize an empty dictionary to store the sample counts for labels 0 to 9.
#         client_actual_sizes[client_id] = {}
#
#         # For each label from 0 to 9, set the initial count to 0.
#         for label in range(10):
#             client_actual_sizes[client_id][label] = 0
#
#     # Iterate over each client and assign data for the specified labels.
#     for client_id, label_info in client_data_sizes.items():
#         selected_indices = []
#
#         for label, required_size in label_info.items():
#             available_size = len(label_to_indices[label])
#
#             # Determine the number of samples to select
#             sample_size = min(available_size, required_size)
#
#             # If the available sample size is less than the required size, print a warning message.
#             if sample_size < required_size:
#                 print(
#                     f"âš ï¸ Warning: Not enough data for label {label}. Client {client_id} can only get {sample_size} samples (required {required_size}).")
#
#             # Randomly select the determined number of indices and add the selected indices to the client's list.
#             sampled_indices = random.sample(label_to_indices[label], sample_size)
#             selected_indices.extend(sampled_indices)
#
#             # Record the actual number of samples allocated for this label for the current client.
#             client_actual_sizes[client_id][label] = sample_size
#
#         # Create a PyTorch Subset for this client using the selected indices.
#         client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)
#
#     print("\nğŸ“Š Actual data distribution per client:")
#     for client_id, label_sizes in client_actual_sizes.items():
#         print(f"Client {client_id}: {label_sizes}")
#
#     # Return both the client data subsets and the dictionary of actual sample sizes.
#     return client_data_subsets, client_actual_sizes
#
#
# def local_train(model, train_loader, epochs=5, lr=0.1, is_LoRA=False, freeze_W=False):
#     """Train the model on a local client, freezing original layers if using LoRA."""
#     criterion = nn.CrossEntropyLoss()
#
#     if is_LoRA and freeze_W:  # If LoRA and freeze_W is enabled, freeze the original layers
#         freeze_linear_layers(model)
#         # Only update LoRA parameters (A and B)
#         params_to_update = [param for param in model.parameters() if param.requires_grad]
#         optimizer = optim.SGD(params_to_update, lr=lr)
#     elif is_LoRA and not freeze_W:
#         # Update LoRA parameters (A and B) and Weight
#         params_to_update = [param for param in model.parameters() if param.requires_grad]
#         optimizer = optim.SGD(params_to_update, lr=lr)
#     else:
#         optimizer = optim.SGD(model.parameters(), lr=lr)
#
#     model.train()
#     for epoch in range(epochs):
#         for batch_x, batch_y in train_loader:
#             optimizer.zero_grad()
#             outputs = model(batch_x)
#             loss = criterion(outputs, batch_y)
#             loss.backward()
#             optimizer.step()
#
#     return model.state_dict()
#
#
# # è”é‚¦å¹³å‡èšåˆå‡½æ•°
# def fed_avg(global_model, client_state_dicts, client_sizes):
#     global_dict = global_model.state_dict()
#
#     # Extract client IDs from client_state_dicts tuples.
#     subkey = [sublist[0] for sublist in client_state_dicts]
#
#     # Create a new dictionary of client sizes using only the clients that were selected.
#     new_client_sizes = dict([(key, client_sizes[key]) for key in subkey])
#
#     # Calculate the total number of samples across all selected clients.
#     # Here, each client size is now assumed to be a nested dictionary (per label), so we sum the values for each client.
#     total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())
#
#     # Update each parameter in the global model.
#     for key in global_dict.keys():
#         global_dict[key] = sum(
#             client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
#             for (client_id, client_state) in client_state_dicts
#         )
#
#     global_model.load_state_dict(global_dict)
#     return global_model
#
#
# # è¯„ä¼°æ¨¡å‹
# def evaluate(model, test_loader):
#     model.eval()
#     criterion = nn.CrossEntropyLoss()
#     correct, total, total_loss = 0, 0, 0.0
#     with torch.no_grad():
#         for batch_x, batch_y in test_loader:
#             outputs = model(batch_x)
#             loss = criterion(outputs, batch_y)
#             total_loss += loss.item()
#             _, predicted = torch.max(outputs, 1)
#             total += batch_y.size(0)
#             correct += (predicted == batch_y).sum().item()
#     accuracy = correct / total * 100
#     return total_loss / len(test_loader), accuracy
#
#
# # ç†µæƒæ³•å®ç°
#
# def entropy_weight(l):
#     l = np.array(l)
#
#     # Step 1: Min-Max å½’ä¸€åŒ–ï¼ˆé¿å…è´Ÿå€¼å’Œçˆ†ç‚¸ï¼‰
#     X_norm = (l - l.min(axis=1, keepdims=True)) / (l.max(axis=1, keepdims=True) - l.min(axis=1, keepdims=True) + 1e-12)
#
#     # Step 2: è½¬ä¸ºæ¦‚ç‡çŸ©é˜µ P_ki
#     P = X_norm / (X_norm.sum(axis=1, keepdims=True) + 1e-12)
#
#     # Step 3: è®¡ç®—ç†µ
#     K = 1 / np.log(X_norm.shape[1])
#     E = -K * np.sum(P * np.log(P + 1e-12), axis=1)  # shape: (2,)
#
#     # Step 4: è®¡ç®—ä¿¡æ¯æ•ˆç”¨å€¼ & æƒé‡
#     d = 1 - E
#     weights = d / np.sum(d)
#     return weights.tolist()
#
#
# # ç°è‰²å…³è”åº¦å®ç°
# def calculate_GRC(global_model, client_models, client_losses):
#     """
#     è®¡ç®— GRC åˆ†æ•° + ç†µæƒæ³•æƒé‡
#     ä¿®æ­£ï¼š
#       - æ˜ å°„é¡ºåºé”™è¯¯
#       - ç†µæƒæ³•ä½¿ç”¨é”™è¯¯æŒ‡æ ‡
#     """
#     # æ­£ç¡®å†™æ³•ï¼šä½¿ç”¨æ•´ä½“å‚æ•°å‘é‡è®¡ç®— L2 èŒƒæ•°ï¼ˆç¬¦åˆæ–‡çŒ®ï¼‰
#     param_diffs = []
#     for model in client_models:
#         global_vec = torch.nn.utils.parameters_to_vector(global_model.parameters()).detach()
#
#         local_vec = torch.nn.utils.parameters_to_vector(model.parameters()).detach()
#         diff = torch.norm(local_vec - global_vec).item()
#         param_diffs.append(diff)
#
#     # 2. æ˜ å°„åŸå§‹æŒ‡æ ‡åˆ° [0, 1] åŒºé—´ï¼ˆä¸ºç†µæƒæ³•å‡†å¤‡ï¼‰
#     def map_sequence_loss(sequence):
#         max_val, min_val = max(sequence), min(sequence)
#         denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
#         return [(max_val - x) / denom for x in sequence]  # è¶Šå°è¶Šå¥½ï¼Œè´Ÿç›¸å…³
#
#     def map_sequence_diff(sequence):
#         max_val, min_val = max(sequence), min(sequence)
#         denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
#         return [(x - min_val) / denom for x in sequence]  # è¶Šå¤§è¶Šå¥½ï¼Œæ­£ç›¸å…³
#
#     # ç”¨äº GRC çš„æ˜ å°„
#     mapped_losses = map_sequence_loss(client_losses)
#     mapped_diffs = map_sequence_diff(param_diffs)
#
#     # 3. ç†µæƒæ³•è®¡ç®—æƒé‡ï¼ˆæ ¹æ®æ˜ å°„å€¼ï¼Œé GRCï¼‰
#     grc_metrics = np.vstack([mapped_losses, mapped_diffs])  # shape: (2, n_clients)
#     weights = entropy_weight(grc_metrics)  # w_loss, w_diff
#
#     # 4. è®¡ç®— GRC åˆ†æ•°ï¼ˆÎ¾kiï¼‰ï¼Œå‚è€ƒå€¼ä¸º 1
#     ref_loss, ref_diff = 1.0, 1.0
#     delta_losses = [abs(x - ref_loss) for x in mapped_losses]
#     delta_diffs = [abs(x - ref_diff) for x in mapped_diffs]
#     all_deltas = delta_losses + delta_diffs
#     max_delta, min_delta = max(all_deltas), min(all_deltas)
#
#     grc_losses = []
#     grc_diffs = []
#     rho = 0.5  # åŒºåˆ†åº¦å› å­
#     for d_loss, d_diff in zip(delta_losses, delta_diffs):
#         grc_loss = (min_delta + rho * max_delta) / (d_loss + rho * max_delta)
#         grc_diff = (min_delta + rho * max_delta) / (d_diff + rho * max_delta)
#         grc_losses.append(grc_loss)
#         grc_diffs.append(grc_diff)
#
#     # 5. åŠ æƒæ±‚å’Œå¾—åˆ°æœ€ç»ˆ GRC åˆ†æ•°
#     grc_losses = np.array(grc_losses)
#     grc_diffs = np.array(grc_diffs)
#     weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]
#
#     # è°ƒè¯•ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯çš„ lossã€diffã€å¾—åˆ†ï¼‰
#     print("\n GRCå¾—åˆ†]")
#     for i in range(len(client_models)):
#         print(f"Client {i} | loss: {client_losses[i]:.4f}, diff: {param_diffs[i]:.4f}, "
#               f"mapped_loss: {mapped_losses[i]:.4f}, mapped_diff: {mapped_diffs[i]:.4f}, "
#               f"GRC: {weighted_score[i]:.4f}")
#     print(f"ç†µæƒæ³•æƒé‡: w_loss = {weights[0]:.4f}, w_diff = {weights[1]:.4f}")
#
#     return weighted_score, weights
#
#
# def select_clients(client_loaders, use_all_clients=False, num_select=None,
#                    select_by_loss=False, global_model=None, grc=True, is_LoRA=False, use_svd=False, freeze_W=False):
#     if grc:  # ä½¿ç”¨ GRC é€‰æ‹©å®¢æˆ·ç«¯
#         client_models = []
#         # 1. è®­ç»ƒæœ¬åœ°æ¨¡å‹å¹¶è®¡ç®—æŸå¤±
#         client_losses = []
#         for client_id, client_loader in client_loaders.items():
#             local_model = MLPModel(is_LoRA=is_LoRA, use_svd=use_svd)  # Define if using LoRA or SVD
#             local_model.load_state_dict(global_model.state_dict())  # åŒæ­¥å…¨å±€æ¨¡å‹
#             local_train(local_model, client_loader, epochs=1, lr=0.01, is_LoRA=is_LoRA,
#                         freeze_W=freeze_W)  # Define if using LoRA
#             client_models.append(local_model)
#             loss, _ = evaluate(local_model, client_loader)
#             client_losses.append(loss)
#
#         # 2. è®¡ç®— GRC åˆ†æ•°
#         grc_scores, grc_weights = calculate_GRC(global_model, client_models, client_losses)
#         select_clients.latest_weights = grc_weights  # è®°å½•æƒé‡
#
#         # 3. æŒ‰ GRC åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼ŒGRCè¶Šé«˜è¡¨ç¤ºè¶Šå¥½ï¼‰
#         client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
#         client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # é™åºæ’åº
#
#         # 4. é€‰æ‹© GRC æœ€é«˜çš„å‰ num_select ä¸ªå®¢æˆ·ç«¯
#         selected = [client_id for client_id, _ in client_grc_pairs[:num_select]]
#         return selected
#
#     # å…¶ä½™é€‰æ‹©é€»è¾‘ä¿æŒä¸å˜
#     if use_all_clients is True:
#         print("Selecting all clients")
#         return list(client_loaders.keys())
#
#     if num_select is None:
#         raise ValueError("If use_all_clients=False, num_select cannot be None!")
#
#     if select_by_loss and global_model:
#         client_losses = {}
#         for client_id, loader in client_loaders.items():
#             loss, _ = evaluate(global_model, loader)
#             client_losses[client_id] = loss
#
#         selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
#         print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
#     else:
#         selected_clients = random.sample(list(client_loaders.keys()), num_select)
#         print(f"Randomly selected {num_select} clients: {selected_clients}")
#
#     return selected_clients
#
#
# def update_communication_counts(communication_counts, selected_clients, event):
#     """
#     å®¢æˆ·ç«¯é€šä¿¡è®¡æ•°
#     - event='receive' è¡¨ç¤ºå®¢æˆ·ç«¯æ¥æ”¶åˆ°å…¨å±€æ¨¡å‹
#     - event='send' è¡¨ç¤ºå®¢æˆ·ç«¯ä¸Šä¼ æœ¬åœ°æ¨¡å‹
#     - event='full_round' ä»…åœ¨å®¢æˆ·ç«¯å®Œæˆå®Œæ•´æ”¶å‘æ—¶å¢åŠ 
#     """
#     for client_id in selected_clients:
#         communication_counts[client_id][event] += 1
#
#         # ä»…å½“å®¢æˆ·ç«¯å®Œæˆä¸€æ¬¡å®Œæ•´çš„ send å’Œ receive æ—¶å¢åŠ  full_round
#         if event == "send" and communication_counts[client_id]['receive'] > 0:
#             communication_counts[client_id]['full_round'] += 1
#
#
# def run_experiment(selection_method, rounds=100, num_selected_clients=2, is_LoRA=False, use_svd=False, freeze_W=False):
#     torch.manual_seed(0)
#     random.seed(0)
#     np.random.seed(0)
#
#     # åŠ è½½ MNIST æ•°æ®é›†
#     train_data, test_data = load_mnist_data()
#
#     # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åªåŒ…å«ç‰¹å®šç±»åˆ«
#     client_datasets, client_data_sizes = split_data_by_label(train_data)
#
#     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
#     client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
#                       for client_id, dataset in client_datasets.items()}
#     test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)
#
#     # Initialize global model, communication_counts, and results storage
#     global_model = MLPModel(is_LoRA=is_LoRA, use_svd=use_svd)  # Use LoRA & SVD or not
#     global_accuracies = []
#     total_communication_counts = []
#     csv_data = []
#
#     # Initialize communication counters for all clients
#     communication_counts = {client_id: {'send': 0, 'receive': 0, 'full_round': 0}
#                             for client_id in client_loaders.keys()}
#
#     for r in range(rounds):
#         print(f"\nRound {r + 1}")
#
#         # Select clients based on the specified method:
#         if selection_method == "fedGRA":
#             selected_clients = select_clients(client_loaders, use_all_clients=False,
#                                               num_select=num_selected_clients,
#                                               select_by_loss=True, global_model=global_model, grc=True, is_LoRA=is_LoRA,
#                                               use_svd=use_svd)
#         elif selection_method == "high_loss":
#             # Use loss-based selection without GRC (select top 2 highest loss clients)
#             selected_clients = select_clients(client_loaders, use_all_clients=False,
#                                               num_select=num_selected_clients,
#                                               select_by_loss=True, global_model=global_model, grc=False,
#                                               is_LoRA=is_LoRA, use_svd=use_svd, freeze_W=freeze_W)
#         elif selection_method == "fedavg":
#             # Use FedAvg with either random selection or all clients.
#             # Using all clients or random selection for FedAvg.
#             selected_clients = select_clients(client_loaders, use_all_clients=True,
#                                               num_select=num_selected_clients,
#                                               select_by_loss=False, global_model=global_model, grc=False)
#
#         # Record receive communication count
#         update_communication_counts(communication_counts, selected_clients, "receive")
#         client_state_dicts = []
#
#         # Perform local training on selected clients
#         for client_id in selected_clients:
#             client_loader = client_loaders[client_id]
#             local_model = MLPModel(is_LoRA=is_LoRA, use_svd=use_svd)  # Use LoRA & SVD or not
#             local_model.load_state_dict(global_model.state_dict())  # Sync with the global model
#             local_train(local_model, client_loader, epochs=1, lr=0.01, is_LoRA=is_LoRA)
#             client_state_dicts.append((client_id, local_model.state_dict()))
#             update_communication_counts(communication_counts, [client_id], "send")
#             print(f"Client {client_id} trained.")
#
#         # Compute communication counts for this round
#         total_send = sum(communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1)
#                          for c in selected_clients)
#         total_receive = sum(communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1)
#                             for c in selected_clients)
#         total_comm = total_send + total_receive
#         total_communication_counts.append(total_comm)
#
#         # Aggregate model updates
#         global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
#
#         # Evaluate global model
#         loss, accuracy = evaluate(global_model, test_loader)
#         global_accuracies.append(accuracy)
#         print(f"Test Accuracy: {accuracy:.2f}%")
#
#         # Save round data; add a column indicating the method used if desired.
#         csv_data.append([r + 1, accuracy, total_comm])
#
#     # Convert collected data to a DataFrame
#     df = pd.DataFrame(csv_data, columns=['Round', f'Accuracy_{selection_method}', f'Comm_{selection_method}'])
#     return df
#
#
# def main_experiments():
#     rounds = 300
#     # Run experiments for each method
#     # df_high_loss = run_experiment("high_loss", rounds, num_selected_clients=2)
#     # df_lora = run_experiment("fedGRA", rounds, num_selected_clients=2, is_LoRA=True, freeze_W=True)
#     # df_lora_svd = run_experiment("high_loss", rounds, num_selected_clients=2, is_LoRA=True, use_svd=True, freeze_W=True)
#     df_lora_svd = run_experiment("fedGRA", rounds, num_selected_clients=2, is_LoRA=True, use_svd=True, freeze_W=True)
#
#     # Merge DataFrames on 'Round'
#     # df_combined = df_lora.merge(df_lora_svd, on='Round').merge(df_lora_W, on='Round')
#     df_combined = df_lora_svd
#
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     csv_filename = f"training_data_lora_{timestamp}.csv"
#     # Save to CSV for later inspection if needed
#     df_combined.to_csv(csv_filename, index=False)
#
#
#
# if __name__ == "__main__":
#     T1 = time.time()
#     main_experiments()
#     T2 = time.time()
#     print('ç¨‹åºè¿è¡Œæ—¶é—´:%sç§’' % ((T2 - T1)))
#     # 737 ("fedGRA", rounds, num_selected_clients=2, is_LoRA=True, use_svd=True, freeze_W=True)
#     # 1312.913296699524
#     # ç¨‹åºè¿è¡Œæ—¶é—´: 2809.2965700626373ç§’

# æµ®ç‚¹æ•°è®¡ç®—åŒ… å‚ä¸è®¡ç®—çš„æ€»å‚æ•°é‡ çº¿æ€§è®­ç»ƒæ—¶é—´ svdæ¯ä¸ªepochåªåšä¸€æ¬¡

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import csv
import pandas as pd
from datetime import datetime
import time


# å®šä¹‰ MLP æ¨¡å‹
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)  # ç¬¬ä¸€å±‚ï¼Œè¾“å…¥ç»´åº¦ 784 -> 200
        self.fc2 = nn.Linear(200, 200)  # ç¬¬äºŒå±‚ï¼Œ200 -> 200
        self.fc3 = nn.Linear(200, 10)  # è¾“å‡ºå±‚ï¼Œ200 -> 10
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)  # å±•å¹³è¾“å…¥ (batch_size, 1, 28, 28) -> (batch_size, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # ç›´æ¥è¾“å‡ºï¼Œä¸ä½¿ç”¨ Softmaxï¼ˆå› ä¸º PyTorch çš„ CrossEntropyLoss é‡Œå·²ç»åŒ…å«äº†ï¼‰
        return x


# SVDç¼“å­˜ç±» - æ–°å¢ä¼˜åŒ–
class SVDCache:
    def __init__(self):
        self.cache = {}  # æ ¼å¼: {layer_name: (U_r, S_r, V_r)}

    def get_svd(self, layer_name, linear_layer, rank):
        """è·å–SVDç»“æœï¼Œå¦‚æœç¼“å­˜ä¸­å­˜åœ¨åˆ™ç›´æ¥è¿”å›ï¼Œå¦åˆ™è®¡ç®—å¹¶ç¼“å­˜"""
        if layer_name in self.cache:
            return self.cache[layer_name]

        # è®¡ç®—SVD
        W = linear_layer.weight.data.float()
        U, S, V = torch.svd(W)

        # æˆªæ–­
        U_r = U[:, :rank]  # [in_features, rank]
        S_r = torch.diag(S[:rank])  # [rank, rank]
        V_r = V.T[:rank, :]  # [rank, out_features]

        # ç¼“å­˜ç»“æœ
        self.cache[layer_name] = (U_r, S_r, V_r)
        return U_r, S_r, V_r


class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=4, alpha=1, use_svd=False, base_linear=None, svd_cache=None,
                 layer_name=None):
        super(LoRALayer, self).__init__()
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        self.use_svd = use_svd
        self.relu = nn.ReLU()

        # Store the base layer's bias if it exists
        self.base_bias = None
        if base_linear is not None and base_linear.bias is not None:
            self.base_bias = base_linear.bias.detach().clone()

        if self.use_svd and base_linear is not None and svd_cache is not None:
            # ä½¿ç”¨ç¼“å­˜çš„SVDç»“æœåˆå§‹åŒ–LoRAå‚æ•°
            U_r, S_r, V_r = svd_cache.get_svd(layer_name, base_linear, rank)

            # ä½¿ç”¨SVDç»“æœåˆå§‹åŒ–Aå’ŒB
            A = U_r  # [out_features, rank]
            B = (S_r @ V_r)  # [rank, in_features]

            self.A = nn.Parameter(A.contiguous())
            self.B = nn.Parameter(B.contiguous())

        else:
            self.A = nn.Parameter(torch.zeros(in_features, rank))
            self.B = nn.Parameter(torch.zeros(rank, out_features))
            nn.init.normal_(self.A, mean=0.0, std=0.02)
            nn.init.zeros_(self.B)

    def forward(self, x):
        AB = self.A @ self.B  # [in, rank] @ [rank, out]
        output =AB @ x.T
        if self.base_bias is not None:
            output += self.base_bias.unsqueeze(1)
        return output.T


# å®šä¹‰å¸¦ LoRA çš„ MLP æ¨¡å‹
class LoRAMLPModel(nn.Module):
    def __init__(self, base_model, rank=4, alpha=1, use_svd=True, svd_cache=None):
        super(LoRAMLPModel, self).__init__()
        self.base_model = base_model

        for param in base_model.parameters():
            param.requires_grad = False

        # ä½¿ç”¨SVDç¼“å­˜
        self.lora_fc1 = LoRALayer(28 * 28, 200, rank=rank, alpha=alpha, use_svd=use_svd,
                                  base_linear=base_model.fc1, svd_cache=svd_cache, layer_name='fc1')
        self.lora_fc2 = LoRALayer(200, 200, rank=rank, alpha=alpha, use_svd=use_svd,
                                  base_linear=base_model.fc2, svd_cache=svd_cache, layer_name='fc2')
        # self.lora_fc3 = LoRALayer(28 * 28, 10, rank=rank, alpha=alpha, use_svd=use_svd,
        #                           base_linear=base_model.fc3, svd_cache=svd_cache, layer_name='fc3')
        self.fc3_2 = nn.Linear(200, 10)

    def forward(self, x):
        x = x.view(x.size(0), -1)  # å±•å¹³è¾“å…¥

        # å‰å‘ä¼ æ’­ç»“åˆåŸºç¡€æ¨¡å‹å’Œ LoRA éƒ¨åˆ†
        fc1_out = self.lora_fc1.relu(self.lora_fc1(x))
        fc2_out = self.lora_fc2.relu(self.lora_fc2(fc1_out))
        out = self.fc3_2(fc2_out)

        return out


# åŠ è½½ MNIST æ•°æ®é›†
def load_mnist_data(data_path="./data"):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
    else:
        print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    # visualize_mnist_samples(train_data)
    return train_data, test_data


# æ˜¾ç¤ºæ•°æ®é›†ç¤ºä¾‹å›¾ç‰‡
def visualize_mnist_samples(dataset, num_samples=10):
    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 1.2, 1.5))
    for i in range(num_samples):
        img, label = dataset[i]
        axes[i].imshow(img.squeeze(), cmap="gray")
        axes[i].set_title(label)
        axes[i].axis("off")
    plt.show()


def split_data_by_label(dataset, num_clients=10):
    client_data_sizes = {
        0: {0: 600},
        1: {1: 600},
        2: {2: 600},
        3: {3: 600},
        4: {4: 600},
        5: {5: 600},
        6: {6: 600},
        7: {7: 600},
        8: {8: 600},
        9: {9: 600}
    }

    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°æ®ç´¢å¼•
    label_to_indices = {i: [] for i in range(10)}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç´¢å¼•
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    # åˆå§‹åŒ–å®¢æˆ·ç«¯æ•°æ®å­˜å‚¨
    client_data_subsets = {}
    client_actual_sizes = {i: {label: 0 for label in range(10)} for i in range(num_clients)}  # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡

    # éå†æ¯ä¸ªå®¢æˆ·ç«¯ï¼Œä¸ºå…¶åˆ†é…æŒ‡å®šç±»åˆ«çš„æ•°æ®
    for client_id, label_info in client_data_sizes.items():
        selected_indices = []  # ä¸´æ—¶å­˜å‚¨è¯¥å®¢æˆ·ç«¯æ‰€æœ‰é€‰ä¸­çš„ç´¢å¼•
        for label, size in label_info.items():
            # ç¡®ä¿ä¸è¶…å‡ºç±»åˆ«æ•°æ®é›†å®é™…å¤§å°
            available_size = len(label_to_indices[label])
            sample_size = min(available_size, size)

            if sample_size < size:
                print(f"âš ï¸ è­¦å‘Šï¼šç±»åˆ« {label} çš„æ•°æ®ä¸è¶³ï¼Œå®¢æˆ·ç«¯ {client_id} åªèƒ½è·å– {sample_size} æ¡æ ·æœ¬ï¼ˆéœ€æ±‚ {size} æ¡ï¼‰")

            # ä»è¯¥ç±»åˆ«ä¸­éšæœºæŠ½å–æ ·æœ¬
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)

            # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡
            client_actual_sizes[client_id][label] = sample_size

        # åˆ›å»º PyTorch Subset
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)

    # æ‰“å°æ¯ä¸ªå®¢æˆ·ç«¯çš„å®é™…åˆ†é…æ•°æ®é‡
    print("\nğŸ“Š æ¯ä¸ªå®¢æˆ·ç«¯å®é™…æ•°æ®åˆ†å¸ƒ:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"å®¢æˆ·ç«¯ {client_id}: {label_sizes}")

    return client_data_subsets, client_actual_sizes


# æœ¬åœ°è®­ç»ƒå‡½æ•° (ç”¨äºæ­£å¸¸çš„è”é‚¦è®­ç»ƒï¼Œè®­ç»ƒæ‰€æœ‰å‚æ•°)
def local_train(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()


# LoRAè®­ç»ƒå‡½æ•° (åªè®­ç»ƒLoRAå‚æ•°ï¼Œç”¨äºå®¢æˆ·ç«¯é€‰æ‹©é˜¶æ®µ)
def local_train_lora(base_model, train_loader, epochs=2, lr=0.01, rank=4, alpha=1, svd_cache=None):
    # åˆ›å»ºLoRAæ¨¡å‹
    lora_model = LoRAMLPModel(base_model, rank=rank, alpha=alpha, svd_cache=svd_cache)

    criterion = nn.CrossEntropyLoss()
    # åªä¼˜åŒ–LoRAå‚æ•°
    optimizer = optim.SGD([p for n, p in lora_model.named_parameters() if p.requires_grad], lr=lr)

    lora_model.train()
    loss_sq_sum = 0.0

    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = lora_model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            loss_sq_sum += loss.item() ** 2

    h_i = loss_sq_sum  # ç´¯ç§¯losså¹³æ–¹å’Œ
    return lora_model, h_i


# å®æ—¶è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„lossç”¨åšFedGRA (ä½¿ç”¨LoRA)
def local_train_fedgra_loss_lora(model, train_loader, epochs=2, lr=0.01, rank=4, alpha=1, svd_cache=None):
    # ä½¿ç”¨LoRAæ¨¡å‹è¿›è¡Œè½»é‡çº§è®­ç»ƒ
    lora_model, h_i = local_train_lora(model, train_loader, epochs=epochs, lr=lr, rank=rank, alpha=alpha,
                                       svd_cache=svd_cache)
    return lora_model, h_i


#  è”é‚¦å¹³å‡èšåˆå‡½æ•°
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    subkey = [sublist[0] for sublist in client_state_dicts]
    new_client_sizes = dict(([(key, client_sizes[key]) for key in subkey]))
    total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())  # è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®æ€»é‡
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
            for (client_id, client_state) in client_state_dicts
        )
    global_model.load_state_dict(global_dict)
    return global_model


# è¯„ä¼°æ¨¡å‹
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy


# ç†µæƒæ³•å®ç°
def entropy_weight(l):
    l = np.array(l)

    # Step 1: Min-Max å½’ä¸€åŒ–ï¼ˆé¿å…è´Ÿå€¼å’Œçˆ†ç‚¸ï¼‰
    X_norm = (l - l.min(axis=1, keepdims=True)) / (l.max(axis=1, keepdims=True) - l.min(axis=1, keepdims=True) + 1e-12)

    # Step 2: è½¬ä¸ºæ¦‚ç‡çŸ©é˜µ P_ki
    P = X_norm / (X_norm.sum(axis=1, keepdims=True) + 1e-12)

    # Step 3: è®¡ç®—ç†µ
    K = 1 / np.log(X_norm.shape[1])
    E = -K * np.sum(P * np.log(P + 1e-12), axis=1)  # shape: (2,)

    # Step 4: è®¡ç®—ä¿¡æ¯æ•ˆç”¨å€¼ & æƒé‡
    d = 1 - E
    weights = d / np.sum(d)
    return weights.tolist()


# ç°è‰²å…³è”åº¦å®ç° (ä½¿ç”¨LoRAæ¨¡å‹)
def calculate_GRC(global_model, client_lora_models, client_losses, initial_lora_params=None):
    """
    è®¡ç®— GRC åˆ†æ•° + ç†µæƒæ³•æƒé‡
    ä½¿ç”¨LoRAæ¨¡å‹ä¸­çš„è®­ç»ƒå‰åå·®å¼‚è®¡ç®—GRC - ä¿®æ”¹ä¸ºå…ˆè®¡ç®—ABä¹˜ç§¯å†è®¡ç®—å·®å¼‚

    Args:
        global_model: å…¨å±€æ¨¡å‹
        client_lora_models: è®­ç»ƒåçš„LoRAæ¨¡å‹åˆ—è¡¨
        client_losses: å®¢æˆ·ç«¯æŸå¤±åˆ—è¡¨
        initial_lora_params: è®­ç»ƒå‰çš„LoRAå‚æ•°å­—å…¸ {name: param_tensor}
    """
    # è®¡ç®—å‚æ•°å·®å¼‚ï¼ˆå…ˆè®¡ç®—ABä¹˜ç§¯ï¼Œå†è®¡ç®—å·®å¼‚ï¼‰
    param_diffs = []

    for trained_lora_model in client_lora_models:
        # å¦‚æœæä¾›äº†åˆå§‹å‚æ•°ï¼Œåˆ™è®¡ç®—ä¸åˆå§‹å‚æ•°çš„å·®å¼‚
        if initial_lora_params is not None:
            # æ”¶é›†å·®å¼‚å‘é‡
            diff_vectors = []

            # è·å–æ¨¡å‹ä¸­æ‰€æœ‰çš„LoRAå±‚
            lora_layers = {
                'lora_fc1': trained_lora_model.lora_fc1,
                'lora_fc2': trained_lora_model.lora_fc2,
                # 'lora_fc3': trained_lora_model.lora_fc3
            }

            for layer_name, lora_layer in lora_layers.items():
                # è®¡ç®—å½“å‰è®­ç»ƒåçš„ABä¹˜ç§¯
                current_AB = lora_layer.A @ lora_layer.B  # [in_features, out_features]

                # æ„å»ºè®­ç»ƒå‰çš„ABä¹˜ç§¯
                A_name = f"{layer_name}.A"
                B_name = f"{layer_name}.B"

                if A_name in initial_lora_params and B_name in initial_lora_params:
                    initial_A = initial_lora_params[A_name]
                    initial_B = initial_lora_params[B_name]
                    initial_AB = initial_A @ initial_B

                    # è®¡ç®—ABä¹˜ç§¯çš„å·®å¼‚
                    diff = current_AB - initial_AB
                    diff_vectors.append(diff.view(-1))

            if diff_vectors:
                # è¿æ¥æ‰€æœ‰å·®å¼‚å‘é‡å¹¶è®¡ç®—L2èŒƒæ•°
                diff_vec = torch.cat(diff_vectors)
                diff = torch.norm(diff_vec, 2).item()
                param_diffs.append(diff)
            else:
                param_diffs.append(0.0)
        else:
            # å¦‚æœæ²¡æœ‰æä¾›åˆå§‹å‚æ•°ï¼Œè®¡ç®—å½“å‰LoRAå±‚ABä¹˜ç§¯çš„L2èŒƒæ•°
            diff_vectors = []

            # è·å–æ¨¡å‹ä¸­æ‰€æœ‰çš„LoRAå±‚
            lora_layers = {
                'lora_fc1': trained_lora_model.lora_fc1,
                'lora_fc2': trained_lora_model.lora_fc2,
                'lora_fc3': trained_lora_model.lora_fc3
            }

            for layer_name, lora_layer in lora_layers.items():
                # è®¡ç®—å½“å‰è®­ç»ƒåçš„ABä¹˜ç§¯
                current_AB = lora_layer.A @ lora_layer.B  # [in_features, out_features]
                diff_vectors.append(current_AB.view(-1))

            if diff_vectors:
                # è¿æ¥æ‰€æœ‰ABä¹˜ç§¯å¹¶è®¡ç®—L2èŒƒæ•°
                diff_vec = torch.cat(diff_vectors)
                diff = torch.norm(diff_vec, 2).item()
                param_diffs.append(diff)
            else:
                param_diffs.append(0.0)

    # 2. æ˜ å°„åŸå§‹æŒ‡æ ‡åˆ° [0, 1] åŒºé—´ï¼ˆä¸ºç†µæƒæ³•å‡†å¤‡ï¼‰
    def map_sequence_loss(sequence):
        max_val, min_val = max(sequence), min(sequence)
        denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
        return [(max_val - x) / denom for x in sequence]  # è¶Šå°è¶Šå¥½ï¼Œè´Ÿç›¸å…³

    def map_sequence_diff(sequence):
        max_val, min_val = max(sequence), min(sequence)
        denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
        return [(x - min_val) / denom for x in sequence]  # è¶Šå¤§è¶Šå¥½ï¼Œæ­£ç›¸å…³

    # ç”¨äº GRC çš„æ˜ å°„
    mapped_losses = map_sequence_loss(client_losses)
    mapped_diffs = map_sequence_diff(param_diffs)

    # 3. ç†µæƒæ³•è®¡ç®—æƒé‡ï¼ˆæ ¹æ®æ˜ å°„å€¼ï¼Œé GRCï¼‰
    grc_metrics = np.vstack([mapped_losses, mapped_diffs])  # shape: (2, n_clients)
    weights = entropy_weight(grc_metrics)  # w_loss, w_diff

    # 4. è®¡ç®— GRC åˆ†æ•°ï¼ˆÎ¾kiï¼‰ï¼Œå‚è€ƒå€¼ä¸º 1
    ref_loss, ref_diff = 1.0, 1.0
    delta_losses = [abs(x - ref_loss) for x in mapped_losses]
    delta_diffs = [abs(x - ref_diff) for x in mapped_diffs]
    all_deltas = delta_losses + delta_diffs
    max_delta, min_delta = max(all_deltas), min(all_deltas)

    grc_losses = []
    grc_diffs = []
    rho = 0.5  # åŒºåˆ†åº¦å› å­
    for d_loss, d_diff in zip(delta_losses, delta_diffs):
        grc_loss = (min_delta + rho * max_delta) / (d_loss + rho * max_delta)
        grc_diff = (min_delta + rho * max_delta) / (d_diff + rho * max_delta)
        grc_losses.append(grc_loss)
        grc_diffs.append(grc_diff)

    # 5. åŠ æƒæ±‚å’Œå¾—åˆ°æœ€ç»ˆ GRC åˆ†æ•°
    grc_losses = np.array(grc_losses)
    grc_diffs = np.array(grc_diffs)
    weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]

    # è°ƒè¯•ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯çš„ lossã€diffã€å¾—åˆ†ï¼‰
    print("\n GRCå¾—åˆ†]")
    for i in range(len(client_lora_models)):
        print(f"Client {i} | loss: {client_losses[i]:.4f}, diff: {param_diffs[i]:.4f}, "
              f"mapped_loss: {mapped_losses[i]:.4f}, mapped_diff: {mapped_diffs[i]:.4f}, "
              f"GRC: {weighted_score[i]:.4f}")
    print(f"ç†µæƒæ³•æƒé‡: w_loss = {weights[0]:.4f}, w_diff = {weights[1]:.4f}")

    return weighted_score, weights


# å®¢æˆ·ç«¯é€‰æ‹©å™¨ (ä½¿ç”¨LoRA)
def select_clients(client_loaders, use_all_clients=False, num_select=None,
                   select_by_loss=False, global_model=None, grc=False,
                   fairness_tracker=None, lora_rank=4, lora_alpha=1):
    if grc:  # ä½¿ç”¨ GRC é€‰æ‹©å®¢æˆ·ç«¯
        client_lora_models = []

        # åˆ›å»ºSVDç¼“å­˜ - ä¼˜åŒ–ç‚¹: åªè®¡ç®—å¹¶ç¼“å­˜ä¸€æ¬¡SVDç»“æœ
        svd_cache = SVDCache()

        # 1. åªåˆ›å»ºä¸€ä¸ªåˆå§‹LoRAæ¨¡å‹ä½œä¸ºåŸºå‡†
        initial_lora_model = LoRAMLPModel(global_model, rank=lora_rank, alpha=lora_alpha, svd_cache=svd_cache)

        # å­˜å‚¨åˆå§‹LoRAå‚æ•°çŠ¶æ€
        initial_lora_params = {}
        for name, param in initial_lora_model.named_parameters():
            if param.requires_grad:  # åªä¿å­˜LoRAéƒ¨åˆ†çš„å‚æ•°
                initial_lora_params[name] = param.clone().detach()

        # 2. ä½¿ç”¨LoRAè®­ç»ƒæœ¬åœ°æ¨¡å‹å¹¶è®¡ç®—æŸå¤± (è½»é‡çº§è®­ç»ƒ)
        client_losses = []
        for client_id, client_loader in client_loaders.items():
            # ä½¿ç”¨LoRAè®­ç»ƒ - å‡å°‘è®­ç»ƒæˆæœ¬ï¼Œå¹¶ä¼ é€’SVDç¼“å­˜
            trained_lora_model, h_i = local_train_fedgra_loss_lora(
                global_model, client_loader, epochs=5, lr=0.0005,
                rank=lora_rank, alpha=lora_alpha, svd_cache=svd_cache
            )
            client_lora_models.append(trained_lora_model)
            client_losses.append(h_i)

        # 3. è®¡ç®— GRC åˆ†æ•°ï¼Œç°åœ¨ä¼ é€’åˆå§‹LoRAå‚æ•°çŠ¶æ€
        grc_scores, grc_weights = calculate_GRC(global_model, client_lora_models, client_losses, initial_lora_params)
        select_clients.latest_weights = grc_weights  # è®°å½•æƒé‡

        # 4. æŒ‰ GRC åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼ŒGRCè¶Šé«˜è¡¨ç¤ºè¶Šå¥½ï¼‰
        client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
        client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # é™åºæ’åº

        # 5. é€‰æ‹© GRC æœ€é«˜çš„å‰ num_select ä¸ªå®¢æˆ·ç«¯
        selected_clients = [client_id for client_id, _ in client_grc_pairs[:num_select]]
        return selected_clients

    # å…¶ä½™é€‰æ‹©é€»è¾‘ä¿æŒä¸å˜

    if use_all_clients is True:
        print("Selecting all clients")
        return list(client_loaders.keys())

    if num_select is None:
        raise ValueError("If use_all_clients=False, num_select cannot be None!")

    if select_by_loss and global_model:
        client_losses = {}

        for client_id, loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            local_train(local_model, loader, epochs=5, lr=0.01)
            loss, _ = evaluate(local_model, loader)
            client_losses[client_id] = loss
        selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
        print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
    else:
        selected_clients = random.sample(list(client_loaders.keys()), num_select)
        print(f"Randomly selected {num_select} clients: {selected_clients}")

    return selected_clients


def update_communication_counts(communication_counts, selected_clients, event):
    for client_id in selected_clients:
        communication_counts[client_id][event] += 1

        if event == "send" and communication_counts[client_id]['receive'] > 0:
            communication_counts[client_id]['full_round'] += 1


def main():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # åŠ è½½ MNIST æ•°æ®é›†
    train_data, test_data = load_mnist_data()

    # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å«å¤šä¸ªç±»åˆ«
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    global_model = MLPModel()
    global_accuracies = []  # è®°å½•æ¯è½®å…¨å±€æ¨¡å‹çš„æµ‹è¯•é›†å‡†ç¡®ç‡
    total_communication_counts = []  # è®°å½•æ¯è½®å®¢æˆ·ç«¯é€šä¿¡æ¬¡æ•°
    rounds = 100  # è”é‚¦å­¦ä¹ è½®æ•°
    use_all_clients = False  # æ˜¯å¦è¿›è¡Œå®¢æˆ·ç«¯é€‰æ‹©
    num_selected_clients = 2  # æ¯è½®é€‰æ‹©å®¢æˆ·ç«¯è®­ç»ƒæ•°é‡
    use_loss_based_selection = False  # æ˜¯å¦æ ¹æ® loss é€‰æ‹©å®¢æˆ·ç«¯
    grc = True

    # LoRAè¶…å‚æ•°
    lora_rank = 180  # LoRAç§©
    lora_alpha = 16  # LoRAç¼©æ”¾å› å­

    # åˆå§‹åŒ–é€šä¿¡è®¡æ•°å™¨
    communication_counts = {}
    for client_id in client_loaders.keys():
        communication_counts[client_id] = {
            'send': 0,
            'receive': 0,
            'full_round': 0
        }

    # å®éªŒæ•°æ®å­˜å‚¨ CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_lora_{timestamp}.csv"
    csv_data = []

    for r in range(rounds):
        print(f"\nğŸ”„ ç¬¬ {r + 1} è½®èšåˆ")
        # é€‰æ‹©å®¢æˆ·ç«¯ (ä½¿ç”¨LoRAå‡å°‘è®¡ç®—æˆæœ¬)
        selected_clients = select_clients(
            client_loaders,
            use_all_clients=use_all_clients,
            num_select=num_selected_clients,
            select_by_loss=use_loss_based_selection,
            global_model=global_model,
            grc=grc,
            lora_rank=lora_rank,
            lora_alpha=lora_alpha
        )

        # è®°å½•å®¢æˆ·ç«¯æ¥æ”¶é€šä¿¡æ¬¡æ•°
        update_communication_counts(communication_counts, selected_clients, "receive")
        client_state_dicts = []

        # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ (æ­£å¸¸è®­ç»ƒæ‰€æœ‰å‚æ•°)
        for client_id in selected_clients:
            client_loader = client_loaders[client_id]
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # å¤åˆ¶å…¨å±€æ¨¡å‹å‚æ•°
            local_state = local_train(local_model, client_loader, epochs=2, lr=0.01)  # è®­ç»ƒ5è½®
            client_state_dicts.append((client_id, local_state))  # å­˜å‚¨ (å®¢æˆ·ç«¯ID, è®­ç»ƒåçš„å‚æ•°)

            update_communication_counts(communication_counts, [client_id], "send")  # è®°å½•å®¢æˆ·ç«¯ä¸ŠæŠ¥é€šä¿¡æ¬¡æ•°

            param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
            print(f"  âœ… å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå®Œæˆ | æ ·æœ¬æ•°é‡: {sum(client_data_sizes[client_id].values())}")
            print(f"  ğŸ“Œ å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å‚æ•°å‡å€¼: {param_mean}")

        # è®¡ç®—æœ¬è½®é€šä¿¡æ¬¡æ•°
        total_send = sum(
            communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_receive = sum(
            communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_comm = total_send + total_receive  # æ¯è½®ç‹¬ç«‹çš„æ€»é€šä¿¡æ¬¡æ•°

        # å¦‚æœä¸æ˜¯ç¬¬ä¸€è½®ï¼Œç´¯åŠ å‰ä¸€è½®çš„é€šä¿¡æ¬¡æ•°
        if len(total_communication_counts) > 0:
            total_comm += total_communication_counts[-1]
        total_communication_counts.append(total_comm)

        # èšåˆæ¨¡å‹å‚æ•°
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

        # è¯„ä¼°æ¨¡å‹
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)
        print(f"ğŸ“Š æµ‹è¯•é›†æŸå¤±: {loss:.4f} | æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")

        # è®°å½•æ•°æ®åˆ° CSV
        if grc and hasattr(select_clients, 'latest_weights'):
            w_loss = select_clients.latest_weights[0]
            w_diff = select_clients.latest_weights[1]
            print(f"ğŸ“ˆ Round {r + 1} | GRC æƒé‡: w_loss = {w_loss:.4f}, w_diff = {w_diff:.4f}")

        else:
            w_loss = 'NA'
            w_diff = 'NA'

        csv_data.append([
            r + 1,
            accuracy,
            total_comm,
            ",".join(map(str, selected_clients)),
            w_loss,
            w_diff
        ])
        df = pd.DataFrame(csv_data, columns=[
            'Round', 'Accuracy', 'Total communication counts', 'Selected Clients',
            'GRC Weight - Loss', 'GRC Weight - Diff'])
        df.to_csv(csv_filename, index=False)

    # è¾“å‡ºæœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½
    final_loss, final_accuracy = evaluate(global_model, test_loader)
    print(f"\nğŸ¯ Loss of final model test dataset: {final_loss:.4f}")
    print(f"ğŸ¯ Final model test set accuracy: {final_accuracy:.2f}%")

    # è¾“å‡ºé€šä¿¡è®°å½•
    print("\n Client Communication Statistics:")
    for client_id, counts in communication_counts.items():
        print(
            f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")

    # # å¯è§†åŒ–å…¨å±€æ¨¡å‹å‡†ç¡®ç‡ vs è½®æ¬¡
    # plt.figure(figsize=(8, 5))
    # plt.plot(range(1, rounds + 1), global_accuracies, marker='o', linestyle='-', color='b', label="Test Accuracy")
    # plt.xlabel("Federated Learning Rounds")
    # plt.ylabel("Accuracy")
    # plt.title("Test Accuracy Over Federated Learning Rounds")
    # plt.legend()
    # plt.grid(True)
    # plt.show()

    # # å¯è§†åŒ–å…¨å±€æ¨¡å‹å‡†ç¡®ç‡ vs å®¢æˆ·ç«¯å®Œæ•´é€šä¿¡æ¬¡æ•°
    # plt.figure(figsize=(8, 5))
    # plt.plot(total_communication_counts, global_accuracies, marker='s', linestyle='-', color='r',
    #          label="Test Accuracy vs. Communication")
    # plt.xlabel("Total Communication Count per Round")
    # plt.ylabel("Accuracy")
    # plt.title("Test Accuracy vs. Total Communication")
    # plt.legend()
    # plt.grid(True)
    # plt.show()


if __name__ == "__main__":
    T1 = time.time()
    main()
    T2 = time.time()
    print('ç¨‹åºè¿è¡Œæ—¶é—´:%sç§’' % ((T2 - T1)))
    # 1520.884345293045
    # 100 453.0890119075775ç§’  ç¨‹åºè¿è¡Œæ—¶é—´:440.32077145576477ç§’


