# -*- coding: utf-8 -*-
"""Federated Training with LoRA Options.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import csv
import pandas as pd
from datetime import datetime
import torch.nn.functional as F
from collections import OrderedDict


class LoRALinear(nn.Module):
    def __init__(self, linear_layer, rank=8, alpha=0.5):
        super().__init__()
        self.original = linear_layer
        self.original.requires_grad_(True)

        # LoRAå‚æ•°
        m, n = linear_layer.weight.shape
        self.rank = rank
        self.alpha = alpha / rank
        self.A = nn.Parameter(torch.randn(m, rank))
        self.B = nn.Parameter(torch.zeros(n, rank))

    def forward(self, x):
        delta_W = self.alpha * (self.A @ self.B.T)
        return self.original(x) + F.linear(x, delta_W)

    def set_requires_grad(self, lora_only=True):
        """åŠ¨æ€è®¾ç½®å‚æ•°æ˜¯å¦éœ€è¦æ¢¯åº¦"""
        self.original.requires_grad_(not lora_only)
        self.A.requires_grad_(True)
        self.B.requires_grad_(True)


# å®šä¹‰ MLP æ¨¡å‹
class MLPModel(nn.Module):
    def __init__(self, use_lora=False, rank=8, lora_alpha=1.0):
        super(MLPModel, self).__init__()
        self.use_lora = use_lora
        self.rank = rank
        self.lora_alpha = lora_alpha

        # å®šä¹‰ç½‘ç»œå±‚
        self.fc1 = nn.Linear(28 * 28, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 10)
        self.relu = nn.ReLU()

        # æ ¹æ®éœ€è¦æ›¿æ¢ä¸ºLoRALinear
        if use_lora:
            self.fc1 = LoRALinear(self.fc1, rank=rank, alpha=lora_alpha)
            self.fc2 = LoRALinear(self.fc2, rank=rank, alpha=lora_alpha)
            self.fc3 = LoRALinear(self.fc3, rank=rank, alpha=lora_alpha)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def set_requires_grad(self, lora_only=True):
        """é€’å½’è®¾ç½®æ‰€æœ‰LoRAå±‚çš„æ¢¯åº¦éœ€æ±‚"""
        for module in self.children():
            if hasattr(module, 'set_requires_grad'):
                module.set_requires_grad(lora_only)


# åŠ è½½ MNIST æ•°æ®é›†
def load_mnist_data(data_path="./data"):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
    else:
        print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)
    return train_data, test_data


def split_data_by_label(dataset, num_clients=10):
    """
    æ‰‹åŠ¨åˆ’åˆ†æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å« 10 ä¸ªç±»åˆ«ï¼Œå¹¶è‡ªå®šä¹‰æ ·æœ¬æ•°é‡ã€‚
    """
    client_data_sizes = {
        0: {0: 600},
        1: {1: 700},
        2: {2: 500},
        3: {3: 600},
        4: {4: 600},
        5: {5: 500},
        6: {6: 500},
        7: {7: 500},
        8: {8: 500},
        9: {9: 500}
    }

    label_to_indices = {i: [] for i in range(10)}
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    client_data_subsets = {}
    client_actual_sizes = {i: {label: 0 for label in range(10)} for i in range(num_clients)}

    for client_id, label_info in client_data_sizes.items():
        selected_indices = []
        for label, size in label_info.items():
            available_size = len(label_to_indices[label])
            sample_size = min(available_size, size)
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)
            client_actual_sizes[client_id][label] = sample_size
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)

    print("\nğŸ“Š æ¯ä¸ªå®¢æˆ·ç«¯å®é™…æ•°æ®åˆ†å¸ƒ:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"å®¢æˆ·ç«¯ {client_id}: {label_sizes}")

    return client_data_subsets, client_actual_sizes


def local_train(model, train_loader, epochs=5, lr=0.01, use_lora=False):
    criterion = nn.CrossEntropyLoss()

    if use_lora:
        # ä»…ä¼˜åŒ– LoRA å‚æ•°ï¼ˆA å’Œ Bï¼‰
        lora_params = []
        for name, param in model.named_parameters():
            if 'A' in name or 'B' in name:
                lora_params.append(param)
        optimizer = optim.SGD(lora_params, lr=lr)
    else:
        # ä¼˜åŒ–æ‰€æœ‰å‚æ•°
        optimizer = optim.SGD(model.parameters(), lr=lr)

    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()


def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    subkey = [sublist[0] for sublist in client_state_dicts]
    new_client_sizes = dict(([(key, client_sizes[key]) for key in subkey]))
    total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
            for (client_id, client_state) in client_state_dicts
        )
    global_model.load_state_dict(global_dict)
    return global_model


def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy


def entropy_weight(l):
    entropies = []
    for X in l:
        P = X / (np.sum(X) + 1e-12)
        K = 1 / np.log(len(X))
        E = -K * np.sum(P * np.log(P + 1e-12))
        entropies.append(E)

    information_gain = [1 - e for e in entropies]
    sum_ig = sum(information_gain)
    weights = [ig / sum_ig for ig in information_gain]
    return weights


def calculate_GRC(global_model, client_models, client_losses):
    param_diffs = []
    for model in client_models:
        diff = 0.0
        for g_param, l_param in zip(global_model.parameters(), model.parameters()):
            diff += torch.norm(g_param - l_param).item()
        param_diffs.append(diff)

    def map_sequence_loss(sequence):
        max_val = max(sequence)
        min_val = min(sequence)
        return [(max_val - x) / (max_val + min_val) for x in sequence]

    def map_sequence_diff(sequence):
        max_val = max(sequence)
        min_val = min(sequence)
        return [(x - min_val) / (max_val + min_val) for x in sequence]

    client_losses = map_sequence_loss(client_losses)
    param_diffs = map_sequence_diff(param_diffs)

    ref_loss = 1.0
    ref_diff = 1.0

    all_deltas = []
    for loss, diff in zip(client_losses, param_diffs):
        all_deltas.append(abs(loss - ref_loss))
        all_deltas.append(abs(diff - ref_diff))
    max_delta = max(all_deltas)
    min_delta = min(all_deltas)

    grc_losses = []
    grc_diffs = []
    for loss, diff in zip(client_losses, param_diffs):
        delta_loss = abs(loss - ref_loss)
        delta_diff = abs(diff - ref_diff)

        grc_loss = (min_delta + 0.5 * max_delta) / (delta_loss + 0.5 * max_delta)
        grc_diff = (min_delta + 0.5 * max_delta) / (delta_diff + 0.5 * max_delta)

        grc_losses.append(grc_loss)
        grc_diffs.append(grc_diff)

    grc_losses = np.array(grc_losses)
    grc_diffs = np.array(grc_diffs)

    grc_metrics = np.vstack([client_losses, param_diffs])
    weights = entropy_weight(grc_metrics)

    weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]
    return weighted_score, weights


def select_clients(client_loaders, use_all_clients=False, num_select=None,
                   select_by_loss=False, global_model=None, grc=False,
                   selection_lora=False, rank=8):
    if grc:
        client_models = []
        client_losses = []

        for client_id, client_loader in client_loaders.items():
            # åœ¨å®¢æˆ·ç«¯é€‰æ‹©é˜¶æ®µä½¿ç”¨ç‹¬ç«‹çš„LoRAé…ç½®
            local_model = MLPModel(use_lora=selection_lora, rank=rank)
            local_model.load_state_dict(global_model.state_dict())

            # è®­ç»ƒæ—¶ä¹Ÿä½¿ç”¨å¯¹åº”çš„LoRAé…ç½®
            local_train(local_model, client_loader, epochs=1, lr=0.01, use_lora=selection_lora)

            client_models.append(local_model)
            loss, _ = evaluate(local_model, client_loader)
            client_losses.append(loss)

        grc_scores, grc_weights = calculate_GRC(global_model, client_models, client_losses)
        select_clients.latest_weights = grc_weights

        client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
        client_grc_pairs.sort(key=lambda x: x[1], reverse=True)
        selected = [client_id for client_id, _ in client_grc_pairs[:num_select]]
        return selected

    if use_all_clients is True:
        print("Selecting all clients")
        return list(client_loaders.keys())

    if num_select is None:
        raise ValueError("If use_all_clients=False, num_select cannot be None!")

    if select_by_loss and global_model:
        client_losses = {}
        for client_id, loader in client_loaders.items():
            # åœ¨å®¢æˆ·ç«¯é€‰æ‹©é˜¶æ®µä½¿ç”¨ç‹¬ç«‹çš„LoRAé…ç½®
            local_model = MLPModel(use_lora=selection_lora, rank=rank)
            local_model.load_state_dict(global_model.state_dict())

            # è®­ç»ƒ5ä¸ªepochå¹¶è®¡ç®—å¹³å‡loss
            total_loss = 0.0
            for epoch in range(3):
                # è®­ç»ƒ1ä¸ªepoch
                local_train(local_model, loader, epochs=1, lr=0.01, use_lora=selection_lora)
                # è®¡ç®—loss
                loss, _ = evaluate(local_model, loader)
                total_loss += loss

            avg_loss = total_loss / 5
            client_losses[client_id] = avg_loss

        selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
        print(f"Selected {num_select} clients with the highest average loss (5 epochs): {selected_clients}")
    else:
        selected_clients = random.sample(list(client_loaders.keys()), num_select)
        print(f"Randomly selected {num_select} clients: {selected_clients}")

    return selected_clients


def update_communication_counts(communication_counts, selected_clients, event):
    for client_id in selected_clients:
        communication_counts[client_id][event] += 1
        if event == "send" and communication_counts[client_id]['receive'] > 0:
            communication_counts[client_id]['full_round'] += 1


def main():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # å‚æ•°é…ç½®
    config = {
        'use_lora': False,
        'selection_lora': False,
        'training_lora': False,
        'rank': 8,
        'lora_alpha': 0.5,
        'rounds': 100,
        'num_selected_clients': 2,
        'use_all_clients': False,
        'use_loss_based_selection': True,
        'grc': False
    }

    # åŠ è½½ MNIST æ•°æ®é›†
    train_data, test_data = load_mnist_data()

    # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    global_model = MLPModel(use_lora=config['use_lora'],
                            rank=config['rank'],
                            lora_alpha=config['lora_alpha'])
    global_accuracies = []
    total_communication_counts = []
    selected_clients_history = []  # æ–°å¢ï¼šè®°å½•æ¯è½®é€‰æ‹©çš„å®¢æˆ·ç«¯

    # åˆå§‹åŒ–é€šä¿¡è®¡æ•°å™¨
    communication_counts = {}
    for client_id in client_loaders.keys():
        communication_counts[client_id] = {
            'send': 0,
            'receive': 0,
            'full_round': 0
        }

    # å®éªŒæ•°æ®å­˜å‚¨
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}.csv"
    csv_data = []

    for r in range(config['rounds']):
        print(f"\nğŸ”„ ç¬¬ {r + 1} è½®èšåˆ")

        # é€‰æ‹©å®¢æˆ·ç«¯
        selected_clients = select_clients(
            client_loaders,
            use_all_clients=config['use_all_clients'],
            num_select=config['num_selected_clients'],
            select_by_loss=config['use_loss_based_selection'],
            global_model=global_model,
            grc=config['grc'],
            selection_lora=config['selection_lora'],
            rank=config['rank']
        )
        selected_clients_history.append(selected_clients)  # è®°å½•é€‰æ‹©çš„å®¢æˆ·ç«¯

        update_communication_counts(communication_counts, selected_clients, "receive")
        client_state_dicts = []

        # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
        for client_id in selected_clients:
            client_loader = client_loaders[client_id]
            local_model = MLPModel(
                use_lora=config['use_lora'],
                rank=config['rank'],
                lora_alpha=config['lora_alpha']
            )
            local_model.load_state_dict(global_model.state_dict())

            local_state = local_train(
                local_model,
                client_loader,
                epochs=1,
                lr=0.1,
                use_lora=config['training_lora']
            )

            client_state_dicts.append((client_id, local_state))
            update_communication_counts(communication_counts, [client_id], "send")

            param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
            print(f"  âœ… å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå®Œæˆ | æ ·æœ¬æ•°é‡: {sum(client_data_sizes[client_id].values())}")
            print(f"  ğŸ“Œ å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å‚æ•°å‡å€¼: {param_mean}")

        # è®¡ç®—é€šä¿¡æ¬¡æ•°
        total_send = sum(
            communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_receive = sum(
            communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_comm = total_send + total_receive
        if len(total_communication_counts) > 0:
            total_comm += total_communication_counts[-1]
        total_communication_counts.append(total_comm)

        # èšåˆæ¨¡å‹å‚æ•°
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

        # è¯„ä¼°æ¨¡å‹
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)
        print(f"ğŸ“Š æµ‹è¯•é›†æŸå¤±: {loss:.4f} | æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")

        # è®°å½•æ•°æ®
        if config['grc'] and hasattr(select_clients, 'latest_weights'):
            w_loss = select_clients.latest_weights[0]
            w_diff = select_clients.latest_weights[1]
            print(f"ğŸ“ˆ Round {r + 1} | GRC æƒé‡: w_loss = {w_loss:.4f}, w_diff = {w_diff:.4f}")
        else:
            w_loss = 'NA'
            w_diff = 'NA'

        csv_data.append([
            r + 1,
            accuracy,
            total_comm,
            ",".join(map(str, selected_clients)),
            w_loss,
            w_diff,
            config['use_lora'],
            config['selection_lora'],
            config['training_lora'],
            config['rank'],
            config['lora_alpha']
        ])
        df = pd.DataFrame(csv_data, columns=[
            'Round', 'Accuracy', 'Total communication counts', 'Selected Clients',
            'GRC Weight - Loss', 'GRC Weight - Diff',
            'Global Use LoRA', 'Selection Use LoRA', 'Training Use LoRA',
            'LoRA Rank', 'LoRA Alpha'
        ])
        df.to_csv(csv_filename, index=False)

    # è¾“å‡ºæœ€ç»ˆç»“æœ
    final_loss, final_accuracy = evaluate(global_model, test_loader)
    print(f"\nğŸ¯ Loss of final model test dataset: {final_loss:.4f}")
    print(f"ğŸ¯ Final model test set accuracy: {final_accuracy:.2f}%")

    # è¾“å‡ºé€šä¿¡è®°å½•
    print("\n Client Communication Statistics:")
    for client_id, counts in communication_counts.items():
        print(
            f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")

    # å¯è§†åŒ–ç»“æœ - å‡†ç¡®ç‡æ›²çº¿
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(range(1, config['rounds'] + 1), global_accuracies, marker='o', linestyle='-', color='b',
             label="Test Accuracy")
    plt.xlabel("Federated Learning Rounds")
    plt.ylabel("Accuracy")
    plt.title("Test Accuracy Over Federated Learning Rounds")
    plt.legend()
    plt.grid(True)

    # å¯è§†åŒ–ç»“æœ - å®¢æˆ·ç«¯é€‰æ‹©æƒ…å†µ
    plt.subplot(1, 2, 2)
    for r in range(config['rounds']):
        for client_id in selected_clients_history[r]:
            plt.scatter(r + 1, client_id, color='r', alpha=0.5)
    plt.xlabel("Federated Learning Rounds")
    plt.ylabel("Client ID")
    plt.title("Selected Clients in Each Round")
    plt.yticks(list(client_loaders.keys()))
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(f"training_results_{timestamp}.png")  # ä¿å­˜å›¾è¡¨
    plt.show()

    # ä¿å­˜å®¢æˆ·ç«¯é€‰æ‹©å†å²
    selection_history_df = pd.DataFrame({
        'Round': range(1, config['rounds'] + 1),
        'Selected_Clients': [",".join(map(str, clients)) for clients in selected_clients_history]
    })
    selection_history_df.to_csv(f"client_selection_history_{timestamp}.csv", index=False)


if __name__ == "__main__":
    main()

# import pandas as pd
# import matplotlib.pyplot as plt
# import numpy as np
# import os
# from datetime import datetime
#
# # åœ¨è¿™é‡Œè®¾ç½®éœ€è¦æ¯”è¾ƒçš„ä¸¤ä¸ªå®éªŒæ–‡ä»¶å
# FILE1 = "training_data_20250501_093553.csv"
# FILE2 = "training_data_20250501_095310.csv"
#
#
# def load_experiment_data(filename):
#     """åŠ è½½å®éªŒæ•°æ®æ–‡ä»¶"""
#     if not os.path.exists(filename):
#         raise FileNotFoundError(f"æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
#
#     # è‡ªåŠ¨æ£€æµ‹å¯¹åº”çš„å®¢æˆ·ç«¯é€‰æ‹©å†å²æ–‡ä»¶
#     base_name = os.path.splitext(filename)[0]
#
#     # æå–å®Œæ•´çš„æ—¶é—´æˆ³éƒ¨åˆ†ï¼ˆåŒ…æ‹¬æ—¥æœŸå’Œæ—¶é—´ï¼‰
#     if base_name.startswith("training_data_"):
#         timestamp_part = "_".join(base_name.split('_')[2:])  # è·å–"20250501_093553"éƒ¨åˆ†
#         selection_file = f"client_selection_history_{timestamp_part}.csv"
#     else:
#         selection_file = f"client_selection_history_{base_name}.csv"
#
#     if not os.path.exists(selection_file):
#         raise FileNotFoundError(f"å®¢æˆ·ç«¯é€‰æ‹©å†å²æ–‡ä»¶æœªæ‰¾åˆ°: {selection_file}")
#
#     df = pd.read_csv(filename)
#     selection_df = pd.read_csv(selection_file)
#     return df, selection_file, selection_df
#
#
# def compare_experiments(file1, file2):
#     """å¯¹æ¯”ä¸¤æ¬¡å®éªŒç»“æœ"""
#     print("è”é‚¦å­¦ä¹ å®éªŒç»“æœå¯¹æ¯”å·¥å…·")
#     print("=" * 40)
#
#     # è·å–å®éªŒåç§°ï¼ˆä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡è¯†ï¼‰
#     exp1_name = os.path.splitext(os.path.basename(file1))[0]
#     exp2_name = os.path.splitext(os.path.basename(file2))[0]
#
#     print(f"\nå¯¹æ¯”å®éªŒ: {exp1_name} vs {exp2_name}")
#
#     try:
#         # åŠ è½½å®éªŒæ•°æ®
#         exp1_df, exp1_sel_file, exp1_selection = load_experiment_data(file1)
#         exp2_df, exp2_sel_file, exp2_selection = load_experiment_data(file2)
#
#         print(f"\nå·²åŠ è½½å®éªŒæ•°æ®:")
#         print(f"- {exp1_name}: è®­ç»ƒæ•°æ® {len(exp1_df)} è½®, é€‰æ‹©æ–‡ä»¶ {exp1_sel_file}")
#         print(f"- {exp2_name}: è®­ç»ƒæ•°æ® {len(exp2_df)} è½®, é€‰æ‹©æ–‡ä»¶ {exp2_sel_file}")
#
#     except FileNotFoundError as e:
#         print(f"\né”™è¯¯: {e}")
#         return
#
#     # ç¡®ä¿è½®æ¬¡ä¸€è‡´
#     min_rounds = min(len(exp1_df), len(exp2_df))
#     exp1_df = exp1_df.head(min_rounds)
#     exp2_df = exp2_df.head(min_rounds)
#
#     # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
#     plt.figure(figsize=(15, 10))
#     plt.suptitle(f"Experiment Comparison\n{exp1_name} vs {exp2_name}", fontsize=14)
#
#     # 1. Accuracyå¯¹æ¯”
#     plt.subplot(2, 2, 1)
#     plt.plot(exp1_df['Round'], exp1_df['Accuracy'], 'b-', label=exp1_name)
#     plt.plot(exp2_df['Round'], exp2_df['Accuracy'], 'r-', label=exp2_name)
#     plt.xlabel('Round')
#     plt.ylabel('Accuracy (%)')
#     plt.title('Accuracy Comparison')
#     plt.legend()
#     plt.grid(True)
#
#     # 2. Accuracyå·®å¼‚
#     plt.subplot(2, 2, 2)
#     accuracy_diff = exp1_df['Accuracy'] - exp2_df['Accuracy']
#     plt.plot(exp1_df['Round'], accuracy_diff, 'g-')
#     plt.axhline(0, color='k', linestyle='--', alpha=0.3)
#     plt.xlabel('Round')
#     plt.ylabel('Accuracy Difference (%)')
#     plt.title(f'Accuracy Difference ({exp1_name} - {exp2_name})')
#     plt.grid(True)
#
#     # 3. å®¢æˆ·ç«¯é€‰æ‹©é¢‘ç‡å¯¹æ¯”
#     plt.subplot(2, 2, 3)
#
#     def get_selection_counts(selection_df):
#         all_clients = []
#         for clients in selection_df['Selected_Clients']:
#             all_clients.extend([int(c) for c in str(clients).split(',')])
#         unique, counts = np.unique(all_clients, return_counts=True)
#         return dict(zip(unique, counts))
#
#     exp1_counts = get_selection_counts(exp1_selection)
#     exp2_counts = get_selection_counts(exp2_selection)
#
#     all_clients = sorted(set(exp1_counts.keys()).union(set(exp2_counts.keys())))
#     exp1_freq = [exp1_counts.get(c, 0) / min_rounds for c in all_clients]
#     exp2_freq = [exp2_counts.get(c, 0) / min_rounds for c in all_clients]
#
#     bar_width = 0.35
#     index = np.arange(len(all_clients))
#
#     plt.bar(index, exp1_freq, bar_width, label=exp1_name, alpha=0.7)
#     plt.bar(index + bar_width, exp2_freq, bar_width, label=exp2_name, alpha=0.7)
#     plt.xlabel('Client ID')
#     plt.ylabel('Selection Frequency')
#     plt.title('Client Selection Frequency Comparison')
#     plt.xticks(index + bar_width / 2, all_clients)
#     plt.legend()
#     plt.grid(True, axis='y')
#
#     # 4. å®¢æˆ·ç«¯é€‰æ‹©æ¨¡å¼å¯¹æ¯”
#     plt.subplot(2, 2, 4)
#
#     def get_selection_rounds(selection_df):
#         client_rounds = {}
#         for round_num, clients in zip(selection_df['Round'], selection_df['Selected_Clients']):
#             for c in str(clients).split(','):
#                 c = int(c)
#                 if c not in client_rounds:
#                     client_rounds[c] = []
#                 client_rounds[c].append(round_num)
#         return client_rounds
#
#     exp1_rounds = get_selection_rounds(exp1_selection)
#     exp2_rounds = get_selection_rounds(exp2_selection)
#
#     for c in all_clients:
#         if c in exp1_rounds:
#             plt.scatter(exp1_rounds[c], [c] * len(exp1_rounds[c]), c='blue', alpha=0.5,
#                         label=exp1_name if c == all_clients[0] else "")
#         if c in exp2_rounds:
#             plt.scatter(exp2_rounds[c], [c] * len(exp2_rounds[c]), c='red', alpha=0.5,
#                         label=exp2_name if c == all_clients[0] else "")
#
#     plt.xlabel('Round')
#     plt.ylabel('Client ID')
#     plt.title('Client Selection Pattern Comparison')
#     plt.yticks(all_clients)
#     plt.legend()
#     plt.grid(True)
#
#     plt.tight_layout()
#
#     # ä¿å­˜å¯¹æ¯”ç»“æœ
#     ts1 = exp1_name.split('_')[-2] + "_" + exp1_name.split('_')[-1]  # è·å–å®Œæ•´æ—¶é—´æˆ³
#     ts2 = exp2_name.split('_')[-2] + "_" + exp2_name.split('_')[-1]  # è·å–å®Œæ•´æ—¶é—´æˆ³
#     comparison_filename = f"comparison_{ts1}_vs_{ts2}.png"
#     plt.savefig(comparison_filename)
#     print(f"\nå¯¹æ¯”ç»“æœå·²ä¿å­˜ä¸º: {comparison_filename}")
#
#     plt.show()
#
#
# if __name__ == "__main__":
#     # ç›´æ¥ä½¿ç”¨é¢„è®¾çš„æ–‡ä»¶åè¿›è¡Œæ¯”è¾ƒ
#     compare_experiments(FILE1, FILE2)