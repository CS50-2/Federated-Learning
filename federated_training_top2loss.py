# -*- coding: utf-8 -*-
"""Federated Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import ssl 
from datetime import datetime
import pandas as pd


# å®šä¹‰ MLP æ¨¡å‹
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)  # ç¬¬ä¸€å±‚ï¼Œè¾“å…¥ç»´åº¦ 784 -> 200
        self.fc2 = nn.Linear(200, 200)      # ç¬¬äºŒå±‚ï¼Œ200 -> 200
        self.fc3 = nn.Linear(200, 10)       # è¾“å‡ºå±‚ï¼Œ200 -> 10
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)  # å±•å¹³è¾“å…¥ (batch_size, 1, 28, 28) -> (batch_size, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # ç›´æ¥è¾“å‡ºï¼Œä¸ä½¿ç”¨ Softmaxï¼ˆå› ä¸º PyTorch çš„ CrossEntropyLoss é‡Œå·²ç»åŒ…å«äº†ï¼‰
        return x

# åŠ è½½ MNIST æ•°æ®é›†
def load_mnist_data(data_path="./data"):
    
    # Temporarily Skip SSL velidation step 
    ssl._create_default_https_context = ssl._create_unverified_context

    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
    else:
        print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    return train_data, test_data

# åˆ†å‰² MNIST æ•°æ®ï¼Œä½¿æ¯ä¸ªå®¢æˆ·ç«¯åªåŒ…å«æŸä¸ªæ•°å­—ç±»åˆ«
def split_data_by_label(dataset, num_clients=10):
    
    # Mannually set each client'id and corresponding dataset distribution 
    client_data_sizes = {
        0: {0: 600, 1: 700, 2: 600, 3: 600, 4: 500, 5: 500, 6: 100, 7: 100, 8: 100, 9: 100},
        1: {0: 700, 1: 600, 2: 600, 3: 600, 4: 500, 5: 100, 6: 100, 7: 100, 8: 100, 9: 600},
        2: {0: 500, 1: 600, 2: 700, 3: 600, 4: 100, 5: 100, 6: 100, 7: 100, 8: 600, 9: 500},
        3: {0: 600, 1: 600, 2: 500, 3: 100, 4: 100, 5: 100, 6: 100, 7: 500, 8: 500, 9: 700},
        4: {0: 600, 1: 500, 2: 100, 3: 100, 4: 100, 5: 100, 6: 600, 7: 700, 8: 500, 9: 500},
        5: {0: 500, 1: 100, 2: 100, 3: 100, 4: 100, 5: 600, 6: 500, 7: 600, 8: 700, 9: 600},
        6: {0: 100, 1: 100, 2: 100, 3: 100, 4: 700, 5: 500, 6: 600, 7: 500, 8: 500, 9: 600},
        7: {0: 100, 1: 100, 2: 100, 3: 600, 4: 500, 5: 600, 6: 500, 7: 600, 8: 500, 9: 100},
        8: {0: 100, 1: 100, 2: 500, 3: 500, 4: 600, 5: 500, 6: 600, 7: 500, 8: 100, 9: 100},
        9: {0: 100, 1: 700, 2: 600, 3: 600, 4: 600, 5: 500, 6: 600, 7: 100, 8: 100, 9: 100}
    }

    # Initialize an empty dictionary to store indices for each label (from 0 to 9) 
    label_to_indices = {}

    for label in range(10):
        label_to_indices[label] = []  

    # Loop through the dataset using enumerate to get both the index and the data item.
    # Each data item is a tuple (image, label).
    for index, (_, label) in enumerate(dataset):
        # Append the current index to the list corresponding to the data's label.
        label_to_indices[label].append(index)

    # Create an empty dictionary to store the data subset for each client.
    client_data_subsets = {}

    # Initialize a dictionary to record the actual number of samples allocated for each label in each client.
    client_actual_sizes = {}
    for client_id in range(num_clients):
        # For each client, initialize an empty dictionary to store the sample counts for labels 0 to 9.
        client_actual_sizes[client_id] = {}
        
        # For each label from 0 to 9, set the initial count to 0.
        for label in range(10):
            client_actual_sizes[client_id][label] = 0
    
    # Iterate over each client and assign data for the specified labels.
    for client_id, label_info in client_data_sizes.items():
        selected_indices = []
        
        for label, required_size in label_info.items():
            available_size = len(label_to_indices[label])
            
            # Determine the number of samples to select
            sample_size = min(available_size, required_size)
            
            # If the available sample size is less than the required size, print a warning message.
            if sample_size < required_size:
                print(f"âš ï¸ Warning: Not enough data for label {label}. Client {client_id} can only get {sample_size} samples (required {required_size}).")
            
            # Randomly select the determined number of indices and add the selected indices to the client's list.
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)
            
            # Record the actual number of samples allocated for this label for the current client.
            client_actual_sizes[client_id][label] = sample_size
        
        # Create a PyTorch Subset for this client using the selected indices.
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)


    print("\nğŸ“Š Actual data distribution per client:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"Client {client_id}: {label_sizes}")

    # Return both the client data subsets and the dictionary of actual sample sizes.
    return client_data_subsets, client_actual_sizes


# æœ¬åœ°è®­ç»ƒå‡½æ•°
def local_train(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()

# è”é‚¦å¹³å‡èšåˆå‡½æ•°
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    
    # Extract client IDs from client_state_dicts tuples.
    subkey = [sublist[0] for sublist in client_state_dicts]
    
    # Create a new dictionary of client sizes using only the clients that were selected.
    new_client_sizes = dict([(key, client_sizes[key]) for key in subkey])
    
    # Calculate the total number of samples across all selected clients.
    # Here, each client size is now assumed to be a nested dictionary (per label), so we sum the values for each client.
    total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())
    
    # Update each parameter in the global model.
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
            for (client_id, client_state) in client_state_dicts
        )
    
    global_model.load_state_dict(global_dict)
    return global_model

# è¯„ä¼°æ¨¡å‹
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy

def entropy_weight(l):
    weight = []
    for X in l:

        P = X / X.sum(axis=0)

        K = 1 / np.log(len(X))
        E = -K * (P * np.log(P + 1e-12)).sum(axis=0)
        weight.append(E)
    one_minus_weight = [1 - w for w in weight]
    sum_one_minus_weight = sum(one_minus_weight)
    new_weight = [w / sum_one_minus_weight for w in one_minus_weight]

    return new_weight


def calculate_GRC(global_model, client_models, client_losses):
    """
    Calculate the Grey Relational Coefficient (GRC) scores for each client.
    
    Parameters:
        global_model (nn.Module): The global model.
        client_models (list): A list of client local models.
        client_losses (list): A list of training losses for each client.
    
    Returns:
        list: The GRC score for each client.
    """
    # Construct reference sequences (ideal values: loss=0, parameter difference=0)
    ref_loss = 0.0
    ref_param_diff = 0.0

    # Compute client metrics: calculate the L2 norm difference between the global model and each client model (parameter differences)
    param_diffs = []
    for model in client_models:
        diff = 0.0
        for g_param, l_param in zip(global_model.parameters(), model.parameters()):
            diff += torch.norm(g_param - l_param).item()  # Compute L2 norm difference for each parameter
        param_diffs.append(diff)

    # Normalize (map) both the client losses and parameter differences.
    def map_sequence(sequence):
        max_val = max(sequence)
        min_val = min(sequence)
        # Normalize each value using the formula: (max_val + x) / (max_val + min_val)
        return [(max_val + x) / (max_val + min_val) for x in sequence]

    client_losses = map_sequence(client_losses)  # Normalized client losses
    param_diffs = map_sequence(param_diffs)      # Normalized parameter differences

    # Get the maximum values from the normalized losses and differences
    max_loss = max(client_losses)
    max_diff = max(param_diffs)

    # Calculate global extreme differences across all clients and metrics
    all_deltas = []
    for nl, nd in zip(client_losses, param_diffs):
        all_deltas.append(abs(nl - max_loss))  # Absolute deviation for loss
        all_deltas.append(abs(nd - max_diff))  # Absolute deviation for parameter difference
    max_delta = max(all_deltas)  # Global maximum deviation (âˆ†max)
    min_delta = min(all_deltas)  # Global minimum deviation (âˆ†min)

    # Compute the Grey Relational Coefficients (GRC) for each metric using Ï = 0.5
    grc_losses = []  # GRC values for loss
    grc_diffs = []   # GRC values for parameter differences
    for nl, nd in zip(client_losses, param_diffs):
        delta_loss = abs(nl - max_loss)
        delta_diff = abs(nd - max_diff)
        
        # Apply the GRC formula:
        # GRC = (âˆ†min + Ïâˆ†max) / (âˆ†ki + Ïâˆ†max)
        grc_loss = (min_delta + 0.5 * max_delta) / (delta_loss + 0.5 * max_delta)
        grc_diff = (min_delta + 0.5 * max_delta) / (delta_diff + 0.5 * max_delta)
        
        grc_losses.append(grc_loss)
        grc_diffs.append(grc_diff)

    # Combine the normalized metrics into a matrix (2 x n_clients)
    grc_metrics = np.array([client_losses, param_diffs])

    # Compute entropy weights for each metric (returns weights for loss and parameter differences)
    weights = entropy_weight(grc_metrics)  # Expected output: [w_loss, w_diff]

    # Compute the final weighted GRC score for each client by combining the GRC values for loss and parameter differences
    weighted_score = grc_losses / weights[0] + grc_diffs / weights[1]

    return weighted_score


def select_clients(client_loaders, use_all_clients=False, num_select=None,
                   select_by_loss=False, global_model=None, grc=False):
    if grc:
        client_models = []

        client_losses = []
        for client_id, client_loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            local_state = local_train(local_model, client_loader, epochs=1, lr=0.01)
            client_models.append(local_model)
            loss, _ = evaluate(global_model, client_loader)
            client_losses.append(loss)


        grc_scores = calculate_GRC(global_model, client_models, client_losses)


        client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
        client_grc_pairs.sort(key=lambda x: x[1], reverse=True)


        selected = [client_id for client_id, _ in client_grc_pairs[:num_select]]
        return selected

    # å…¶ä½™é€‰æ‹©é€»è¾‘ä¿æŒä¸å˜
    if use_all_clients is True:
        print("Selecting all clients")
        return list(client_loaders.keys())

    if num_select is None:
        raise ValueError("If use_all_clients=False, num_select cannot be None!")

    if select_by_loss and global_model:
        client_losses = {}
        for client_id, loader in client_loaders.items():
            loss, _ = evaluate(global_model, loader)
            client_losses[client_id] = loss

        selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
        print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
    else:
        selected_clients = random.sample(list(client_loaders.keys()), num_select)
        print(f"Randomly selected {num_select} clients: {selected_clients}")

    return selected_clients


def update_communication_counts(communication_counts, selected_clients, event):
    """
    å®¢æˆ·ç«¯é€šä¿¡è®¡æ•°
    - event='receive' è¡¨ç¤ºå®¢æˆ·ç«¯æ¥æ”¶åˆ°å…¨å±€æ¨¡å‹
    - event='send' è¡¨ç¤ºå®¢æˆ·ç«¯ä¸Šä¼ æœ¬åœ°æ¨¡å‹
    - event='full_round' ä»…åœ¨å®¢æˆ·ç«¯å®Œæˆå®Œæ•´æ”¶å‘æ—¶å¢åŠ 
    """
    for client_id in selected_clients:
        communication_counts[client_id][event] += 1

        # ä»…å½“å®¢æˆ·ç«¯å®Œæˆä¸€æ¬¡å®Œæ•´çš„ send å’Œ receive æ—¶å¢åŠ  full_round
        if event == "send" and communication_counts[client_id]['receive'] > 0:
            communication_counts[client_id]['full_round'] += 1

def run_experiment(selection_method, rounds=100, num_selected_clients=2):
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # åŠ è½½ MNIST æ•°æ®é›†
    train_data, test_data = load_mnist_data()

    # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åªåŒ…å«ç‰¹å®šç±»åˆ«
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # Initialize global model, communication_counts, and results storage
    global_model = MLPModel()
    global_accuracies = []
    total_communication_counts = []
    csv_data = []

    # Initialize communication counters for all clients
    communication_counts = {client_id: {'send': 0, 'receive': 0, 'full_round': 0}
                            for client_id in client_loaders.keys()}

    for r in range(rounds):
        print(f"\nRound {r+1}")
        
        # Select clients based on the specified method:
        if selection_method == "fedGRA":
            selected_clients = select_clients(client_loaders, use_all_clients=False,
                                              num_select=num_selected_clients,
                                              select_by_loss=True, global_model=global_model, grc=True)
        elif selection_method == "high_loss":
            # Use loss-based selection without GRC (select top 2 highest loss clients)
            selected_clients = select_clients(client_loaders, use_all_clients=False,
                                              num_select=num_selected_clients,
                                              select_by_loss=True, global_model=global_model, grc=False)
        elif selection_method == "fedavg":
            # Use FedAvg with either random selection or all clients.
            # Here we assume using all clients or random selection for FedAvg.
            selected_clients = list(client_loaders.keys())  # Or random.sample(...)

        # Record receive communication count
        update_communication_counts(communication_counts, selected_clients, "receive")
        client_state_dicts = []

        # Perform local training on selected clients
        for client_id in selected_clients:
            client_loader = client_loaders[client_id]
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            local_train(local_model, client_loader, epochs=1, lr=0.01)
            client_state_dicts.append((client_id, local_model.state_dict()))
            update_communication_counts(communication_counts, [client_id], "send")
            print(f"Client {client_id} trained.")

        # Compute communication counts for this round
        total_send = sum(communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1)
                         for c in selected_clients)
        total_receive = sum(communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1)
                            for c in selected_clients)
        total_comm = total_send + total_receive
        total_communication_counts.append(total_comm)

        # Aggregate model updates
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

        # Evaluate global model
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)
        print(f"Test Accuracy: {accuracy:.2f}%")
        
        # Save round data; add a column indicating the method used if desired.
        csv_data.append([r+1, accuracy, total_comm])

    # Convert collected data to a DataFrame
    df = pd.DataFrame(csv_data, columns=['Round', f'Accuracy_{selection_method}', f'Comm_{selection_method}'])
    return df

def main_experiments():
    # Assume client_loaders, client_data_sizes, test_loader, etc., are already defined.
    
    rounds = 200
    # Run experiments for each method
    df_fedGRA = run_experiment("fedGRA", rounds, num_selected_clients=2)
    df_high_loss = run_experiment("high_loss", rounds, num_selected_clients=2)
    df_fedavg = run_experiment("fedavg", rounds, num_selected_clients=2)
    
    # Merge DataFrames on 'Round'
    df_combined = df_fedGRA.merge(df_high_loss, on='Round').merge(df_fedavg, on='Round')
    
    # Save to CSV for later inspection if needed
    df_combined.to_csv("comparison_results.csv", index=False)
    
    # Plot comparison: For example, plot Accuracy over Rounds for each method.
    plt.figure(figsize=(10,6))
    plt.plot(df_combined['Round'], df_combined['Accuracy_fedGRA'], marker='o', label='FedGRA')
    plt.plot(df_combined['Round'], df_combined['Accuracy_high_loss'], marker='s', label='High-Loss Filtering')
    plt.plot(df_combined['Round'], df_combined['Accuracy_fedavg'], marker='^', label='FedAvg')
    plt.xlabel("Rounds")
    plt.ylabel("Test Accuracy (%)")
    plt.title("Comparison of FedGRA, High-Loss Filtering, and FedAvg")
    plt.legend()
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    main_experiments()


# def main():
#     torch.manual_seed(0)
#     random.seed(0)
#     np.random.seed(0)

#     # åŠ è½½ MNIST æ•°æ®é›†
#     train_data, test_data = load_mnist_data()

#     # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åªåŒ…å«ç‰¹å®šç±»åˆ«
#     client_datasets, client_data_sizes = split_data_by_label(train_data)

#     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
#     client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
#                       for client_id, dataset in client_datasets.items()}
#     test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

#     # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
#     global_model = MLPModel()
#     global_accuracies = []  # è®°å½•æ¯è½®å…¨å±€æ¨¡å‹çš„æµ‹è¯•é›†å‡†ç¡®ç‡
#     total_communication_counts = []  # è®°å½•æ¯è½®å®¢æˆ·ç«¯é€šä¿¡æ¬¡æ•°
#     rounds = 300  # è”é‚¦å­¦ä¹ è½®æ•°
#     use_all_clients = False  # æ˜¯å¦è¿›è¡Œå®¢æˆ·ç«¯é€‰æ‹©
#     num_selected_clients = 2  # æ¯è½®é€‰æ‹©å®¢æˆ·ç«¯è®­ç»ƒæ•°é‡
#     use_loss_based_selection = True  # æ˜¯å¦æ ¹æ® loss é€‰æ‹©å®¢æˆ·ç«¯
#     grc = True

#     # åˆå§‹åŒ–é€šä¿¡è®¡æ•°å™¨
#     communication_counts = {}
#     for client_id in client_loaders.keys():
#         communication_counts[client_id] = {
#             'send': 0,  # è®°å½•å‘é€æ¬¡æ•°
#             'receive': 0,  # è®°å½•æ¥æ”¶æ¬¡æ•°
#             'full_round': 0  # è®°å½•å®Œæ•´æ”¶å‘æ¬¡æ•°
#         }
#     # å®éªŒæ•°æ®å­˜å‚¨ CSV
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     csv_filename = f"training_data_{timestamp}.csv"
#     csv_data = []

#     for r in range(rounds):
#         print(f"\nğŸ”„ ç¬¬ {r + 1} è½®èšåˆ")
#         # é€‰æ‹©å®¢æˆ·ç«¯
#         if r % 3 == 0:
#             selected_clients = select_clients(client_loaders, use_all_clients=use_all_clients,
#                                           num_select=num_selected_clients,
#                                           select_by_loss=use_loss_based_selection, global_model=global_model, grc=grc)

#         # è®°å½•å®¢æˆ·ç«¯æ¥æ”¶é€šä¿¡æ¬¡æ•°
#         update_communication_counts(communication_counts, selected_clients, "receive")
#         client_state_dicts = []

#         # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
#         for client_id in selected_clients:
#             client_loader = client_loaders[client_id]
#             local_model = MLPModel()
#             local_model.load_state_dict(global_model.state_dict())  # å¤åˆ¶å…¨å±€æ¨¡å‹å‚æ•°
#             local_state = local_train(local_model, client_loader, epochs=1, lr=0.01)  # è®­ç»ƒ 1 è½®
#             client_state_dicts.append((client_id, local_state))  # å­˜å‚¨ (å®¢æˆ·ç«¯ID, è®­ç»ƒåçš„å‚æ•°)

#             update_communication_counts(communication_counts, [client_id], "send")  # è®°å½•å®¢æˆ·ç«¯ä¸ŠæŠ¥é€šä¿¡æ¬¡æ•°

#             param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
#             print(f"  âœ… å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå®Œæˆ | æ ·æœ¬æ•°é‡: {sum(client_data_sizes[client_id].values())}")
#             print(f"  ğŸ“Œ å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å‚æ•°å‡å€¼: {param_mean}")

#         # è®¡ç®—æœ¬è½®é€šä¿¡æ¬¡æ•°
#         total_send = sum(
#             communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
#         total_receive = sum(
#             communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
#         total_comm = total_send + total_receive  # æ¯è½®ç‹¬ç«‹çš„æ€»é€šä¿¡æ¬¡æ•°
#         total_communication_counts.append(total_comm)  # è®°å½•å½“å‰è½®çš„é€šä¿¡æ¬¡æ•°

#         # èšåˆæ¨¡å‹å‚æ•°
#         global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

#         # # è®¡ç®—å…¨å±€æ¨¡å‹å‚æ•°å¹³å‡å€¼
#         # global_param_mean = {name: param.mean().item() for name, param in global_model.named_parameters()}
#         # print(f"ğŸ”„ è½® {r + 1} ç»“æŸåï¼Œå…¨å±€æ¨¡å‹å‚æ•°å‡å€¼: {global_param_mean}")

#         # è¯„ä¼°æ¨¡å‹
#         loss, accuracy = evaluate(global_model, test_loader)
#         global_accuracies.append(accuracy)
#         print(f"ğŸ“Š æµ‹è¯•é›†æŸå¤±: {loss:.4f} | æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")

#         # è®°å½•æ•°æ®åˆ° CSV
#         csv_data.append([
#             r + 1,
#             accuracy,
#             total_comm,
#             ",".join(map(str, selected_clients))
#         ])

#     # ä¿å­˜æ•°æ®åˆ° CSV æ–‡ä»¶
#     df = pd.DataFrame(csv_data, columns=[
#         'Round', 'Accuracy', 'Total communication counts', 'Selected Clients'
#     ])
#     df.to_csv(csv_filename, index=False)
#     print(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜è‡³ {csv_filename}")

#     # è¾“å‡ºæœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½
#     final_loss, final_accuracy = evaluate(global_model, test_loader)
#     print(f"\nğŸ¯ Loss of final model test dataset: {final_loss:.4f}")
#     print(f"ğŸ¯ Final model test set accuracy: {final_accuracy:.2f}%")

#     # è¾“å‡ºé€šä¿¡è®°å½•
#     print("\n Client Communication Statistics:")
#     for client_id, counts in communication_counts.items():
#         print(
#             f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")

#        # å¯è§†åŒ–å…¨å±€æ¨¡å‹å‡†ç¡®ç‡ vs è½®æ¬¡
#     plt.figure(figsize=(8, 5))
#     plt.plot(range(1, rounds + 1), global_accuracies, marker='o', linestyle='-', color='b', label="Test Accuracy")
#     plt.xlabel("Federated Learning Rounds")
#     plt.ylabel("Accuracy")
#     plt.title("Test Accuracy Over Federated Learning Rounds")
#     plt.legend()
#     plt.grid(True)
#     plt.show()

#     # å¯è§†åŒ–å…¨å±€æ¨¡å‹å‡†ç¡®ç‡ vs å®¢æˆ·ç«¯å®Œæ•´é€šä¿¡æ¬¡æ•°
#     plt.figure(figsize=(8, 5))
#     plt.plot(total_communication_counts, global_accuracies, marker='s', linestyle='-', color='r',
#              label="Test Accuracy vs. Communication")
#     plt.xlabel("Total Communication Count per Round")
#     plt.ylabel("Accuracy")
#     plt.title("Test Accuracy vs. Total Communication")
#     plt.legend()
#     plt.grid(True)
#     plt.show()

# if __name__ == "__main__":
#     main()