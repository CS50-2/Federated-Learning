# -*- coding: utf-8 -*-
"""Federated Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import ssl 


# 定义 MLP 模型
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)  # 第一层，输入维度 784 -> 200
        self.fc2 = nn.Linear(200, 200)      # 第二层，200 -> 200
        self.fc3 = nn.Linear(200, 10)       # 输出层，200 -> 10
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)  # 展平输入 (batch_size, 1, 28, 28) -> (batch_size, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # 直接输出，不使用 Softmax（因为 PyTorch 的 CrossEntropyLoss 里已经包含了）
        return x

# 加载 MNIST 数据集
def load_mnist_data(data_path="./data"):
    
    # Temporarily Skip SSL velidation step 
    ssl._create_default_https_context = ssl._create_unverified_context

    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("✅ MNIST 数据集已存在，跳过下载。")
    else:
        print("⬇️ 正在下载 MNIST 数据集...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    visualize_mnist_samples(train_data)
    return train_data, test_data

# 显示数据集示例图片
def visualize_mnist_samples(dataset, num_samples=10):
    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 1.2, 1.5))
    for i in range(num_samples):
        img, label = dataset[i]
        axes[i].imshow(img.squeeze(), cmap="gray")
        axes[i].set_title(label)
        axes[i].axis("off")
    plt.show()

# 分割 MNIST 数据，使每个客户端只包含某个数字类别
def split_data_by_label(dataset, num_clients=10):
    
    # Mannually set each client'id and corresponding dataset distribution 
    client_data_sizes = {
        0: {0: 600, 1: 700, 2: 600, 3: 600, 4: 500, 5: 500, 6: 100, 7: 100, 8: 100, 9: 100},
        1: {0: 700, 1: 600, 2: 600, 3: 600, 4: 500, 5: 100, 6: 100, 7: 100, 8: 100, 9: 600},
        2: {0: 500, 1: 600, 2: 700, 3: 600, 4: 100, 5: 100, 6: 100, 7: 100, 8: 600, 9: 500},
        3: {0: 600, 1: 600, 2: 500, 3: 100, 4: 100, 5: 100, 6: 100, 7: 500, 8: 500, 9: 700},
        4: {0: 600, 1: 500, 2: 100, 3: 100, 4: 100, 5: 100, 6: 600, 7: 700, 8: 500, 9: 500},
        5: {0: 500, 1: 100, 2: 100, 3: 100, 4: 100, 5: 600, 6: 500, 7: 600, 8: 700, 9: 600},
        6: {0: 100, 1: 100, 2: 100, 3: 100, 4: 700, 5: 500, 6: 600, 7: 500, 8: 500, 9: 600},
        7: {0: 100, 1: 100, 2: 100, 3: 600, 4: 500, 5: 600, 6: 500, 7: 600, 8: 500, 9: 100},
        8: {0: 100, 1: 100, 2: 500, 3: 500, 4: 600, 5: 500, 6: 600, 7: 500, 8: 100, 9: 100},
        9: {0: 100, 1: 700, 2: 600, 3: 600, 4: 600, 5: 500, 6: 600, 7: 100, 8: 100, 9: 100}
    }

    # Initialize an empty dictionary to store indices for each label (from 0 to 9) 
    label_to_indices = {}

    for label in range(10):
        label_to_indices[label] = []  

    # Loop through the dataset using enumerate to get both the index and the data item.
    # Each data item is a tuple (image, label).
    for index, (_, label) in enumerate(dataset):
        # Append the current index to the list corresponding to the data's label.
        label_to_indices[label].append(index)

    # Create an empty dictionary to store the data subset for each client.
    client_data_subsets = {}

    # Initialize a dictionary to record the actual number of samples allocated for each label in each client.
    client_actual_sizes = {}
    for client_id in range(num_clients):
        # For each client, initialize an empty dictionary to store the sample counts for labels 0 to 9.
        client_actual_sizes[client_id] = {}
        
        # For each label from 0 to 9, set the initial count to 0.
        for label in range(10):
            client_actual_sizes[client_id][label] = 0
    
    # Iterate over each client and assign data for the specified labels.
    for client_id, label_info in client_data_sizes.items():
        selected_indices = []
        
        for label, required_size in label_info.items():
            available_size = len(label_to_indices[label])
            
            # Determine the number of samples to select
            sample_size = min(available_size, required_size)
            
            # If the available sample size is less than the required size, print a warning message.
            if sample_size < required_size:
                print(f"⚠️ Warning: Not enough data for label {label}. Client {client_id} can only get {sample_size} samples (required {required_size}).")
            
            # Randomly select the determined number of indices and add the selected indices to the client's list.
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)
            
            # Record the actual number of samples allocated for this label for the current client.
            client_actual_sizes[client_id][label] = sample_size
        
        # Create a PyTorch Subset for this client using the selected indices.
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)


    print("\n📊 Actual data distribution per client:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"Client {client_id}: {label_sizes}")

    # Return both the client data subsets and the dictionary of actual sample sizes.
    return client_data_subsets, client_actual_sizes


# 本地训练函数
def local_train(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()

# 联邦平均聚合函数
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    total_data = sum(client_sizes.values())  # 计算所有客户端数据总量
    for key in global_dict.keys():
        global_dict[key] = sum(client_state[key] * (client_sizes[label] / total_data)
                               for (label, client_state) in client_state_dicts)
    global_model.load_state_dict(global_dict)
    return global_model

# 评估模型
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy

def main():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # 加载 MNIST 数据集
    train_data, test_data = load_mnist_data()

    # 生成客户端数据集，每个客户端只包含特定类别
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # 创建数据加载器
    client_loaders = {label: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for label, dataset in client_datasets}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # 初始化全局模型
    global_model = MLPModel()

    ###################### changed by TAIGE for top two loss client training ###########################

    # 4. Federated training
    rounds = 100
    for r in range(rounds):
        print(f"\n=== Round {r+1} FedAvg ===")

        # First, measure the average loss of each client under the current global model
        client_losses = {}
        for label, loader in client_loaders.items():
            loss_val = evaluate(global_model, loader)
            client_losses[label] = loss_val

        # Sort the clients by loss in descending order, and select the top 2
        sorted_clients = sorted(client_losses.items(), key=lambda x: x[1], reverse=True)
        top2 = sorted_clients[:2]  # top 2 (label, lossValue)
        selected_clients = [item[0] for item in top2]  # extract only the labels

        print("    All clients' loss:", client_losses)
        print("    Top 2 clients with the highest loss:", selected_clients)

        # Perform local training on these two selected clients
        client_state_dicts = []
        selected_client_sizes = {}
        for label in selected_clients:
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # copy the global model parameters
            updated_params = local_train(
                local_model,
                client_loaders[label],
                epochs=1,   
                lr=0.01     
            )
            client_state_dicts.append((label, updated_params))
            selected_client_sizes[label] = client_data_sizes[label]  # needed for weighted FedAvg

        # Aggregate with FedAvg
        fed_avg(global_model, client_state_dicts, selected_client_sizes)

        # Evaluate the current global model on the test set
        loss, acc = evaluate(global_model, test_loader)
        print(f"  [Round {r+1}] Test Loss = {loss:.4f}, Test Acc = {acc:.2f}%")


        ##########################################################################################

    # 输出最终模型的性能
    final_loss, final_accuracy = evaluate(global_model, test_loader)
    print(f"\n🎯 Loss of final model test dataset: {final_loss:.4f}")
    print(f"🎯 Final model test set accuracy: {final_accuracy:.2f}%")

if __name__ == "__main__":
    main()