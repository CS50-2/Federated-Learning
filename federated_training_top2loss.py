# -*- coding: utf-8 -*-
"""Federated Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import ssl 


# å®šä¹‰ MLP æ¨¡å‹
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)  # ç¬¬ä¸€å±‚ï¼Œè¾“å…¥ç»´åº¦ 784 -> 200
        self.fc2 = nn.Linear(200, 200)      # ç¬¬äºŒå±‚ï¼Œ200 -> 200
        self.fc3 = nn.Linear(200, 10)       # è¾“å‡ºå±‚ï¼Œ200 -> 10
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)  # å±•å¹³è¾“å…¥ (batch_size, 1, 28, 28) -> (batch_size, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # ç›´æ¥è¾“å‡ºï¼Œä¸ä½¿ç”¨ Softmaxï¼ˆå› ä¸º PyTorch çš„ CrossEntropyLoss é‡Œå·²ç»åŒ…å«äº†ï¼‰
        return x

# åŠ è½½ MNIST æ•°æ®é›†
def load_mnist_data(data_path="./data"):
    
    # Temporarily Skip SSL velidation step 
    ssl._create_default_https_context = ssl._create_unverified_context

    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
    else:
        print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    visualize_mnist_samples(train_data)
    return train_data, test_data

# æ˜¾ç¤ºæ•°æ®é›†ç¤ºä¾‹å›¾ç‰‡
def visualize_mnist_samples(dataset, num_samples=10):
    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 1.2, 1.5))
    for i in range(num_samples):
        img, label = dataset[i]
        axes[i].imshow(img.squeeze(), cmap="gray")
        axes[i].set_title(label)
        axes[i].axis("off")
    plt.show()

# åˆ†å‰² MNIST æ•°æ®ï¼Œä½¿æ¯ä¸ªå®¢æˆ·ç«¯åªåŒ…å«æŸä¸ªæ•°å­—ç±»åˆ«
def split_data_by_label(dataset):
    # è‡ªå®šä¹‰æ¯ä¸ªç±»åˆ«çš„æ•°æ®é‡
    client_data_sizes = {
        0: 5000,
        1: 7000,
        2: 6000,
        3: 8000,
        4: 4000,
        5: 9000,
        6: 3000,
        7: 10000,
        8: 7500,
        9: 6500
    }

    label_to_indices = {i: [] for i in range(10)}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç´¢å¼•

    # æ”¶é›†æ¯ä¸ªç±»åˆ«çš„æ•°æ®ç´¢å¼•
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    # ä¸ºæ¯ä¸ª client é€‰æ‹©å¯¹åº”ç±»åˆ«çš„æ•°æ®ï¼Œå¹¶è£å‰ªæˆéœ€è¦çš„æ•°é‡
    client_datasets = []
    for label, size in client_data_sizes.items():
        indices = label_to_indices[label][:size]  # å–å‰ size ä¸ªæ ·æœ¬
        client_datasets.append((label, torch.utils.data.Subset(dataset, indices)))  # å­˜å‚¨ (ç±»åˆ«, æ•°æ®é›†)

    print("ğŸ“Š å®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒ:", client_data_sizes)
    return client_datasets, client_data_sizes

# æœ¬åœ°è®­ç»ƒå‡½æ•°
def local_train(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()

# è”é‚¦å¹³å‡èšåˆå‡½æ•°
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    total_data = sum(client_sizes.values())  # è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®æ€»é‡
    for key in global_dict.keys():
        global_dict[key] = sum(client_state[key] * (client_sizes[label] / total_data)
                               for (label, client_state) in client_state_dicts)
    global_model.load_state_dict(global_dict)
    return global_model

# è¯„ä¼°æ¨¡å‹
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy

def main():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # åŠ è½½ MNIST æ•°æ®é›†
    train_data, test_data = load_mnist_data()

    # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åªåŒ…å«ç‰¹å®šç±»åˆ«
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    client_loaders = {label: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for label, dataset in client_datasets}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    global_model = MLPModel()

    ###################### changed by TAIGE for top two loss client training ###########################

    # 4. Federated training
    rounds = 100
    for r in range(rounds):
        print(f"\n=== Round {r+1} FedAvg ===")

        # (a) First, measure the average loss of each client under the current global model
        client_losses = {}
        for label, loader in client_loaders.items():
            loss_val = evaluate(global_model, loader)
            client_losses[label] = loss_val

        # (b) Sort the clients by loss in descending order, and select the top 2
        sorted_clients = sorted(client_losses.items(), key=lambda x: x[1], reverse=True)
        top2 = sorted_clients[:2]  # top 2 (label, lossValue)
        selected_clients = [item[0] for item in top2]  # extract only the labels

        print("    All clients' loss:", client_losses)
        print("    Top 2 clients with the highest loss:", selected_clients)

        # (c) Perform local training on these two selected clients
        client_state_dicts = []
        selected_client_sizes = {}
        for label in selected_clients:
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # copy the global model parameters
            updated_params = local_train(
                local_model,
                client_loaders[label],
                epochs=1,   # you can increase this for more local training
                lr=0.01     # you can tune this learning rate
            )
            client_state_dicts.append((label, updated_params))
            selected_client_sizes[label] = client_data_sizes[label]  # needed for weighted FedAvg

        # (d) Aggregate with FedAvg
        fed_avg(global_model, client_state_dicts, selected_client_sizes)

        # (e) Evaluate the current global model on the test set
        loss, acc = evaluate(global_model, test_loader)
        print(f"  [Round {r+1}] Test Loss = {loss:.4f}, Test Acc = {acc:.2f}%")


        ##########################################################################################

    # è¾“å‡ºæœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½
    final_loss, final_accuracy = evaluate(global_model, test_loader)
    print(f"\nğŸ¯ Loss of final model test dataset: {final_loss:.4f}")
    print(f"ğŸ¯ Final model test set accuracy: {final_accuracy:.2f}%")

if __name__ == "__main__":
    main()