import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import csv
import pandas as pd
from datetime import datetime
import torch.nn.functional as F

# å®šä¹‰ MLP æ¨¡å‹
class MLPModel(nn.Module):
    def __init__(self, input_size=28*28, hidden_size=200, num_classes=10):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

def compute_training_flops(model, frozen_layers=None):
    """
    è®¡ç®—æ•´ä¸ªæ¨¡å‹ä¸€æ¬¡è®­ç»ƒ(iter)çš„ FLOPsï¼ˆå‰å‘+åå‘+SGDæ›´æ–°ï¼‰ï¼Œ
    frozen_layers åˆ—è¡¨ä¸­çš„å±‚åªè®¡ç®—å‰å‘ FLOPsã€‚
    è¿”å› (total_flops, per_layer_dict)
    """
    if frozen_layers is None:
        frozen_layers = []

    total_flops = 0
    flops_dict = {}

    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            in_f = module.in_features
            out_f = module.out_features

            # 1) å‰å‘
            fwd = 2 * in_f * out_f + out_f

            if name in frozen_layers:
                layer_flops = fwd
            else:
                # 2) åå‘
                bw_w    = 2 * in_f * out_f      # æƒé‡æ¢¯åº¦
                bw_in   =     in_f * out_f      # è¾“å…¥æ¢¯åº¦
                bw_b    =         out_f         # åç½®æ¢¯åº¦

                # 3) SGD æ›´æ–°
                upd_w   = 2 * in_f * out_f      # æƒé‡æ›´æ–°
                upd_b   = 2 * out_f             # åç½®æ›´æ–°

                layer_flops = fwd + (bw_w + bw_in + bw_b) + (upd_w + upd_b)

            flops_dict[name] = layer_flops
            total_flops += layer_flops

    return total_flops, flops_dict


# åŠ è½½ MNIST æ•°æ®é›†
def load_mnist_data(data_path="./data"):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
    else:
        print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    # visualize_mnist_samples(train_data)
    return train_data, test_data


def split_data_by_label(dataset, num_clients=10):
    """
    æ‰‹åŠ¨åˆ’åˆ†æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å« 10 ä¸ªç±»åˆ«ï¼Œå¹¶è‡ªå®šä¹‰æ ·æœ¬æ•°é‡ã€‚
    :param dataset: åŸå§‹æ•°æ®é›†ï¼ˆå¦‚ MNISTï¼‰
    :param num_clients: å®¢æˆ·ç«¯æ€»æ•°
    :return: (å®¢æˆ·ç«¯æ•°æ®é›†, å®¢æˆ·ç«¯æ•°æ®å¤§å°)
    """
    # æ‰‹åŠ¨åˆ’åˆ†çš„æ ·æœ¬æ•°é‡ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯ 10 ä¸ªç±»åˆ«çš„æ•°æ®é‡ï¼‰
    client_data_sizes = {
        0: {0: 600},
        1: {1: 700},
        2: {2: 500},
        3: {3: 600},
        4: {4: 600},
        5: {5: 500},
        6: {6: 500},
        7: {7: 500},
        8: {8: 500},
        9: {9: 500}
    }

    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°æ®ç´¢å¼•
    label_to_indices = {i: [] for i in range(10)}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç´¢å¼•
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    # åˆå§‹åŒ–å®¢æˆ·ç«¯æ•°æ®å­˜å‚¨
    client_data_subsets = {}
    client_actual_sizes = {i: {label: 0 for label in range(10)} for i in range(num_clients)}  # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡

    # éå†æ¯ä¸ªå®¢æˆ·ç«¯ï¼Œä¸ºå…¶åˆ†é…æŒ‡å®šç±»åˆ«çš„æ•°æ®
    for client_id, label_info in client_data_sizes.items():
        selected_indices = []  # ä¸´æ—¶å­˜å‚¨è¯¥å®¢æˆ·ç«¯æ‰€æœ‰é€‰ä¸­çš„ç´¢å¼•
        for label, size in label_info.items():
            # ç¡®ä¿ä¸è¶…å‡ºç±»åˆ«æ•°æ®é›†å®é™…å¤§å°
            available_size = len(label_to_indices[label])
            sample_size = min(available_size, size)

            if sample_size < size:
                print(f"âš ï¸ è­¦å‘Šï¼šç±»åˆ« {label} çš„æ•°æ®ä¸è¶³ï¼Œå®¢æˆ·ç«¯ {client_id} åªèƒ½è·å– {sample_size} æ¡æ ·æœ¬ï¼ˆéœ€æ±‚ {size} æ¡ï¼‰")

            # ä»è¯¥ç±»åˆ«ä¸­éšæœºæŠ½å–æ ·æœ¬
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)

            # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡
            client_actual_sizes[client_id][label] = sample_size

        # åˆ›å»º PyTorch Subset
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)

    # æ‰“å°æ¯ä¸ªå®¢æˆ·ç«¯çš„å®é™…åˆ†é…æ•°æ®é‡
    print("\nğŸ“Š æ¯ä¸ªå®¢æˆ·ç«¯å®é™…æ•°æ®åˆ†å¸ƒ:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"å®¢æˆ·ç«¯ {client_id}: {label_sizes}")

    return client_data_subsets, client_actual_sizes


# æœ¬åœ°è®­ç»ƒå‡½æ•°
def local_train(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()



#  è”é‚¦å¹³å‡èšåˆå‡½æ•°
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    subkey = [sublist[0] for sublist in client_state_dicts]
    new_client_sizes = dict(([(key, client_sizes[key]) for key in subkey]))
    total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())  # è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®æ€»é‡
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
            for (client_id, client_state) in client_state_dicts
        )
    global_model.load_state_dict(global_dict)
    return global_model


# è¯„ä¼°æ¨¡å‹
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy

def entropy_weight(l):
    entropies = []
    for X in l:
        P = X / (np.sum(X) + 1e-12)  # å½’ä¸€åŒ–å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
        K = 1 / np.log(len(X))
        E = -K * np.sum(P * np.log(P + 1e-12))  # è®¡ç®—ç†µï¼Œè¶Šå¤§è¶Šæ— åŒºåˆ†åº¦
        entropies.append(E)

        # ç®—ä¿¡æ¯é‡
    information_gain = [1 - e for e in entropies]
    # å½’ä¸€åŒ–
    sum_ig = sum(information_gain)
    weights = [ig / sum_ig for ig in information_gain]

    return weights


def calculate_GRC(global_model, client_models, client_losses):
    """
    æ­£ç¡®è®¡ç®— GRC åˆ†æ•°ï¼Œå¹¶ä¿®æ­£åç»­æ­¥éª¤
    """

    # 1. è®¡ç®—å®¢æˆ·ç«¯æŒ‡æ ‡ï¼ˆå‚æ•°å·®å¼‚ï¼‰
    param_diffs = []
    for model in client_models:
        diff = 0.0
        for g_param, l_param in zip(global_model.parameters(), model.parameters()):
            diff += torch.norm(g_param - l_param).item()
        param_diffs.append(diff)

    # 2. å¯¹ losses å’Œ diffs è¿›è¡Œæ­£ç¡® mapping
    def map_sequence_loss(sequence):
        max_val = max(sequence)
        min_val = min(sequence)
        return [(max_val - x) / (max_val + min_val) for x in sequence]  # ã€âœ”ã€‘è´Ÿç›¸å…³

    def map_sequence_diff(sequence):
        max_val = max(sequence)
        min_val = min(sequence)
        return [(x - min_val) / (max_val + min_val) for x in sequence]  # ã€âœ”ã€‘æ­£ç›¸å…³

    client_losses = map_sequence_loss(client_losses)
    param_diffs = map_sequence_diff(param_diffs)

    # 3. æ„å»ºå‚è€ƒåºåˆ— (ç†æƒ³å€¼ = 1)
    ref_loss = 1.0
    ref_diff = 1.0

    # 4. è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„ Î”
    all_deltas = []
    for loss, diff in zip(client_losses, param_diffs):
        all_deltas.append(abs(loss - ref_loss))
        all_deltas.append(abs(diff - ref_diff))
    max_delta = max(all_deltas)
    min_delta = min(all_deltas)

    # 5. è®¡ç®—ç°è‰²å…³è”ç³»æ•° (GRC)ï¼ŒÏ=0.5
    grc_losses = []
    grc_diffs = []
    for loss, diff in zip(client_losses, param_diffs):
        delta_loss = abs(loss - ref_loss)
        delta_diff = abs(diff - ref_diff)

        grc_loss = (min_delta + 0.5 * max_delta) / (delta_loss + 0.5 * max_delta)
        grc_diff = (min_delta + 0.5 * max_delta) / (delta_diff + 0.5 * max_delta)

        grc_losses.append(grc_loss)
        grc_diffs.append(grc_diff)

    grc_losses = np.array(grc_losses)
    grc_diffs = np.array(grc_diffs)

    # 6. è®¡ç®—ç†µæƒï¼ˆåŸºäºåŸå§‹mappedæ•°æ®ï¼‰
    grc_metrics = np.vstack([client_losses, param_diffs])  # ã€æ³¨æ„ã€‘è¿™é‡Œ shape æ˜¯ (2, n_clients)
    weights = entropy_weight(grc_metrics)  # ã€âœ”ã€‘ç†µæƒç®—çš„æ˜¯åŸmappedæŒ‡æ ‡ï¼Œä¸æ˜¯grcï¼

    # 7. åŠ æƒæ±‚å’Œï¼Œæ³¨æ„æ˜¯ã€ä¹˜æ³•ã€‘ä¸æ˜¯é™¤æ³•
    weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]  # ã€ä¿®æ”¹ç‚¹ã€‘ä¹˜æ³•ï¼

    return weighted_score, weights


def select_clients(client_loaders, use_all_clients=False, num_select=None,
                   select_by_loss=False, global_model=None, grc=False):
    if grc:  # ä½¿ç”¨ GRC é€‰æ‹©å®¢æˆ·ç«¯
        client_models = []
        # 1. è®­ç»ƒæœ¬åœ°æ¨¡å‹å¹¶è®¡ç®—æŸå¤±
        client_losses = []
        for client_id, client_loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())  # åŒæ­¥å…¨å±€æ¨¡å‹
            local_train(local_model, client_loader, epochs=1, lr=0.01)
            client_models.append(local_model)
            loss, _ = evaluate(local_model, client_loader)
            client_losses.append(loss)

        # 2. è®¡ç®— GRC åˆ†æ•°
        grc_scores, grc_weights = calculate_GRC(global_model, client_models, client_losses)
        select_clients.latest_weights = grc_weights  # è®°å½•æƒé‡

        # 3. æŒ‰ GRC åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼ŒGRCè¶Šé«˜è¡¨ç¤ºè¶Šå¥½ï¼‰
        client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
        client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # é™åºæ’åº

        # 4. é€‰æ‹© GRC æœ€é«˜çš„å‰ num_select ä¸ªå®¢æˆ·ç«¯
        selected = [client_id for client_id, _ in client_grc_pairs[:num_select]]
        return selected

    # å…¶ä½™é€‰æ‹©é€»è¾‘ä¿æŒä¸å˜

    if use_all_clients is True:
        print("Selecting all clients")
        return list(client_loaders.keys())

    if num_select is None:
        raise ValueError("If use_all_clients=False, num_select cannot be None!")

    if select_by_loss and global_model:
        client_losses = {}
        for client_id, loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            local_train(local_model, loader, epochs=1, lr=0.01)
            loss, _ = evaluate(local_model, loader)
            client_losses[client_id] = loss
        selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
        print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
    else:
        selected_clients = random.sample(list(client_loaders.keys()), num_select)
        print(f"Randomly selected {num_select} clients: {selected_clients}")

    return selected_clients


def update_communication_counts(communication_counts, selected_clients, event):
    """
    å®¢æˆ·ç«¯é€šä¿¡è®¡æ•°
    - event='receive' è¡¨ç¤ºå®¢æˆ·ç«¯æ¥æ”¶åˆ°å…¨å±€æ¨¡å‹
    - event='send' è¡¨ç¤ºå®¢æˆ·ç«¯ä¸Šä¼ æœ¬åœ°æ¨¡å‹
    - event='full_round' ä»…åœ¨å®¢æˆ·ç«¯å®Œæˆå®Œæ•´æ”¶å‘æ—¶å¢åŠ 
    """
    for client_id in selected_clients:
        communication_counts[client_id][event] += 1

        # ä»…å½“å®¢æˆ·ç«¯å®Œæˆä¸€æ¬¡å®Œæ•´çš„ send å’Œ receive æ—¶å¢åŠ  full_round
        if event == "send" and communication_counts[client_id]['receive'] > 0:
            communication_counts[client_id]['full_round'] += 1

def perform_local_training(selected_clients, client_loaders, global_model, client_data_sizes, communication_counts):
    client_state_dicts = []
    round_tflops_list = []
    for client_id in selected_clients:
        client_loader = client_loaders[client_id]
        local_model = MLPModel()
        local_model.load_state_dict(global_model.state_dict())
        
        # æœ¬åœ°è®­ç»ƒ
        local_state = local_train(local_model, client_loader, epochs=1, lr=0.1)
        client_state_dicts.append((client_id, local_state))
        update_communication_counts(communication_counts, [client_id], "send")
 
        param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
        print(f"  âœ… å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå®Œæˆ | æ ·æœ¬æ•°é‡: {sum(client_data_sizes[client_id].values())}")
        print(f"  ğŸ“Œ å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å‚æ•°å‡å€¼: {param_mean}")


    return client_state_dicts, round_tflops_list

def run_experiment(grc, rounds, client_loaders, client_data_sizes, test_loader):
    global_model = MLPModel()
    results = []
    communication_counts = {client_id: {'send': 0, 'receive': 0, 'full_round': 0} for client_id in client_loaders.keys()}
    

    for r in range(rounds):
        print(f"\nğŸ”„ å®éªŒ grc={grc}ï¼‰ç¬¬ {r + 1} è½®")
        selected_clients = select_clients(client_loaders, use_all_clients=False, num_select=2,
                                          select_by_loss=False, global_model=global_model, grc=grc)
        print(f"  é€‰ä¸­å®¢æˆ·ç«¯: {selected_clients}")
        update_communication_counts(communication_counts, selected_clients, "receive")
        client_state_dicts, round_tflops_list = perform_local_training(
            selected_clients, client_loaders, global_model,
            client_data_sizes, communication_counts
        )
        total_send = sum(communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_receive = sum(communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_comm = total_send + total_receive
        if results:
            total_comm += results[-1]["TotalComm"]
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
        loss, accuracy = evaluate(global_model, test_loader)
        avg_tflops = np.mean(round_tflops_list) if round_tflops_list else 0
        result_dict = {
            "Round": r+1,
            "Accuracy": accuracy,
            "TotalComm": total_comm,
            "AvgTFlops": avg_tflops
        }
        results.append(result_dict)
        print(f"  è½® {r+1} æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%ï¼Œå¹³å‡ t-FLOPs: {avg_tflops:.0f}")
    return results

def main():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)
    
    # åŠ è½½æ•°æ®é›†ã€åˆ’åˆ†æ•°æ®ä¸æ„å»º DataLoader
    train_data, test_data = load_mnist_data()
    client_datasets, client_data_sizes = split_data_by_label(train_data)
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)
    
    rounds = 200  # è”é‚¦å­¦ä¹ è½®æ•°
    
    # setting experiments 
    experiment_1 = run_experiment(
        grc=True,
        rounds=rounds,
        client_loaders=client_loaders,
        client_data_sizes=client_data_sizes,
        test_loader=test_loader
    )
    
    # # Combine two results into one 
    # combined_results = []
    # for r in range(rounds):
    #     row = {
    #         "Round": exp_no_freeze[r]["Round"],
    #         "Accuracy_NoFreeze": exp_no_freeze[r]["Accuracy"],
    #         "TotalComm_NoFreeze": exp_no_freeze[r]["TotalComm"],
    #         "Accuracy_Freeze": exp_freeze[r]["Accuracy"],
    #         "TotalComm_Freeze": exp_freeze[r]["TotalComm"],
    #     }
    #     combined_results.append(row)
    
    df = pd.DataFrame(experiment_1)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"combined_training_data_{timestamp}.csv"
    df.to_csv(csv_filename, index=False)
    print(f"\næ‰€æœ‰å®éªŒç»“æœå·²ä¿å­˜åˆ° {csv_filename}")

    # # ç»˜åˆ¶æµ‹è¯•å‡†ç¡®ç‡éšè½®æ¬¡å˜åŒ–å¯¹æ¯”å›¾
    # plt.figure(figsize=(10, 6))
    # plt.plot(df["Round"], df["Accuracy_NoFreeze"], marker='o', label="No Freeze")
    # # plt.plot(df["Round"], df["Accuracy_Freeze"], marker='s', label="Freeze (APF on fc2)")
    # plt.xlabel("Federated Learning Rounds")
    # plt.ylabel("Test Accuracy")
    # plt.title("Test Accuracy vs Rounds")
    # plt.legend()
    # plt.grid(True)
    # plt.show()

if __name__ == "__main__":
    main()