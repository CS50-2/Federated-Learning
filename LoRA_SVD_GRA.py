# -*- coding: utf-8 -*-
"""Federated Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import ssl 
from datetime import datetime
import pandas as pd
import torch.nn.functional as F
import time


class LoRALayer(nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha, use_svd=False, linear=None):
        super().__init__()
        self.use_svd = use_svd  # Flag to decide if SVD should be used
        self.alpha = alpha
        self.rank = rank
        self.scaling = alpha / rank

        if use_svd:
            # Initialize LoRA parameters using SVD
            source_linear = linear.weight.data
            source_linear = source_linear.float()
            U, S, V = torch.linalg.svd(source_linear)  # Perform SVD
            U_r = U[:, :rank]  # Take the first 'rank' singular vectors
            S_r = torch.diag(S[:rank])  # Create the diagonal matrix for the top 'rank' singular values
            V_r = V[:, :rank].t()  # Take the first 'rank' singular vectors
            
            # Use SVD components to initialize A and B
            self.A = nn.Parameter(U_r.contiguous())  # The A matrix
            self.B = nn.Parameter((S_r @ V_r).contiguous())  # The B matrix (rank x out_dim) using S_r * V_r^T
        else:
            # Random initialization
            self.A = nn.Parameter(torch.zeros(in_dim, rank))
            self.B = nn.Parameter(torch.zeros(rank, out_dim))
            nn.init.normal_(self.A, mean=0.0, std=0.02)
            nn.init.zeros_(self.B)

    def forward(self, x):
        x=self.scaling*(x@self.A@self.B)
        return x


# class LinearWithLoRA(nn.Module):
#     def __init__(self, linear, rank, alpha, use_svd=False):
#         super().__init__()
#         self.linear = linear
#         self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha, use_svd=use_svd, linear=linear)

#     def forward(self, x):
#         # Apply LoRA to original weights
#         combined_weight = self.linear.weight + self.lora.alpha * (self.lora.A @ self.lora.B).T
#         return F.linear(x, combined_weight, self.linear.bias)

    
# 定义 MLP 模型
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)  # 第一层，输入维度 784 -> 200
        self.fc2 = nn.Linear(200, 200)  # 第二层，200 -> 200
        self.fc3 = nn.Linear(200, 10)  # 输出层，200 -> 10
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)  # 展平输入 (batch_size, 1, 28, 28) -> (batch_size, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # 直接输出，不使用 Softmax（因为 PyTorch 的 CrossEntropyLoss 里已经包含了）
        return x
    
    # 定义带 LoRA 的 MLP 模型
class LoRAMLPModel(nn.Module):
    def __init__(self, base_model, rank=4, alpha=1):
        super(LoRAMLPModel, self).__init__()
        self.base_model = base_model
        # 冻结基础模型参数
        for param in base_model.parameters():
            param.requires_grad = False

        # 添加 LoRA 适配器
        self.lora_fc1 = LoRALayer(28 * 28, 200, rank=rank, alpha=alpha)
        self.lora_fc2 = LoRALayer(200, 200, rank=rank, alpha=alpha)
        self.lora_fc3 = LoRALayer(200, 10, rank=rank, alpha=alpha)

    def forward(self, x):
        x = x.view(x.size(0), -1)  # 展平输入

        # 前向传播结合基础模型和 LoRA 部分
        fc1_out = self.base_model.relu(self.base_model.fc1(x) + self.lora_fc1(x))
        fc2_out = self.base_model.relu(self.base_model.fc2(fc1_out) + self.lora_fc2(fc1_out))
        out = self.base_model.fc3(fc2_out) + self.lora_fc3(fc2_out)

        return out


# 加载 MNIST 数据集
def load_mnist_data(data_path="./data"):
    
    # Temporarily Skip SSL velidation step 
    ssl._create_default_https_context = ssl._create_unverified_context

    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("✅ MNIST 数据集已存在，跳过下载。")
    else:
        print("⬇️ 正在下载 MNIST 数据集...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    return train_data, test_data

# 分割 MNIST 数据，使每个客户端只包含某个数字类别
def split_data_by_label(dataset, num_clients=10):
    
    # Mannually set each client'id and corresponding dataset distribution 
    # client_data_sizes = {
    #     0: {0: 600, 1: 700, 2: 600, 3: 600, 4: 500, 5: 500, 6: 100, 7: 100, 8: 100, 9: 100},
    #     1: {0: 700, 1: 600, 2: 600, 3: 600, 4: 500, 5: 100, 6: 100, 7: 100, 8: 100, 9: 600},
    #     2: {0: 500, 1: 600, 2: 700, 3: 600, 4: 100, 5: 100, 6: 100, 7: 100, 8: 600, 9: 500},
    #     3: {0: 600, 1: 600, 2: 500, 3: 100, 4: 100, 5: 100, 6: 100, 7: 500, 8: 500, 9: 700},
    #     4: {0: 600, 1: 500, 2: 100, 3: 100, 4: 100, 5: 100, 6: 600, 7: 700, 8: 500, 9: 500},
    #     5: {0: 500, 1: 100, 2: 100, 3: 100, 4: 100, 5: 600, 6: 500, 7: 600, 8: 700, 9: 600},
    #     6: {0: 100, 1: 100, 2: 100, 3: 100, 4: 700, 5: 500, 6: 600, 7: 500, 8: 500, 9: 600},
    #     7: {0: 100, 1: 100, 2: 100, 3: 600, 4: 500, 5: 600, 6: 500, 7: 600, 8: 500, 9: 100},
    #     8: {0: 100, 1: 100, 2: 500, 3: 500, 4: 600, 5: 500, 6: 600, 7: 500, 8: 100, 9: 100},
    #     9: {0: 100, 1: 700, 2: 600, 3: 600, 4: 600, 5: 500, 6: 600, 7: 100, 8: 100, 9: 100}
    # }

    client_data_sizes = {
        0: {0: 600},
        1: {1: 600},
        2: {2: 500},
        3: {3: 600},
        4: {4: 600},
        5: {5: 500},
        6: {6: 100},
        7: {7: 100},
        8: {8: 100},
        9: {9: 100}
    }

    # Initialize an empty dictionary to store indices for each label (from 0 to 9) 
    label_to_indices = {}

    for label in range(10):
        label_to_indices[label] = []  

    # Loop through the dataset using enumerate to get both the index and the data item.
    # Each data item is a tuple (image, label).
    for index, (_, label) in enumerate(dataset):
        # Append the current index to the list corresponding to the data's label.
        label_to_indices[label].append(index)

    # Create an empty dictionary to store the data subset for each client.
    client_data_subsets = {}

    # Initialize a dictionary to record the actual number of samples allocated for each label in each client.
    client_actual_sizes = {}
    for client_id in range(num_clients):
        # For each client, initialize an empty dictionary to store the sample counts for labels 0 to 9.
        client_actual_sizes[client_id] = {}
        
        # For each label from 0 to 9, set the initial count to 0.
        for label in range(10):
            client_actual_sizes[client_id][label] = 0
    
    # Iterate over each client and assign data for the specified labels.
    for client_id, label_info in client_data_sizes.items():
        selected_indices = []
        
        for label, required_size in label_info.items():
            available_size = len(label_to_indices[label])
            
            # Determine the number of samples to select
            sample_size = min(available_size, required_size)
            
            # If the available sample size is less than the required size, print a warning message.
            if sample_size < required_size:
                print(f"⚠️ Warning: Not enough data for label {label}. Client {client_id} can only get {sample_size} samples (required {required_size}).")
            
            # Randomly select the determined number of indices and add the selected indices to the client's list.
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)
            
            # Record the actual number of samples allocated for this label for the current client.
            client_actual_sizes[client_id][label] = sample_size
        
        # Create a PyTorch Subset for this client using the selected indices.
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)


    print("\n📊 Actual data distribution per client:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"Client {client_id}: {label_sizes}")

    # Return both the client data subsets and the dictionary of actual sample sizes.
    return client_data_subsets, client_actual_sizes


def local_train(model, train_loader, epochs=5, lr=0.1, is_LoRA=False):
    """Train the model on a local client, freezing original layers if using LoRA."""

    if is_LoRA: 
        lora_model = LoRAMLPModel(model)

        criterion = nn.CrossEntropyLoss()

        # Only update LoRA parameters (A and B)
        optimizer = optim.SGD([p for n, p in lora_model.named_parameters() if p.requires_grad], lr=lr)

        model.train()
        loss_sq_sum = 0.0

        for epoch in range(epochs):
            for batch_x, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                loss_sq_sum += loss.item() ** 2

        h_i = loss_sq_sum  # 累积loss平方和
        return model, h_i
    else: 
        criterion = nn.CrossEntropyLoss()

        optimizer = optim.SGD(model.parameters(), lr=lr)

        model.train()

        for epoch in range(epochs):
            for batch_x, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()

        return model.state_dict()

# 联邦平均聚合函数
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    
    # Extract client IDs from client_state_dicts tuples.
    subkey = [sublist[0] for sublist in client_state_dicts]
    
    # Create a new dictionary of client sizes using only the clients that were selected.
    new_client_sizes = dict([(key, client_sizes[key]) for key in subkey])
    
    # Calculate the total number of samples across all selected clients.
    # Here, each client size is now assumed to be a nested dictionary (per label), so we sum the values for each client.
    total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())
    
    # Update each parameter in the global model.
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
            for (client_id, client_state) in client_state_dicts
        )
    
    global_model.load_state_dict(global_dict)
    return global_model

# 评估模型
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy

# 熵权法实现

def entropy_weight(l):
    l = np.array(l)
    
    # Step 1: Min-Max 归一化（避免负值和爆炸）
    X_norm = (l - l.min(axis=1, keepdims=True)) / (l.max(axis=1, keepdims=True) - l.min(axis=1, keepdims=True) + 1e-12)

    # Step 2: 转为概率矩阵 P_ki
    P = X_norm / (X_norm.sum(axis=1, keepdims=True) + 1e-12)

    # Step 3: 计算熵
    K = 1 / np.log(X_norm.shape[1])
    E = -K * np.sum(P * np.log(P + 1e-12), axis=1)  # shape: (2,)

    # Step 4: 计算信息效用值 & 权重
    d = 1 - E
    weights = d / np.sum(d)
    return weights.tolist()


# 灰色关联度实现
def calculate_GRC(global_model, client_models, client_losses):
    """
    计算 GRC 分数 + 熵权法权重
    修正：
      - 映射顺序错误
      - 熵权法使用错误指标
    """
    # 正确写法：使用整体参数向量计算 L2 范数（符合文献）
    param_diffs = []
    for model in client_models:
        global_vec = torch.nn.utils.parameters_to_vector(global_model.parameters()).detach()

        local_vec = torch.nn.utils.parameters_to_vector(model.parameters()).detach()
        diff = torch.norm(local_vec - global_vec).item()
        param_diffs.append(diff)

    # 2. 映射原始指标到 [0, 1] 区间（为熵权法准备）
    def map_sequence_loss(sequence):
        max_val, min_val = max(sequence), min(sequence)
        denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
        return [(max_val - x) / denom for x in sequence]  # 越小越好，负相关

    def map_sequence_diff(sequence):
        max_val, min_val = max(sequence), min(sequence)
        denom = max_val - min_val if abs(max_val - min_val) > 1e-8 else 1e-8
        return [(x - min_val) / denom for x in sequence]  # 越大越好，正相关

    # 用于 GRC 的映射
    mapped_losses = map_sequence_loss(client_losses)
    mapped_diffs = map_sequence_diff(param_diffs)

    # 3. 熵权法计算权重（根据映射值，非 GRC）
    grc_metrics = np.vstack([mapped_losses, mapped_diffs])  # shape: (2, n_clients)
    weights = entropy_weight(grc_metrics)  # w_loss, w_diff

    # 4. 计算 GRC 分数（ξki），参考值为 1
    ref_loss, ref_diff = 1.0, 1.0
    delta_losses = [abs(x - ref_loss) for x in mapped_losses]
    delta_diffs = [abs(x - ref_diff) for x in mapped_diffs]
    all_deltas = delta_losses + delta_diffs
    max_delta, min_delta = max(all_deltas), min(all_deltas)

    grc_losses = []
    grc_diffs = []
    rho = 0.5  # 区分度因子
    for d_loss, d_diff in zip(delta_losses, delta_diffs):
        grc_loss = (min_delta + rho * max_delta) / (d_loss + rho * max_delta)
        grc_diff = (min_delta + rho * max_delta) / (d_diff + rho * max_delta)
        grc_losses.append(grc_loss)
        grc_diffs.append(grc_diff)

    # 5. 加权求和得到最终 GRC 分数
    grc_losses = np.array(grc_losses)
    grc_diffs = np.array(grc_diffs)
    weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]

    # 调试（每个客户端的 loss、diff、得分）
    print("\n GRC得分]")
    for i in range(len(client_models)):
        print(f"Client {i} | loss: {client_losses[i]:.4f}, diff: {param_diffs[i]:.4f}, "
              f"mapped_loss: {mapped_losses[i]:.4f}, mapped_diff: {mapped_diffs[i]:.4f}, "
              f"GRC: {weighted_score[i]:.4f}")
    print(f"熵权法权重: w_loss = {weights[0]:.4f}, w_diff = {weights[1]:.4f}")

    return weighted_score, weights


def select_clients(client_loaders, use_all_clients=False, num_select=None,
                   select_by_loss=False, global_model=None, grc=True, is_LoRA=False, use_svd=False):
    if grc:  # 使用 GRC 选择客户端
        client_models = []
        # 1. 训练本地模型并计算损失
        client_losses = []
        for client_id, client_loader in client_loaders.items():
            if is_LoRA:
                # 使用LoRA训练 - 减少训练成本
                trained_lora_model, h_i = local_train(global_model, client_loader, epochs=5, lr=0.01, is_LoRA=is_LoRA)
                client_models.append(trained_lora_model)
                client_losses.append(h_i)
            else: 
                local_model = MLPModel() # Define if using LoRA or SVD
                local_model.load_state_dict(global_model.state_dict())  # 同步全局模型
                local_train(local_model, client_loader, epochs=1, lr=0.01, is_LoRA=is_LoRA) # Define if using LoRA 
                client_models.append(local_model)
                loss, _ = evaluate(local_model, client_loader)
                client_losses.append(loss)

        # 2. 计算 GRC 分数
        grc_scores, grc_weights = calculate_GRC(global_model, client_models, client_losses)
        select_clients.latest_weights = grc_weights  # 记录权重

        # 3. 按 GRC 分数排序（从高到低，GRC越高表示越好）
        client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
        client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # 降序排序

        # 4. 选择 GRC 最高的前 num_select 个客户端
        selected = [client_id for client_id, _ in client_grc_pairs[:num_select]]
        return selected

    # 其余选择逻辑保持不变
    if use_all_clients is True:
        print("Selecting all clients")
        return list(client_loaders.keys())

    if num_select is None:
        raise ValueError("If use_all_clients=False, num_select cannot be None!")

    if select_by_loss and global_model:
        client_losses = {}

        for client_id, loader in client_loaders.items():
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            local_train(local_model, loader, epochs=5, lr=0.01)
            loss, _ = evaluate(local_model, loader)
            client_losses[client_id] = loss
        selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
        print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
    else:
        selected_clients = random.sample(list(client_loaders.keys()), num_select)
        print(f"Randomly selected {num_select} clients: {selected_clients}")

    return selected_clients


def update_communication_counts(communication_counts, selected_clients, event):
    """
    客户端通信计数
    - event='receive' 表示客户端接收到全局模型
    - event='send' 表示客户端上传本地模型
    - event='full_round' 仅在客户端完成完整收发时增加
    """
    for client_id in selected_clients:
        communication_counts[client_id][event] += 1

        # 仅当客户端完成一次完整的 send 和 receive 时增加 full_round
        if event == "send" and communication_counts[client_id]['receive'] > 0:
            communication_counts[client_id]['full_round'] += 1


def print_lora_params(model, round_num, prefix="Global"):
    print(f"\n[📦 {prefix} Model - Round {round_num}]")
    for name, param in model.named_parameters():
        if 'lora.A' in name or 'lora.B' in name:
            print(f"{name}: norm={param.data.norm():.4f}")


def run_experiment(selection_method, rounds=100, num_selected_clients=2, is_LoRA=False, use_svd=False):
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # 加载 MNIST 数据集
    train_data, test_data = load_mnist_data()

    # 生成客户端数据集，每个客户端只包含特定类别
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # 创建数据加载器
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # Initialize global model, communication_counts, and results storage
    global_model = MLPModel()

    global_accuracies = []
    total_communication_counts = []
    csv_data = []

    # Initialize communication counters for all clients
    communication_counts = {client_id: {'send': 0, 'receive': 0, 'full_round': 0}
                            for client_id in client_loaders.keys()}

    for r in range(rounds):
        print(f"\nRound {r+1}")
        
        # Select clients based on the specified method:
        if selection_method == "fedGRA":
            selected_clients = select_clients(client_loaders, use_all_clients=False,
                                              num_select=num_selected_clients,
                                              select_by_loss=True, global_model=global_model, grc=True, is_LoRA=is_LoRA, use_svd=use_svd)
        elif selection_method == "high_loss":
            # Use loss-based selection without GRC (select top 2 highest loss clients)
            selected_clients = select_clients(client_loaders, use_all_clients=False,
                                              num_select=num_selected_clients,
                                              select_by_loss=True, global_model=global_model, grc=False, is_LoRA=is_LoRA, use_svd=use_svd)
        elif selection_method == "fedavg":
            # Use FedAvg with either random selection or all clients.
            # Using all clients or random selection for FedAvg.
            selected_clients = select_clients(client_loaders, use_all_clients=True,
                                              num_select=num_selected_clients,
                                              select_by_loss=False, global_model=global_model, grc=False)

        # Record receive communication count
        update_communication_counts(communication_counts, selected_clients, "receive")
        client_state_dicts = []

        print_lora_params(global_model, r+1, prefix="Before")

        # Perform local training on selected clients
        for client_id in selected_clients:
            client_loader = client_loaders[client_id]
            local_model = MLPModel()  
            local_model.load_state_dict(global_model.state_dict())  # Sync with the global model
            local_train(local_model, client_loader, epochs=1, lr=0.01, is_LoRA=is_LoRA)
            client_state_dicts.append((client_id, local_model.state_dict()))
            update_communication_counts(communication_counts, [client_id], "send")
            print(f"Client {client_id} trained.")
        
        print_lora_params(global_model, r+1, prefix="After")

        # Compute communication counts for this round
        total_send = sum(communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1)
                         for c in selected_clients)
        total_receive = sum(communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1)
                            for c in selected_clients)
        total_comm = total_send + total_receive
        total_communication_counts.append(total_comm)

        # Aggregate model updates
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)

        # Evaluate global model
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)
        print(f"Test Accuracy: {accuracy:.2f}%")
        
        # Save round data; add a column indicating the method used if desired.
        csv_data.append([r+1, accuracy, total_comm])

    # Convert collected data to a DataFrame
    df = pd.DataFrame(csv_data, columns=['Round', f'Accuracy_{selection_method}', f'Comm_{selection_method}'])
    return df

def main_experiments():
    
    rounds = 10
    # Run experiments for each method
    # df_high_loss = run_experiment("high_loss", rounds, num_selected_clients=2)
    # df_lora_svd = run_experiment("high_loss", rounds, num_selected_clients=2, is_LoRA=True, use_svd=True, freeze_W=True)

    start_time_1 = time.time()
    df_lora = run_experiment("high_loss", rounds, num_selected_clients=2, is_LoRA=True)
    end_time_1 = time.time()

    # start_time_2 = time.time()
    # df_lora_svd = run_experiment("fedGRA", rounds, num_selected_clients=2, is_LoRA=True, use_svd=True, freeze_W=True)
    # end_time_2 = time.time()

    # start_time_3 = time.time()
    # df_lora_W = run_experiment("fedGRA", rounds, num_selected_clients=2, is_LoRA=True, freeze_W=False)
    # end_time_3 = time.time()

    start_time_4 = time.time()
    df_GRA = run_experiment("high_loss", rounds, num_selected_clients=2)
    end_time_4 = time.time()


    # Merge DataFrames on 'Round'
    # df_combined = df_lora.merge(df_lora_svd, on='Round').merge(df_lora_W, on='Round').merge(df_GRA, on='Round', suffixes=('_fedGRA', '_fedGRA_pure'))
    df_combined = df_lora.merge(df_GRA, on='Round')

    # Save to CSV for later inspection if needed
    df_combined.to_csv("comparison_results_test.csv", index=False)

    print(f"LoRA GRA Freeze Original Weight Experiment took {end_time_1 - start_time_1:.2f} .")
    # print(f"LoRA SVD GRA Freeze Original Weight Experiment took {end_time_2 - start_time_2:.2f} .")
    # print(f"LoRA SVD GRA NOT Freeze Original Weight Experiment took {end_time_3 - start_time_3:.2f} .")
    print(f"Pure GRA Experiment took {end_time_4 - start_time_4:.2f} .")

if __name__ == "__main__":
    main_experiments()

