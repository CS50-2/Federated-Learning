# -*- coding: utf-8 -*-
"""FedGRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oCDVGlfmhZxd7S-pA9nYyVbZcD6JEhAI
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import matplotlib.pyplot as plt

# ----------- 模型定义 -----------
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# ----------- 加载数据 -----------
def load_mnist_data(data_path="./data"):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)
    return train_data, test_data

# ----------- 自定义数据分布 -----------
def split_data_custom_matrix(dataset):
    distribution_matrix = [
        [100, 90, 80, 70, 60, 50, 40, 30, 20, 10],
        [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
        [15, 25, 35, 45, 55, 65, 75, 85, 95, 105],
        [5, 15, 25, 35, 45, 55, 65, 75, 85, 95],
        [95, 85, 75, 65, 55, 45, 35, 25, 15, 5],
        [50, 50, 50, 50, 50, 50, 50, 50, 50, 50],
        [0, 10, 0, 10, 0, 10, 0, 10, 0, 10],
        [10, 0, 10, 0, 10, 0, 10, 0, 10, 0],
        [30, 30, 30, 30, 30, 30, 30, 30, 30, 30],
        [20, 25, 20, 25, 20, 25, 20, 25, 20, 25],
    ]

    num_classes = 10
    label_to_indices = {i: [] for i in range(num_classes)}
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    client_datasets = []
    client_data_sizes = {}
    for client_id, class_counts in enumerate(distribution_matrix):
        indices = []
        for label, count in enumerate(class_counts):
            label_indices = label_to_indices[label][:count]
            label_to_indices[label] = label_to_indices[label][count:]
            indices.extend(label_indices)
        client_datasets.append((client_id, torch.utils.data.Subset(dataset, indices)))
        client_data_sizes[client_id] = len(indices)

    return client_datasets, client_data_sizes

# ----------- 本地训练 -----------
def local_train(model, train_loader, epochs=1, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for _ in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x.view(batch_x.size(0), -1))
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()

# ----------- 聚合 -----------
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    total_data = sum(client_sizes[label] for (label, _) in client_state_dicts)
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (client_sizes[label] / total_data)
            for (label, client_state) in client_state_dicts
        )
    global_model.load_state_dict(global_dict)
    return global_model

# ----------- 评估 -----------
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x.view(batch_x.size(0), -1))
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return total_loss / len(test_loader), correct / total * 100

# ----------- GRC & GRG -----------
def compute_grc_scores(metrics_dict):
    matrix = np.array(list(metrics_dict.values()))
    matrix = (matrix - matrix.min(axis=0)) / (matrix.max(axis=0) - matrix.min(axis=0) + 1e-8)
    ideal = matrix.max(axis=0)
    delta = np.abs(matrix - ideal)
    delta_min = delta.min()
    delta_max = delta.max()
    rho = 0.5
    grc = (delta_min + rho * delta_max) / (delta + rho * delta_max)
    grg = grc.mean(axis=1)
    return {client: grg[i] for i, client in enumerate(metrics_dict.keys())}

# ----------- 主程序 -----------
def run_grc_federated(rounds=50):
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    train_data, test_data = load_mnist_data()
    client_datasets, client_data_sizes = split_data_custom_matrix(train_data)
    client_loaders = {label: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for label, dataset in client_datasets}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    global_model = MLPModel()
    communication_rounds = []
    accuracy_list = []

    for r in range(rounds):
        print(f"🔄 Round {r+1}")
        metrics = {}
        for label, loader in client_loaders.items():
            model_copy = MLPModel()
            model_copy.load_state_dict(global_model.state_dict())
            loss = evaluate(model_copy, loader)[0]
            ram = random.uniform(0.5, 1.0)
            divergence = random.uniform(0.2, 0.8)
            metrics[label] = [1 - loss, ram, divergence]

        scores = compute_grc_scores(metrics)
        selected = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:2]
        selected_labels = [label for label, _ in selected]
        print(f"✅ Selected clients: {selected_labels}")

        client_state_dicts = []
        for label in selected_labels:
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            local_state = local_train(local_model, client_loaders[label], epochs=1, lr=0.01)
            client_state_dicts.append((label, local_state))

        client_sizes_subset = {label: client_data_sizes[label] for label in selected_labels}
        global_model = fed_avg(global_model, client_state_dicts, client_sizes_subset)

        loss, acc = evaluate(global_model, test_loader)
        accuracy_list.append(acc)
        communication_rounds.append((r + 1) * len(selected_labels))
        print(f"📊 Accuracy: {acc:.2f}%")

    return communication_rounds, accuracy_list

# ----------- 执行并绘图 -----------
if __name__ == "__main__":
    comms, accs = run_grc_federated(rounds=30)
    plt.figure(figsize=(10, 5))
    plt.plot(comms, accs, marker='o', label="FedGRA (GRC-based)")
    plt.xlabel("通信次数")
    plt.ylabel("测试集准确率 (%)")
    plt.title("FedGRA：通信次数 vs 准确率")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()