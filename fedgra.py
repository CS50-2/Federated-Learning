# -*- coding: utf-8 -*-
"""FedGRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oCDVGlfmhZxd7S-pA9nYyVbZcD6JEhAI
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import matplotlib.pyplot as plt

# ----------- æ¨¡å‹å®šä¹‰ -----------
class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# ----------- åŠ è½½æ•°æ® -----------
def load_mnist_data(data_path="./data"):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)
    return train_data, test_data

# ----------- è‡ªå®šä¹‰æ•°æ®åˆ†å¸ƒ -----------
def split_data_custom_matrix(dataset):
    distribution_matrix = [
        [100, 90, 80, 70, 60, 50, 40, 30, 20, 10],
        [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
        [15, 25, 35, 45, 55, 65, 75, 85, 95, 105],
        [5, 15, 25, 35, 45, 55, 65, 75, 85, 95],
        [95, 85, 75, 65, 55, 45, 35, 25, 15, 5],
        [50, 50, 50, 50, 50, 50, 50, 50, 50, 50],
        [0, 10, 0, 10, 0, 10, 0, 10, 0, 10],
        [10, 0, 10, 0, 10, 0, 10, 0, 10, 0],
        [30, 30, 30, 30, 30, 30, 30, 30, 30, 30],
        [20, 25, 20, 25, 20, 25, 20, 25, 20, 25],
    ]

    num_classes = 10
    label_to_indices = {i: [] for i in range(num_classes)}
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    client_datasets = []
    client_data_sizes = {}
    for client_id, class_counts in enumerate(distribution_matrix):
        indices = []
        for label, count in enumerate(class_counts):
            label_indices = label_to_indices[label][:count]
            label_to_indices[label] = label_to_indices[label][count:]
            indices.extend(label_indices)
        client_datasets.append((client_id, torch.utils.data.Subset(dataset, indices)))
        client_data_sizes[client_id] = len(indices)

    return client_datasets, client_data_sizes

# ----------- æœ¬åœ°è®­ç»ƒ -----------
def local_train(model, train_loader, epochs=1, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for _ in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x.view(batch_x.size(0), -1))
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()

# ----------- èšåˆ -----------
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    total_data = sum(client_sizes[label] for (label, _) in client_state_dicts)
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (client_sizes[label] / total_data)
            for (label, client_state) in client_state_dicts
        )
    global_model.load_state_dict(global_dict)
    return global_model

# ----------- è¯„ä¼° -----------
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x.view(batch_x.size(0), -1))
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    return total_loss / len(test_loader), correct / total * 100

# ----------- GRC & GRG -----------
def compute_grc_scores(metrics_dict):
    matrix = np.array(list(metrics_dict.values()))
    matrix = (matrix - matrix.min(axis=0)) / (matrix.max(axis=0) - matrix.min(axis=0) + 1e-8)
    ideal = matrix.max(axis=0)
    delta = np.abs(matrix - ideal)
    delta_min = delta.min()
    delta_max = delta.max()
    rho = 0.5
    grc = (delta_min + rho * delta_max) / (delta + rho * delta_max)
    grg = grc.mean(axis=1)
    return {client: grg[i] for i, client in enumerate(metrics_dict.keys())}

# ----------- ä¸»ç¨‹åº -----------
def run_grc_federated(rounds=50):
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    train_data, test_data = load_mnist_data()
    client_datasets, client_data_sizes = split_data_custom_matrix(train_data)
    client_loaders = {label: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for label, dataset in client_datasets}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    global_model = MLPModel()
    communication_rounds = []
    accuracy_list = []

    for r in range(rounds):
        print(f"ğŸ”„ Round {r+1}")
        metrics = {}
        for label, loader in client_loaders.items():
            model_copy = MLPModel()
            model_copy.load_state_dict(global_model.state_dict())
            loss = evaluate(model_copy, loader)[0]
            ram = random.uniform(0.5, 1.0)
            divergence = random.uniform(0.2, 0.8)
            metrics[label] = [1 - loss, ram, divergence]

        scores = compute_grc_scores(metrics)
        selected = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:2]
        selected_labels = [label for label, _ in selected]
        print(f"âœ… Selected clients: {selected_labels}")

        client_state_dicts = []
        for label in selected_labels:
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            local_state = local_train(local_model, client_loaders[label], epochs=1, lr=0.01)
            client_state_dicts.append((label, local_state))

        client_sizes_subset = {label: client_data_sizes[label] for label in selected_labels}
        global_model = fed_avg(global_model, client_state_dicts, client_sizes_subset)

        loss, acc = evaluate(global_model, test_loader)
        accuracy_list.append(acc)
        communication_rounds.append((r + 1) * len(selected_labels))
        print(f"ğŸ“Š Accuracy: {acc:.2f}%")

    return communication_rounds, accuracy_list

# ----------- æ‰§è¡Œå¹¶ç»˜å›¾ -----------
if __name__ == "__main__":
    comms, accs = run_grc_federated(rounds=30)
    plt.figure(figsize=(10, 5))
    plt.plot(comms, accs, marker='o', label="FedGRA (GRC-based)")
    plt.xlabel("é€šä¿¡æ¬¡æ•°")
    plt.ylabel("æµ‹è¯•é›†å‡†ç¡®ç‡ (%)")
    plt.title("FedGRAï¼šé€šä¿¡æ¬¡æ•° vs å‡†ç¡®ç‡")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()