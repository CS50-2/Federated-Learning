# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10l6XsOmDKyRTRMMtinKx0cKuKb7Zu_c-
"""

# -*- coding: utf-8 -*-
"""Federated Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tE-M1T-9BL-HglL5A7asx4b31Sdyhcdq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import csv
import pandas as pd
from datetime import datetime
! pip install fvcore
from fvcore.nn import FlopCountAnalysis
import torch.nn.functional as F
import torch.nn.utils.prune as prune


class LoRALinear(nn.Module):
    def __init__(self, linear_layer, rank=8, alpha=1.0):
        super().__init__()
        self.original = linear_layer  # åŸå§‹çº¿æ€§å±‚ï¼ˆå†»ç»“ï¼‰
        self.original.requires_grad_(False)

        m, n = linear_layer.weight.shape
        self.rank = rank
        self.alpha = alpha / rank  # ç¼©æ”¾ç³»æ•°

        # åˆå§‹åŒ–ä½ç§©çŸ©é˜µ A å’Œ B
        self.A = nn.Parameter(torch.randn(m, rank))  # éšæœºåˆå§‹åŒ– A
        self.B = nn.Parameter(torch.zeros(n, rank))  # é›¶åˆå§‹åŒ– B

    def forward(self, x):
        # è®¡ç®—ä½ç§©æ›´æ–°ï¼šÎ”W = A @ B^T
        delta_W = self.alpha * (self.A @ self.B.T)
        # è¾“å‡º = åŸå§‹æƒé‡è¾“å‡º + ä½ç§©æ›´æ–°è¾“å‡º
        return self.original(x) + F.linear(x, delta_W)



# å®šä¹‰ MLP æ¨¡å‹
class MLPModel(nn.Module):
    def __init__(self, use_lora=False, rank=8):
        super(MLPModel, self).__init__()
        self.use_lora = use_lora
        self.rank = rank

        # åŸå§‹å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(28 * 28, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 10)
        self.relu = nn.ReLU()

        # å¦‚æœå¯ç”¨ LoRAï¼Œæ›¿æ¢ä¸º LoRALinear
        if use_lora:
            self.fc1 = LoRALinear(self.fc1, rank=rank)
            self.fc2 = LoRALinear(self.fc2, rank=rank)
            self.fc3 = LoRALinear(self.fc3, rank=rank)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x


# åŠ è½½ MNIST æ•°æ®é›†
def load_mnist_data(data_path="./data"):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

    if os.path.exists(os.path.join(data_path, "MNIST/raw/train-images-idx3-ubyte")):
        print("âœ… MNIST æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
    else:
        print("â¬‡ï¸ æ­£åœ¨ä¸‹è½½ MNIST æ•°æ®é›†...")

    train_data = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)

    # visualize_mnist_samples(train_data)
    return train_data, test_data


# æ˜¾ç¤ºæ•°æ®é›†ç¤ºä¾‹å›¾ç‰‡
def visualize_mnist_samples(dataset, num_samples=10):
    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 1.2, 1.5))
    for i in range(num_samples):
        img, label = dataset[i]
        axes[i].imshow(img.squeeze(), cmap="gray")
        axes[i].set_title(label)
        axes[i].axis("off")
    plt.show()




def local_train_lora(model, train_loader, epochs=1, lr=0.01):
    criterion = nn.CrossEntropyLoss()

    # ä»…ä¼˜åŒ– LoRA å‚æ•°ï¼ˆA å’Œ Bï¼‰
    lora_params = []
    for name, param in model.named_parameters():
        if 'A' in name or 'B' in name:  # åªé€‰æ‹© LoRA çš„å‚æ•°
            lora_params.append(param)
    optimizer = optim.SGD(lora_params, lr=lr)

    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()

def split_data_by_label(dataset, num_clients=10):
    """
    æ‰‹åŠ¨åˆ’åˆ†æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å« 10 ä¸ªç±»åˆ«ï¼Œå¹¶è‡ªå®šä¹‰æ ·æœ¬æ•°é‡ã€‚
    :param dataset: åŸå§‹æ•°æ®é›†ï¼ˆå¦‚ MNISTï¼‰
    :param num_clients: å®¢æˆ·ç«¯æ€»æ•°
    :return: (å®¢æˆ·ç«¯æ•°æ®é›†, å®¢æˆ·ç«¯æ•°æ®å¤§å°)
    """
    # æ‰‹åŠ¨åˆ’åˆ†çš„æ ·æœ¬æ•°é‡ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯ 10 ä¸ªç±»åˆ«çš„æ•°æ®é‡ï¼‰
    client_data_sizes = {
        0: {0: 600},
        1: {1: 700},
        2: {2: 500},
        3: {3: 600},
        4: {4: 600},
        5: {5: 500},
        6: {6: 500},
        7: {7: 500},
        8: {8: 500},
        9: {9: 500}
    }

    # client_data_sizes = {
    #     0: {0: 600, 1: 600, 2: 600, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100},
    #     1: {1: 700, 2: 700, 3: 700, 0: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100},
    #     2: {2: 700, 3: 700, 4: 700, 0: 100, 1: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100},
    #     3: {3: 700, 4: 700, 5: 700, 0: 100, 1: 100, 2: 100, 6: 100, 7: 100, 8: 100, 9: 100},
    #     4: {4: 700, 5: 700, 6: 700, 0: 100, 1: 100, 2: 100, 3: 100, 7: 100, 8: 100, 9: 100},
    #     5: {5: 700, 6: 700, 7: 700, 0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 8: 100, 9: 100},
    #     6: {6: 700, 7: 700, 8: 700, 0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 9: 100},
    #     7: {7: 700, 8: 700, 9: 700, 0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100},
    #     8: {8: 700, 9: 700, 0: 700, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100},
    #     9: {9: 700, 0: 700, 1: 700, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100}
    # }

    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°æ®ç´¢å¼•
    label_to_indices = {i: [] for i in range(10)}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç´¢å¼•
    for idx, (_, label) in enumerate(dataset):
        label_to_indices[label].append(idx)

    # åˆå§‹åŒ–å®¢æˆ·ç«¯æ•°æ®å­˜å‚¨
    client_data_subsets = {}
    client_actual_sizes = {i: {label: 0 for label in range(10)} for i in range(num_clients)}  # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡

    # éå†æ¯ä¸ªå®¢æˆ·ç«¯ï¼Œä¸ºå…¶åˆ†é…æŒ‡å®šç±»åˆ«çš„æ•°æ®
    for client_id, label_info in client_data_sizes.items():
        selected_indices = []  # ä¸´æ—¶å­˜å‚¨è¯¥å®¢æˆ·ç«¯æ‰€æœ‰é€‰ä¸­çš„ç´¢å¼•
        for label, size in label_info.items():
            # ç¡®ä¿ä¸è¶…å‡ºç±»åˆ«æ•°æ®é›†å®é™…å¤§å°
            available_size = len(label_to_indices[label])
            sample_size = min(available_size, size)

            if sample_size < size:
                print(f"âš ï¸ è­¦å‘Šï¼šç±»åˆ« {label} çš„æ•°æ®ä¸è¶³ï¼Œå®¢æˆ·ç«¯ {client_id} åªèƒ½è·å– {sample_size} æ¡æ ·æœ¬ï¼ˆéœ€æ±‚ {size} æ¡ï¼‰")

            # ä»è¯¥ç±»åˆ«ä¸­éšæœºæŠ½å–æ ·æœ¬
            sampled_indices = random.sample(label_to_indices[label], sample_size)
            selected_indices.extend(sampled_indices)

            # è®°å½•å®é™…åˆ†é…çš„æ•°æ®é‡
            client_actual_sizes[client_id][label] = sample_size

        # åˆ›å»º PyTorch Subset
        client_data_subsets[client_id] = torch.utils.data.Subset(dataset, selected_indices)

    # æ‰“å°æ¯ä¸ªå®¢æˆ·ç«¯çš„å®é™…åˆ†é…æ•°æ®é‡
    print("\nğŸ“Š æ¯ä¸ªå®¢æˆ·ç«¯å®é™…æ•°æ®åˆ†å¸ƒ:")
    for client_id, label_sizes in client_actual_sizes.items():
        print(f"å®¢æˆ·ç«¯ {client_id}: {label_sizes}")

    return client_data_subsets, client_actual_sizes


# æœ¬åœ°è®­ç»ƒå‡½æ•°
def local_train(model, train_loader, epochs=5, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    model.train()
    for epoch in range(epochs):
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
    return model.state_dict()



#  è”é‚¦å¹³å‡èšåˆå‡½æ•°
def fed_avg(global_model, client_state_dicts, client_sizes):
    global_dict = global_model.state_dict()
    subkey = [sublist[0] for sublist in client_state_dicts]
    new_client_sizes = dict(([(key, client_sizes[key]) for key in subkey]))
    total_data = sum(sum(label_sizes.values()) for label_sizes in new_client_sizes.values())  # è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®æ€»é‡
    for key in global_dict.keys():
        global_dict[key] = sum(
            client_state[key] * (sum(new_client_sizes[client_id].values()) / total_data)
            for (client_id, client_state) in client_state_dicts
        )
    global_model.load_state_dict(global_dict)
    return global_model


# è¯„ä¼°æ¨¡å‹
def evaluate(model, test_loader):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    correct, total, total_loss = 0, 0, 0.0
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    accuracy = correct / total * 100
    return total_loss / len(test_loader), accuracy


# def entropy_weight(matrix):
#     """
#     matrix: np.array of shape (n_clients, n_indicators)ï¼Œå³ N Ã— 2 çš„æŒ‡æ ‡çŸ©é˜µ
#     return: weight of each indicator (np.array of shape (2,))
#     """
#     P = matrix / matrix.sum(axis=0)  # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡çŸ©é˜µ
#     K = 1 / np.log(matrix.shape[0])
#     E = -K * np.sum(P * np.log(P + 1e-12), axis=0)  # ç†µå€¼
#     d = 1 - E
#     w = d / np.sum(d)
#     return w

# def calculate_GRC(global_model, client_models, client_losses):
#     """
#     è®¡ç®—å®¢æˆ·ç«¯çš„ GRC åˆ†æ•°ã€‚

#     å‚æ•°:
#         global_model (nn.Module): å…¨å±€æ¨¡å‹ã€‚
#         client_models (list): å®¢æˆ·ç«¯æœ¬åœ°æ¨¡å‹åˆ—è¡¨ã€‚
#         client_losses (list): å®¢æˆ·ç«¯è®­ç»ƒæŸå¤±åˆ—è¡¨ã€‚

#     è¿”å›:
#         list: æ¯ä¸ªå®¢æˆ·ç«¯çš„ GRC åˆ†æ•°ã€‚
#     """
#     # 1. æ„å»ºå‚è€ƒåºåˆ—ï¼ˆç†æƒ³å€¼ï¼šæŸå¤±=0ï¼Œæ¨¡å‹å‚æ•°å·®å¼‚=0ï¼‰
#     ref_loss = 0.0
#     ref_param_diff = 0.0

#     # 2. è®¡ç®—å®¢æˆ·ç«¯æŒ‡æ ‡
#     param_diffs = []
#     for model in client_models:
#         diff = 0.0
#         for g_param, l_param in zip(global_model.parameters(), model.parameters()):
#             diff += torch.norm(g_param - l_param).item()  # å‚æ•°å·®å¼‚ï¼ˆL2èŒƒæ•°ï¼‰
#         param_diffs.append(diff)

#     # 3. å¯¹ losses å’Œ diffs è¿›è¡Œ mapping
#     def map_sequence(sequence):
#         max_val = max(sequence)
#         min_val = min(sequence)
#         return [(max_val + x) / (max_val + min_val) for x in sequence]

#     client_losses = map_sequence(client_losses)  # æ˜ å°„åçš„ losses
#     param_diffs = map_sequence(param_diffs)  # æ˜ å°„åçš„ diffs


#     max_loss = max(client_losses)
#     max_diff = max(param_diffs)


#     # 4. è®¡ç®—å…¨å±€æå€¼
#     all_deltas = []
#     for nl, nd in zip(client_losses, param_diffs):
#         all_deltas.append(abs(nl - max_loss))  # æŸå¤±å·®å€¼
#         all_deltas.append(abs(nd - max_diff))  # å‚æ•°å·®å¼‚å·®å€¼
#     max_delta = max(all_deltas)  # å…¨å±€æœ€å¤§å€¼
#     min_delta = min(all_deltas)  # å…¨å±€æœ€å°å€¼

#     # 5. è®¡ç®— GRCï¼ˆåˆ†è¾¨ç³»æ•° Ï=0.5ï¼‰
#     grc_scores = []
#     grc_losses = []
#     grc_diffs = []
#     for nl, nd in zip(client_losses, param_diffs):
#         delta_loss = abs(nl - max_loss)
#         delta_diff = abs(nd - max_diff)

#         # ç°è‰²å…³è”ç³»æ•°å…¬å¼
#         grc_loss = (min_delta + 0.5 * max_delta) / (delta_loss + 0.5 * max_delta)
#         grc_diff = (min_delta + 0.5 * max_delta) / (delta_diff + 0.5 * max_delta)

#         grc_losses.append(grc_loss)
#         grc_diffs.append(grc_diff)

#     # å°† grc_loss å’Œ grc_diff ç»„åˆæˆæŒ‡æ ‡çŸ©é˜µ (n_clients Ã— 2)
#     grc_metrics = np.stack([client_losses, param_diffs], axis=1)

#     # è®¡ç®—ç†µæƒæ³•æƒé‡
#     weights = entropy_weight(grc_metrics)  # [w_loss, w_diff]

#     # è®¡ç®—æœ€ç»ˆçš„åŠ æƒåˆ†æ•°
#     weighted_score = grc_losses / weights[0] + grc_diffs / weights[1]

#     return weighted_score,weights


def entropy_weight(l):
    entropies = []
    for X in l:
        P = X / (np.sum(X) + 1e-12)  # å½’ä¸€åŒ–å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
        K = 1 / np.log(len(X))
        E = -K * np.sum(P * np.log(P + 1e-12))  # è®¡ç®—ç†µï¼Œè¶Šå¤§è¶Šæ— åŒºåˆ†åº¦
        entropies.append(E)

        # ç®—ä¿¡æ¯é‡
    information_gain = [1 - e for e in entropies]
    # å½’ä¸€åŒ–
    sum_ig = sum(information_gain)
    weights = [ig / sum_ig for ig in information_gain]

    return weights


def calculate_GRC(global_model, client_models, client_losses):
    """
    æ­£ç¡®è®¡ç®— GRC åˆ†æ•°ï¼Œå¹¶ä¿®æ­£åç»­æ­¥éª¤
    """

    # 1. è®¡ç®—å®¢æˆ·ç«¯æŒ‡æ ‡ï¼ˆå‚æ•°å·®å¼‚ï¼‰
    param_diffs = []
    for model in client_models:
        diff = 0.0
        for g_param, l_param in zip(global_model.parameters(), model.parameters()):
            diff += torch.norm(g_param - l_param).item()
        param_diffs.append(diff)

    # 2. å¯¹ losses å’Œ diffs è¿›è¡Œæ­£ç¡® mapping
    def map_sequence_loss(sequence):
        max_val = max(sequence)
        min_val = min(sequence)
        return [(max_val - x) / (max_val + min_val) for x in sequence]  # ã€âœ”ã€‘è´Ÿç›¸å…³

    def map_sequence_diff(sequence):
        max_val = max(sequence)
        min_val = min(sequence)
        return [(x - min_val) / (max_val + min_val) for x in sequence]  # ã€âœ”ã€‘æ­£ç›¸å…³

    client_losses = map_sequence_loss(client_losses)
    param_diffs = map_sequence_diff(param_diffs)

    # 3. æ„å»ºå‚è€ƒåºåˆ— (ç†æƒ³å€¼ = 1)
    ref_loss = 1.0
    ref_diff = 1.0

    # 4. è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„ Î”
    all_deltas = []
    for loss, diff in zip(client_losses, param_diffs):
        all_deltas.append(abs(loss - ref_loss))
        all_deltas.append(abs(diff - ref_diff))
    max_delta = max(all_deltas)
    min_delta = min(all_deltas)

    # 5. è®¡ç®—ç°è‰²å…³è”ç³»æ•° (GRC)ï¼ŒÏ=0.5
    grc_losses = []
    grc_diffs = []
    for loss, diff in zip(client_losses, param_diffs):
        delta_loss = abs(loss - ref_loss)
        delta_diff = abs(diff - ref_diff)

        grc_loss = (min_delta + 0.5 * max_delta) / (delta_loss + 0.5 * max_delta)
        grc_diff = (min_delta + 0.5 * max_delta) / (delta_diff + 0.5 * max_delta)

        grc_losses.append(grc_loss)
        grc_diffs.append(grc_diff)

    grc_losses = np.array(grc_losses)
    grc_diffs = np.array(grc_diffs)

    # 6. è®¡ç®—ç†µæƒï¼ˆåŸºäºåŸå§‹mappedæ•°æ®ï¼‰
    grc_metrics = np.vstack([client_losses, param_diffs])  # ã€æ³¨æ„ã€‘è¿™é‡Œ shape æ˜¯ (2, n_clients)
    weights = entropy_weight(grc_metrics)  # ã€âœ”ã€‘ç†µæƒç®—çš„æ˜¯åŸmappedæŒ‡æ ‡ï¼Œä¸æ˜¯grcï¼

    # 7. åŠ æƒæ±‚å’Œï¼Œæ³¨æ„æ˜¯ã€ä¹˜æ³•ã€‘ä¸æ˜¯é™¤æ³•
    weighted_score = grc_losses * weights[0] + grc_diffs * weights[1]  # ã€ä¿®æ”¹ç‚¹ã€‘ä¹˜æ³•ï¼

    return weighted_score, weights

def compute_model_flops(
    model, input_shape=(1, 1, 28, 28), batch_size=1, num_samples=1, epochs=1, backward_factor=0, verbose=False
):
    """
    è®¡ç®—è®­ç»ƒé˜¶æ®µçš„æ€» FLOPsï¼ˆä¼°ç®—ï¼‰
    :param model: PyTorch æ¨¡å‹
    :param input_shape: å•ä¸ªæ ·æœ¬çš„è¾“å…¥ shapeï¼ˆå¦‚ MNIST ä¸º (1, 28, 28)ï¼‰
    :param batch_size: æ¯æ¬¡è¾“å…¥çš„æ ·æœ¬æ•°
    :param num_samples: è¯¥å®¢æˆ·ç«¯ä¸€å…±å¤šå°‘è®­ç»ƒæ ·æœ¬
    :param epochs: æ¯è½®è®­ç»ƒå‡ ä¸ª epoch
    :param backward_factor: backward FLOPs æ˜¯ forward çš„å‡ å€ï¼ˆä¸€èˆ¬æ˜¯ 2~3ï¼‰
    :param verbose: æ˜¯å¦æ‰“å°æ¯å±‚ FLOPs
    :return: ä¼°ç®—æ€» FLOPs
    """
    model.eval()
    dummy_input = torch.randn((batch_size, *input_shape[1:]))  # æ³¨æ„æ‹†è§£ shape
    flops_analyzer = FlopCountAnalysis(model, dummy_input)

    if verbose:
        print(flops_analyzer.by_module())  # æ‰“å°æ¯å±‚ FLOPs
        print(f"ğŸ“Š å•ä¸ª batch çš„ forward FLOPs: {flops_analyzer.total() / 1e6:.2f} MFLOPs")

    # å•ä¸ª batch çš„ forward FLOPs
    forward_flops_per_batch = flops_analyzer.total()

    # è®­ç»ƒ FLOPs â‰ˆ (forward + backward) Ã— æ‰¹æ¬¡æ•° Ã— epoch
    num_batches = int(np.ceil(num_samples / batch_size))
    total_training_flops = forward_flops_per_batch * (1 + backward_factor) * num_batches * epochs

    return total_training_flops

def apply_pruning(model, amount=0.3):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            torch.nn.utils.prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)
            torch.nn.utils.prune.remove(module, 'weight')  # ğŸ”¥ ç§»é™¤æ©ç ï¼Œå®é™…ä¿®æ”¹æ¨¡å‹ç»“æ„
    return model

def apply_freezing(model):
    for name, param in model.named_parameters():
        if 'weight' in name or 'bias' in name:
            param.requires_grad = False
    return model

def apply_partial_freezing(model, freeze_layers=1):
    """
    å†»ç»“æ¨¡å‹å‰ freeze_layers ä¸ª Linear å±‚çš„æƒé‡å’Œåç½®ã€‚
    """
    count = 0
    for module in model.modules():
        if isinstance(module, nn.Linear):
            for name, param in module.named_parameters():
                param.requires_grad = False
            count += 1
            if count >= freeze_layers:
                break
    return model

def select_clients(client_loaders, use_all_clients=False, num_select=None,
                   select_by_loss=False, global_model=None, grc=False,lora=True, prune=False, freeze=False):
    total_flops = 0
    epochs = 1
    backward_factor = 3  # è®­ç»ƒè¿‡ç¨‹ä¸­ FLOPs æ˜¯ forward çš„ 3 å€å·¦å³
    if grc:  # ä½¿ç”¨ GRC é€‰æ‹©å®¢æˆ·ç«¯
        client_models = []
        # 1. è®­ç»ƒæœ¬åœ°æ¨¡å‹å¹¶è®¡ç®—æŸå¤±
        client_losses = []
        # åˆå§‹åŒ– FLOPs ç´¯åŠ å™¨
        for client_id, client_loader in client_loaders.items():
            local_model = MLPModel(use_lora=lora, rank=8) # åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
            local_model.load_state_dict(global_model.state_dict()) # åŒæ­¥å…¨å±€æ¨¡å‹
            if prune:
                local_model = apply_pruning(local_model, amount=0.3)
            if freeze:
                local_model = apply_partial_freezing(local_model)
            # æ¯è½®éƒ½é‡æ–°ä¼°è®¡ä¸€æ¬¡ FLOPsï¼ˆé¿å…æ¨¡å‹å˜ç»“æ„æ—¶ä¸æ›´æ–°ï¼‰
            n_samples = len(client_loader.dataset)
            sample_flops = compute_model_flops(local_model, input_shape=(1, 1, 28, 28), num_samples = n_samples)
            total_flops += sample_flops * n_samples * epochs * backward_factor
            local_train_lora(local_model, client_loader, epochs=1, lr=0.01)
            client_models.append(local_model)
            loss, _ = evaluate(local_model, client_loader)
            client_losses.append(loss)
        print(f"ğŸ“Š æœ¬è½®å®¢æˆ·ç«¯é€‰æ‹©é˜¶æ®µä¼°ç®—æ€» FLOPs: {total_flops / 1e12:.4f} TFLOPs")

        # 2. è®¡ç®— GRC åˆ†æ•°
        grc_scores, grc_weights = calculate_GRC(global_model, client_models, client_losses)
        select_clients.latest_weights = grc_weights  # è®°å½•æƒé‡

        # 3. æŒ‰ GRC åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼ŒGRCè¶Šé«˜è¡¨ç¤ºè¶Šå¥½ï¼‰
        client_grc_pairs = list(zip(client_loaders.keys(), grc_scores))
        client_grc_pairs.sort(key=lambda x: x[1], reverse=True)  # é™åºæ’åº

        # 4. é€‰æ‹© GRC æœ€é«˜çš„å‰ num_select ä¸ªå®¢æˆ·ç«¯
        selected = [client_id for client_id, _ in client_grc_pairs[:num_select]]
        return selected, total_flops

    # å…¶ä½™é€‰æ‹©é€»è¾‘ä¿æŒä¸å˜

    if use_all_clients is True:
        print("Selecting all clients")
        return list(client_loaders.keys())

    if num_select is None:
        raise ValueError("If use_all_clients=False, num_select cannot be None!")

    if select_by_loss and global_model:
        client_losses = {}
        for client_id, loader in client_loaders.items():
            local_model = MLPModel(use_lora=lora, rank=8)
            local_model.load_state_dict(global_model.state_dict())
            if prune:
                local_model = apply_pruning(local_model, amount=0.3)
            if freeze:
                local_model = apply_partial_freezing(local_model)
            # æ¯è½®éƒ½é‡æ–°ä¼°è®¡ä¸€æ¬¡ FLOPsï¼ˆé¿å…æ¨¡å‹å˜ç»“æ„æ—¶ä¸æ›´æ–°ï¼‰
            sample_flops = compute_model_flops(local_model, input_shape=(1, 1, 28, 28))
            n_samples = len(loader.dataset)
            total_flops += sample_flops * n_samples * epochs
            # local_train(local_model, loader, epochs=1, lr=0.01)
            loss, _ = evaluate(local_model, loader)
            client_losses[client_id] = loss
        selected_clients = sorted(client_losses, key=client_losses.get, reverse=True)[:num_select]
        print(f"Selected {num_select} clients with the highest loss: {selected_clients}")
        print(f"ğŸ“Š æœ¬è½®å®¢æˆ·ç«¯é€‰æ‹©é˜¶æ®µä¼°ç®—æ€» FLOPs: {total_flops / 1e12:.4f} TFLOPs")
    else:
        selected_clients = random.sample(list(client_loaders.keys()), num_select)
        print(f"Randomly selected {num_select} clients: {selected_clients}")

    return selected_clients, total_flops


def update_communication_counts(communication_counts, selected_clients, event):
    """
    å®¢æˆ·ç«¯é€šä¿¡è®¡æ•°
    - event='receive' è¡¨ç¤ºå®¢æˆ·ç«¯æ¥æ”¶åˆ°å…¨å±€æ¨¡å‹
    - event='send' è¡¨ç¤ºå®¢æˆ·ç«¯ä¸Šä¼ æœ¬åœ°æ¨¡å‹
    - event='full_round' ä»…åœ¨å®¢æˆ·ç«¯å®Œæˆå®Œæ•´æ”¶å‘æ—¶å¢åŠ 
    """
    for client_id in selected_clients:
        communication_counts[client_id][event] += 1

        # ä»…å½“å®¢æˆ·ç«¯å®Œæˆä¸€æ¬¡å®Œæ•´çš„ send å’Œ receive æ—¶å¢åŠ  full_round
        if event == "send" and communication_counts[client_id]['receive'] > 0:
            communication_counts[client_id]['full_round'] += 1

def run_experiment(rounds, client_loaders, test_loader, client_data_sizes, freeze=False, prune=False, grc = False, label="",):
    global_model = MLPModel()
    global_accuracies = []
    total_communication_counts = []
    total_tflops = 0

    communication_counts = {
        client_id: {'send': 0, 'receive': 0, 'full_round': 0}
        for client_id in client_loaders.keys()
    }

    # å®éªŒæ•°æ®å­˜å‚¨ CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"training_data_{timestamp}.csv"
    csv_data = []

    for r in range(rounds):
        print(f"\nğŸ” [{label}] Round {r + 1}")
        selected_clients, round_flops = select_clients(
            client_loaders, use_all_clients=False, num_select=2,
            select_by_loss=True, global_model=global_model,
            prune=prune, freeze=freeze, lora=False, grc=False
        )

        update_communication_counts(communication_counts, selected_clients, "receive")
        total_tflops += round_flops / 1e12

        client_state_dicts = []
        for client_id in selected_clients:
            local_model = MLPModel()
            local_model.load_state_dict(global_model.state_dict())
            local_state = local_train(local_model, client_loaders[client_id], epochs=1, lr=0.1)
            update_communication_counts(communication_counts, [client_id], "send")
            param_mean = {name: param.mean().item() for name, param in local_model.named_parameters()}
            client_state_dicts.append((client_id, local_state))
            print(f"  âœ… å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå®Œæˆ | æ ·æœ¬æ•°é‡: {sum(client_data_sizes[client_id].values())}")
            print(f"  ğŸ“Œ å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å‚æ•°å‡å€¼: {param_mean}")

        # é€šä¿¡æ¬¡æ•°è®¡ç®—
        total_send = sum(communication_counts[c]['send'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_receive = sum(communication_counts[c]['receive'] - (communication_counts[c]['full_round'] - 1) for c in selected_clients)
        total_comm = total_send + total_receive
        if len(total_communication_counts) > 0:
            total_comm += total_communication_counts[-1]
        total_communication_counts.append(total_comm)

        # èšåˆ
        global_model = fed_avg(global_model, client_state_dicts, client_data_sizes)
        loss, accuracy = evaluate(global_model, test_loader)
        global_accuracies.append(accuracy)

        # è¾“å‡ºæœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½
        final_loss, final_accuracy = evaluate(global_model, test_loader)
        print(f"ğŸ¯ Final model test set accuracy: {final_accuracy:.2f}%")

        # è¾“å‡ºé€šä¿¡è®°å½•
        print("\n Client Communication Statistics:")
        for client_id, counts in communication_counts.items():
            print(f"Client {client_id}: Sent {counts['send']} times, Received {counts['receive']} times, Completed full_round {counts['full_round']} times")
        # è®°å½•æ•°æ®åˆ° CSV
        if grc and hasattr(select_clients, 'latest_weights'):
            w_loss = select_clients.latest_weights[0]
            w_diff = select_clients.latest_weights[1]
            print(f"ğŸ“ˆ Round {r + 1} | GRC æƒé‡: w_loss = {w_loss:.4f}, w_diff = {w_diff:.4f}")

        else:
            w_loss = 'NA'
            w_diff = 'NA'

        csv_data.append([
            r + 1,
            accuracy,
            total_comm,
            ",".join(map(str, selected_clients)),
            w_loss,
            w_diff
        ])
        df = pd.DataFrame(csv_data, columns=[
            'Round', 'Accuracy', 'Total communication counts', 'Selected Clients',
            'GRC Weight - Loss', 'GRC Weight - Diff'])
        df.to_csv(csv_filename, index=False)

    return global_accuracies, total_communication_counts ,total_tflops


def main():
    torch.manual_seed(0)
    random.seed(0)
    np.random.seed(0)

    # åŠ è½½ MNIST æ•°æ®é›†
    train_data, test_data = load_mnist_data()

    # ç”Ÿæˆå®¢æˆ·ç«¯æ•°æ®é›†ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åŒ…å«å¤šä¸ªç±»åˆ«
    client_datasets, client_data_sizes = split_data_by_label(train_data)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    client_loaders = {client_id: data.DataLoader(dataset, batch_size=32, shuffle=True)
                      for client_id, dataset in client_datasets.items()}
    test_loader = data.DataLoader(test_data, batch_size=32, shuffle=False)

    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
    lora = False
    global_model = MLPModel(use_lora=lora, rank=8)
    global_accuracies = []  # è®°å½•æ¯è½®å…¨å±€æ¨¡å‹çš„æµ‹è¯•é›†å‡†ç¡®ç‡
    total_communication_counts = []  # è®°å½•æ¯è½®å®¢æˆ·ç«¯é€šä¿¡æ¬¡æ•°
    rounds = 50  # è”é‚¦å­¦ä¹ è½®æ•°
    use_all_clients = False  # æ˜¯å¦è¿›è¡Œå®¢æˆ·ç«¯é€‰æ‹©
    num_selected_clients = 2  # æ¯è½®é€‰æ‹©å®¢æˆ·ç«¯è®­ç»ƒæ•°é‡
    use_loss_based_selection = True  # æ˜¯å¦æ ¹æ® loss é€‰æ‹©å®¢æˆ·ç«¯
    t_flops = 0
    grc = False
    prune = True
    freeze = False

    # ä¸‰ç»„å®éªŒ
    acc_base, comm_base, flops_base = run_experiment(rounds, client_loaders, test_loader, client_data_sizes, freeze=False, prune=False, grc = False, label="Loss Only")
    acc_freeze, comm_freeze, flops_freeze = run_experiment(rounds, client_loaders, test_loader, client_data_sizes, freeze=True, prune=False, grc = False, label="Loss + Freeze")
    acc_prune, comm_prune, flops_prune = run_experiment(rounds, client_loaders, test_loader, client_data_sizes, freeze=False, prune=True, grc = False, label="Loss + Prune")


    # ğŸ”‹ ç»˜åˆ¶ FLOPs æŸ±çŠ¶å›¾
    plt.figure(figsize=(6, 5))
    labels = ["Loss Only", "Loss + Freeze", "Loss + Prune"]
    flops_values = [flops_base, flops_freeze, flops_prune]
    colors = ["blue", "green", "red"]

    plt.bar(labels, flops_values, color=colors)
    plt.ylabel("Total Estimated Training FLOPs (TFLOPs)")
    plt.title("FLOPs Comparison Across Strategies")
    plt.grid(axis="y")
    plt.tight_layout()
    plt.show()


    # ç»˜å›¾
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, rounds + 1 ), acc_base, 'b-', marker='o', label="Loss Only")
    plt.plot(range(1, rounds + 1), acc_freeze, 'g-', marker='^', label="Loss + Freeze")
    plt.plot(range(1, rounds + 1), acc_prune, 'r-', marker='s', label="Loss + Prune")
    plt.xlabel("Federated Learning Rounds")
    plt.ylabel("Accuracy")
    plt.title("Test Accuracy Over Federated Learning Rounds")
    plt.legend()
    plt.grid(True)
    plt.show()

    plt.figure(figsize=(8, 5))
    plt.plot(comm_base, acc_base, 'b-', marker='o', label="Loss Only")
    plt.plot(comm_freeze, acc_freeze, 'g-', marker='^', label="Loss + Freeze")
    plt.plot(comm_prune, acc_prune, 'r-', marker='s', label="Loss + Prune")
    plt.xlabel("Total Communication Count")
    plt.ylabel("Accuracy")
    plt.title("Test Accuracy vs. Total Communication")
    plt.legend()
    plt.grid(True)
    plt.show()

if __name__ == "__main__":
    main()